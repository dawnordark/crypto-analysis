#!/data/data/com.termux/files/usr/bin/python3
# -*- coding: utf-8 -*-

import sys
import subprocess
import os
import time
import re
import traceback  # æ·»åŠ  traceback å¯¼å…¥
import json
import math
import sqlite3
import requests
import threading
import queue
import logging
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import Flask, jsonify, send_from_directory, request
from binance.client import Client

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)])
logger = logging.getLogger(__name__)

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

app = Flask(__name__, static_folder=os.path.join(BASE_DIR, 'static'), static_url_path='/static')

# Binance API é…ç½® - ä½¿ç”¨ç¯å¢ƒå˜é‡
API_KEY = os.environ.get('BINANCE_API_KEY', 'your_api_key_here')
API_SECRET = os.environ.get('BINANCE_API_SECRET', 'your_api_secret_here')
client = None

# ==================== æ•°æ®ç¼“å­˜ ====================
data_cache = {
    "last_updated": None,
    "daily_rising": [],
    "short_term_active": [],
    "all_cycle_rising": [],
    "analysis_time": 0
}

# åªè¯»æ•°æ®ç¼“å­˜
current_data_cache = data_cache.copy()

# æŒä»“é‡æ•°æ®ç¼“å­˜ {symbol: {period: {data: ..., next_update: ...}}}
oi_data_cache = {}

# é˜»åŠ›ä½æ•°æ®ç¼“å­˜ {symbol: {interval: levels, expiration}}
resistance_cache = {}
RESISTANCE_CACHE_EXPIRATION = 24 * 3600  # 24å°æ—¶ç¼“å­˜

# ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œçº¿ç¨‹é—´é€šä¿¡
analysis_queue = queue.Queue()

# çº¿ç¨‹æ± æ‰§è¡Œå™¨
executor = ThreadPoolExecutor(max_workers=10)

# å‘¨æœŸè®¾ç½® (åˆ†é’Ÿ)
PERIOD_MINUTES = {
    '5m': 5,
    '15m': 15,
    '30m': 30,
    '1h': 60,
    '2h': 120,
    '4h': 240,
    '6h': 360,
    '12h': 720,
    '1d': 1440
}

# é˜»åŠ›ä½åˆ†æå‘¨æœŸ
RESISTANCE_INTERVALS = ['1m', '3m', '5m', '15m', '30m', '1h', '2h', '4h', '6h', '8h', '12h', '1d', '3d', '1w', '1M']

# æ‰€æœ‰åˆ†æå‘¨æœŸ
ALL_PERIODS = ['5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h', '1d']

# åˆå§‹åŒ–æ•°æ®åº“
def init_db():
    try:
        conn = sqlite3.connect('data.db')
        c = conn.cursor()

        c.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='crypto_data'"
        )
        table_exists = c.fetchone()

        if not table_exists:
            logger.info("ğŸ› ï¸ åˆ›å»ºæ•°æ®åº“è¡¨...")
            c.execute('''CREATE TABLE crypto_data
                        (id INTEGER PRIMARY KEY, 
                        last_updated TEXT,
                        daily_rising TEXT,
                        short_term_active TEXT,
                        all_cycle_rising TEXT,
                        analysis_time REAL,
                        resistance_data TEXT)''')
            conn.commit()
            logger.info("âœ… æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ")
        else:
            logger.info("âœ… æ•°æ®åº“è¡¨å·²å­˜åœ¨")

        conn.close()
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        try:
            os.remove('data.db')
            logger.warning("ğŸ—‘ï¸ åˆ é™¤æŸåçš„æ•°æ®åº“æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ•°æ®åº“")
            init_db()
        except Exception:
            logger.critical("ğŸ”¥ æ— æ³•ä¿®å¤æ•°æ®åº“ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥")

# ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
def save_to_db(data):
    try:
        conn = sqlite3.connect('data.db')
        c = conn.cursor()

        c.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='crypto_data'"
        )
        if not c.fetchone():
            logger.warning("âš ï¸ æ•°æ®åº“è¡¨ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            c.execute('''CREATE TABLE crypto_data
                        (id INTEGER PRIMARY KEY, 
                        last_updated TEXT,
                        daily_rising TEXT,
                        short_term_active TEXT,
                        all_cycle_rising TEXT,
                        analysis_time REAL,
                        resistance_data TEXT)''')
            conn.commit()

        resistance_json = json.dumps(resistance_cache)

        c.execute(
            "INSERT INTO crypto_data VALUES (NULL, ?, ?, ?, ?, ?, ?)",
            (data['last_updated'], json.dumps(data['daily_rising']),
             json.dumps(data['short_term_active']),
             json.dumps(data['all_cycle_rising']), 
             data['analysis_time'],
             resistance_json))
        conn.commit()
        conn.close()
        logger.info("ğŸ’¾ æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        try:
            init_db()
            save_to_db(data)
        except Exception:
            logger.critical("ğŸ”¥ æ— æ³•ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“")

# è·å–æœ€åæœ‰æ•ˆæ•°æ®
def get_last_valid_data():
    try:
        conn = sqlite3.connect('data.db')
        c = conn.cursor()

        c.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='crypto_data'"
        )
        if not c.fetchone():
            return None

        c.execute("SELECT * FROM crypto_data ORDER BY id DESC LIMIT 1")
        row = c.fetchone()
        conn.close()

        if row:
            resistance_data = json.loads(row[6]) if row[6] else {}
            for symbol, data in resistance_data.items():
                resistance_cache[symbol] = data

            return {
                'last_updated': row[1],
                'daily_rising': json.loads(row[2]),
                'short_term_active': json.loads(row[3]),
                'all_cycle_rising': json.loads(row[4]),
                'analysis_time': row[5]
            }
        return None
    except Exception as e:
        logger.error(f"âŒ è·å–æœ€åæœ‰æ•ˆæ•°æ®å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return None

# åˆå§‹åŒ–å®¢æˆ·ç«¯
def init_client():
    global client
    try:
        # å¦‚æœå·²ç»åˆå§‹åŒ–åˆ™ç›´æ¥è¿”å›
        if client:
            return True
            
        client_params = {
            'api_key': API_KEY, 
            'api_secret': API_SECRET,
            'requests_params': {'timeout': 30}
        }

        client = Client(**client_params)
        
        # æµ‹è¯•è¿æ¥
        server_time = client.get_server_time()
        logger.info(f"âœ… Binanceå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒæœåŠ¡å™¨æ—¶é—´: {datetime.fromtimestamp(server_time['serverTime']/1000)}")
        
        return True
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return False

# è®¡ç®—ä¸‹ä¸€ä¸ªæ›´æ–°æ—¶é—´ç‚¹
def get_next_update_time(period):
    minutes = PERIOD_MINUTES.get(period, 5)
    now = datetime.now(timezone.utc)

    if period.endswith('m'):
        current_minute = now.minute
        period_minutes = int(period[:-1])
        current_period_minute = (current_minute // period_minutes) * period_minutes
        current_period_start = now.replace(minute=current_period_minute,
                                           second=0,
                                           microsecond=0)
    elif period.endswith('h'):
        period_hours = int(period[:-1])
        current_hour = now.hour
        current_period_hour = (current_hour // period_hours) * period_hours
        current_period_start = now.replace(hour=current_period_hour,
                                           minute=0,
                                           second=0,
                                           microsecond=0)
    else:  # 1d
        current_period_start = now.replace(hour=0,
                                           minute=0,
                                           second=0,
                                           microsecond=0)

    next_update = current_period_start + timedelta(minutes=minutes)

    if next_update < now:
        next_update += timedelta(minutes=minutes)

    return next_update

# è·å–æŒä»“é‡æ•°æ®
def get_open_interest(symbol, period, use_cache=True):
    try:
        # éªŒè¯å¸ç§æ ¼å¼ - æ›´ä¸¥æ ¼çš„éªŒè¯
        if not re.match(r"^[A-Z]{3,15}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

        if period not in PERIOD_MINUTES:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å‘¨æœŸ: {period}, ä½¿ç”¨é»˜è®¤5m")
            period = '5m'

        current_time = datetime.now(timezone.utc)

        cache_key = f"{symbol}_{period}"
        if use_cache and oi_data_cache.get(cache_key):
            cached_data = oi_data_cache[cache_key]
            if 'next_update' in cached_data and cached_data['next_update'] > current_time:
                logger.debug(f"ğŸ“ˆ ä½¿ç”¨ç¼“å­˜æ•°æ®: {symbol} {period}")
                return {
                    'series': cached_data['data']['series'].copy(),
                    'timestamps': cached_data['data']['timestamps'].copy(),
                    'cache_time': cached_data['data']['cache_time']
                }

        url = "https://fapi.binance.com/futures/data/openInterestHist"
        params = {'symbol': symbol, 'period': period, 'limit': 30}

        logger.info(f"ğŸ“¡ è¯·æ±‚æŒä»“é‡æ•°æ®: {url}?symbol={symbol}&period={period}")
        response = requests.get(url, params=params, timeout=15)

        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code != 200:
            logger.error(f"âŒ è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: HTTP {response.status_code} - {response.text}")
            return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

        data = response.json()

        # æ£€æŸ¥è¿”å›çš„æ•°æ®æ ¼å¼
        if not isinstance(data, list):
            logger.error(f"âŒ æ— æ•ˆçš„æŒä»“é‡æ•°æ®æ ¼å¼: {symbol} {period} - å“åº”: {data}")
            return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

        if len(data) == 0:
            logger.warning(f"âš ï¸ {symbol}çš„{period}æŒä»“é‡æ•°æ®ä¸ºç©º")
            return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

        data.sort(key=lambda x: x['timestamp'])
        oi_series = [float(item['sumOpenInterest']) for item in data]
        timestamps = [item['timestamp'] for item in data]
        cache_time = datetime.now(timezone.utc).isoformat()

        # æ·»åŠ æ•°æ®éªŒè¯
        if len(oi_series) < 5:
            logger.warning(f"âš ï¸ {symbol}çš„{period}æŒä»“é‡æ•°æ®ä¸è¶³: åªæœ‰{len(oi_series)}ä¸ªç‚¹")
            return {'series': [], 'timestamps': [], 'cache_time': cache_time}

        oi_data = {
            'series': oi_series, 
            'timestamps': timestamps,
            'cache_time': cache_time
        }
        next_update = get_next_update_time(period)

        oi_data_cache[cache_key] = {
            'data': oi_data,
            'next_update': next_update
        }

        logger.info(f"ğŸ“ˆ è·å–æ–°æ•°æ®: {symbol} {period} ({len(oi_series)}ç‚¹)")
        return oi_data
    except Exception as e:
        logger.error(f"âŒ è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

# æ£€æŸ¥æŒä»“é‡æ˜¯å¦åˆ›æ–°é«˜
def is_latest_highest(oi_data):
    if len(oi_data) < 30:
        return False

    latest_value = oi_data[-1]
    prev_data = oi_data[-30:-1]

    if not prev_data:
        return False

    return latest_value > max(prev_data)

# è®¡ç®—é˜»åŠ›ä½
def calculate_resistance_levels(symbol):
    try:
        now = time.time()
        if symbol in resistance_cache:
            cache_data = resistance_cache[symbol]
            if cache_data['expiration'] > now:
                return cache_data['levels']

        # ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
        if not client:
            if not init_client():
                logger.error(f"âŒ æ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œæ— æ³•è·å–{symbol}çš„é˜»åŠ›ä½")
                return {}

        levels = {}

        for interval in RESISTANCE_INTERVALS:
            try:
                klines = client.futures_klines(symbol=symbol, interval=interval, limit=100)
                if not klines or len(klines) < 10:
                    logger.warning(f"âš ï¸ {symbol}åœ¨{interval}çš„Kçº¿æ•°æ®ä¸è¶³")
                    continue

                high_prices = [float(k[2]) for k in klines]
                low_prices = [float(k[3]) for k in klines]
                recent_high = max(high_prices[-10:])
                recent_low = min(low_prices[-10:])

                lookback = min(30, len(high_prices))
                recent_max = max(high_prices[-lookback:])
                recent_min = min(low_prices[-lookback:])

                if recent_max <= recent_min:
                    logger.warning(f"âš ï¸ {symbol}åœ¨{interval}çš„æœ€è¿‘é«˜ç‚¹å’Œä½ç‚¹æ— æ•ˆ")
                    continue

                fib_levels = {
                    '0.236': recent_max - (recent_max - recent_min) * 0.236,
                    '0.382': recent_max - (recent_max - recent_min) * 0.382,
                    '0.5': recent_max - (recent_max - recent_min) * 0.5,
                    '0.618': recent_max - (recent_max - recent_min) * 0.618,
                    '0.786': recent_max - (recent_max - recent_min) * 0.786,
                    '1.0': recent_max
                }

                price_levels = [recent_high, recent_low]
                price_levels.extend(fib_levels.values())

                base = 10 ** (math.floor(math.log10(recent_high)) - 1)
                integer_level = round(recent_high / base) * base
                price_levels.append(integer_level)

                price_levels = sorted(set(price_levels), reverse=True)

                resistance = [p for p in price_levels if p > recent_min and p <= recent_max]
                support = [p for p in price_levels if p < recent_min or p > recent_max]

                levels[interval] = {
                    'resistance': resistance[:3],
                    'support': support[:3]
                }
            except Exception as e:
                logger.error(f"è®¡ç®—{symbol}åœ¨{interval}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
                logger.error(traceback.format_exc())
                levels[interval] = {
                    'resistance': [],
                    'support': []
                }

        resistance_cache[symbol] = {
            'levels': levels,
            'expiration': now + RESISTANCE_CACHE_EXPIRATION
        }
        return levels
    except Exception as e:
        logger.error(f"è®¡ç®—{symbol}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return {}

# åˆ†æå•ä¸ªå¸ç§è¶‹åŠ¿
def analyze_symbol(symbol):
    try:
        symbol_result = {
            'symbol': symbol,
            'daily_rising': None,
            'short_term_active': None,
            'all_cycle_rising': None,
            'rising_periods': [],
            'period_status': {p: False for p in ALL_PERIODS},
            'period_count': 0,
            'oi_data': {}  # å­˜å‚¨æ‰€æœ‰å‘¨æœŸçš„æŒä»“é‡æ•°æ®
        }

        # 1. è·å–æ—¥çº¿æŒä»“é‡æ•°æ®
        daily_oi = get_open_interest(symbol, '1d', use_cache=True)
        symbol_result['oi_data']['1d'] = daily_oi
        daily_series = daily_oi['series']

        # 2. æ£€æŸ¥æ—¥çº¿ä¸Šæ¶¨æ¡ä»¶
        if len(daily_series) >= 30:
            daily_up = is_latest_highest(daily_series)

            if daily_up:
                daily_change = ((daily_series[-1] - daily_series[-30]) / daily_series[-30]) * 100
                symbol_result['daily_rising'] = {
                    'symbol': symbol,
                    'oi': daily_series[-1],
                    'change': round(daily_change, 2),
                    'period_count': 1
                }
                symbol_result['rising_periods'].append('1d')
                symbol_result['period_status']['1d'] = True
                symbol_result['period_count'] = 1

                # 3. å…¨å‘¨æœŸåˆ†æ
                all_intervals_up = True
                for period in ALL_PERIODS:
                    if period == '1d':
                        continue

                    oi_data = get_open_interest(symbol, period, use_cache=True)
                    symbol_result['oi_data'][period] = oi_data
                    oi_series = oi_data['series']
                    if len(oi_series) < 30:
                        status = False
                    else:
                        status = is_latest_highest(oi_series)
                    symbol_result['period_status'][period] = status

                    if status:
                        symbol_result['rising_periods'].append(period)
                        symbol_result['period_count'] += 1
                    else:
                        all_intervals_up = False

                if all_intervals_up:
                    symbol_result['all_cycle_rising'] = {
                        'symbol': symbol,
                        'oi': daily_series[-1],
                        'change': round(daily_change, 2),
                        'periods': symbol_result['rising_periods'],
                        'period_status': symbol_result['period_status'],
                        'period_count': symbol_result['period_count']
                    }

        # 4. çŸ­æœŸæ´»è·ƒåº¦åˆ†æ
        min5_oi = get_open_interest(symbol, '5m', use_cache=True)
        symbol_result['oi_data']['5m'] = min5_oi
        min5_series = min5_oi['series']
        if len(min5_series) >= 30 and len(daily_series) >= 30:
            min5_max = max(min5_series[-30:])
            daily_avg = sum(daily_series[-30:]) / 30

            if min5_max > 0 and daily_avg > 0:
                ratio = min5_max / daily_avg
                if ratio > 1.5:
                    symbol_result['short_term_active'] = {
                        'symbol': symbol,
                        'oi': min5_series[-1],
                        'ratio': round(ratio, 2),
                        'period_status': symbol_result['period_status'],
                        'period_count': symbol_result['period_count']
                    }

        if symbol_result['daily_rising']:
            symbol_result['daily_rising']['period_status'] = symbol_result['period_status']
            symbol_result['daily_rising']['period_count'] = symbol_result['period_count']

        return symbol_result
    except Exception as e:
        logger.error(f"âŒ å¤„ç†{symbol}æ—¶å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'symbol': symbol,
            'period_status': {p: False for p in ALL_PERIODS},
            'period_count': 0,
            'oi_data': {}
        }

# åˆ†æå¸ç§è¶‹åŠ¿
def analyze_trends():
    start_time = time.time()
    symbols = get_high_volume_symbols()
    if not symbols:
        logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°é«˜äº¤æ˜“é‡å¸ç§")
        return data_cache

    logger.info(f"ğŸ” å¼€å§‹åˆ†æ {len(symbols)} ä¸ªå¸ç§")

    daily_rising = []
    short_term_active = []
    all_cycle_rising = []

    futures = {
        executor.submit(analyze_symbol, symbol): symbol
        for symbol in symbols
    }
    processed = 0
    total_symbols = len(symbols)

    for future in as_completed(futures):
        processed += 1
        symbol = futures[future]

        try:
            result = future.result()
            if result.get('daily_rising'):
                daily_rising.append(result['daily_rising'])
            if result.get('short_term_active'):
                short_term_active.append(result['short_term_active'])
            if result.get('all_cycle_rising'):
                all_cycle_rising.append(result['all_cycle_rising'])
        except Exception as e:
            logger.error(f"âŒ å¤„ç†{symbol}æ—¶å‡ºé”™: {str(e)}")

        if processed % max(1, total_symbols // 10) == 0 or processed == total_symbols:
            logger.info(f"â³ åˆ†æè¿›åº¦: {processed}/{total_symbols} ({int(processed/total_symbols*100)}%)")

    daily_rising.sort(key=lambda x: (x.get('period_count', 0), x.get('change', 0)), reverse=True)
    short_term_active.sort(key=lambda x: (x.get('period_count', 0), x.get('ratio', 0)), reverse=True)
    all_cycle_rising.sort(key=lambda x: (x.get('period_count', 0), x.get('change', 0)), reverse=True)

    logger.info(f"ğŸ“Š åˆ†æç»“æœ: æ—¥çº¿ä¸Šæ¶¨ {len(daily_rising)}ä¸ª, çŸ­æœŸæ´»è·ƒ {len(short_term_active)}ä¸ª, å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨ {len(all_cycle_rising)}ä¸ª")
    analysis_time = time.time() - start_time
    logger.info(f"âœ… åˆ†æå®Œæˆ: ç”¨æ—¶{analysis_time:.2f}ç§’")

    return {
        'daily_rising': daily_rising,
        'short_term_active': short_term_active,
        'all_cycle_rising': all_cycle_rising,
        'analysis_time': analysis_time
    }

# è·å–é«˜äº¤æ˜“é‡å¸ç§
def get_high_volume_symbols():
    if not client:
        if not init_client():
            return []

    try:
        tickers = client.futures_ticker()
        filtered = [
            t for t in tickers if float(t.get('quoteVolume', 0)) > 10000000
            and t.get('symbol', '').endswith('USDT')
        ]
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(filtered)} ä¸ªé«˜äº¤æ˜“é‡å¸ç§")
        return [t['symbol'] for t in filtered]
    except Exception as e:
        logger.error(f"âŒ è·å–é«˜äº¤æ˜“é‡å¸ç§å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return []

# æ•°æ®åˆ†æå·¥ä½œçº¿ç¨‹
def analysis_worker():
    global data_cache, current_data_cache
    logger.info("ğŸ”§ æ•°æ®åˆ†æçº¿ç¨‹å¯åŠ¨")
    init_db()

    initial_data = get_last_valid_data()
    if initial_data:
        data_cache = initial_data
        current_data_cache = data_cache.copy()
        logger.info(f"ğŸ” åŠ è½½å†å²æ•°æ®")
    else:
        logger.info("ğŸ†• æ²¡æœ‰å†å²æ•°æ®ï¼Œå°†è¿›è¡Œé¦–æ¬¡åˆ†æ")

    while True:
        try:
            task = analysis_queue.get()
            if task == "STOP":
                break

            analysis_start = datetime.now(timezone.utc)
            logger.info(f"â±ï¸ å¼€å§‹æ›´æ–°æ•°æ® ({analysis_start.strftime('%H:%M:%S')})...")

            backup_cache = data_cache.copy()
            current_backup = current_data_cache.copy()

            try:
                result = analyze_trends()
                new_data = {
                    "last_updated": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"),
                    "daily_rising": result['daily_rising'],
                    "short_term_active": result['short_term_active'],
                    "all_cycle_rising": result['all_cycle_rising'],
                    "analysis_time": result['analysis_time']
                }

                data_cache = new_data
                save_to_db(new_data)
                current_data_cache = new_data.copy()
                logger.info(f"âœ… æ•°æ®æ›´æ–°æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
                data_cache = backup_cache
                current_data_cache = current_backup
                logger.info("ğŸ”„ æ¢å¤å†å²æ•°æ®")

            analysis_end = datetime.now(timezone.utc)
            analysis_duration = (analysis_end - analysis_start).total_seconds()
            logger.info(f"â±ï¸ åˆ†æè€—æ—¶: {analysis_duration:.2f}ç§’")
            logger.info("=" * 50)

        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
        analysis_queue.task_done()

# å®šæ—¶è§¦å‘æ•°æ®åˆ†æ
def schedule_analysis():
    now = datetime.now(timezone.utc)
    current_minute = now.minute
    next_minute = ((current_minute // 5) + 1) * 5
    if next_minute >= 60:
        next_minute = 0
        next_time = now.replace(hour=now.hour + 1, minute=next_minute, second=0, microsecond=0)
    else:
        next_time = now.replace(minute=next_minute, second=0, microsecond=0)

    initial_wait = (next_time - now).total_seconds()
    logger.info(f"â³ é¦–æ¬¡åˆ†æå°†åœ¨ {initial_wait:.1f} ç§’åå¼€å§‹ ({next_time.strftime('%H:%M:%S')})...")
    time.sleep(initial_wait)

    while True:
        analysis_start = datetime.now(timezone.utc)
        analysis_queue.put("ANALYZE")
        analysis_queue.join()

        analysis_duration = (datetime.now(timezone.utc) - analysis_start).total_seconds()
        now = datetime.now(timezone.utc)
        current_minute = now.minute
        next_minute = ((current_minute // 5) + 1) * 5
        if next_minute >= 60:
            next_minute = 0
            next_time = now.replace(hour=now.hour + 1, minute=next_minute, second=0, microsecond=0)
        else:
            next_time = now.replace(minute=next_minute, second=0, microsecond=0)

        wait_time = (next_time - now).total_seconds()

        if wait_time < 0:
            wait_time = 0
        elif analysis_duration > 240:
            wait_time = 0
            logger.warning("âš ï¸ åˆ†æè€—æ—¶è¿‡é•¿ï¼Œç«‹å³å¼€å§‹ä¸‹ä¸€æ¬¡åˆ†æ")
        elif wait_time > 300:
            adjusted_wait = max(60, wait_time - 120)
            logger.info(f"â³ è°ƒæ•´ç­‰å¾…æ—¶é—´: {wait_time:.1f}ç§’ -> {adjusted_wait:.1f}ç§’")
            wait_time = adjusted_wait

        logger.info(f"â³ ä¸‹æ¬¡åˆ†æå°†åœ¨ {wait_time:.1f} ç§’å ({next_time.strftime('%H:%M:%S')}")
        time.sleep(wait_time)

# APIè·¯ç”±
@app.route('/')
def index():
    try:
        static_path = app.static_folder
        if static_path is None:
            return "é™æ€æ–‡ä»¶è·¯å¾„æœªé…ç½®", 500

        index_path = os.path.join(static_path, 'index.html')
        if not os.path.exists(index_path):
            return "index.html not found", 404
        return send_from_directory(static_path, 'index.html', mimetype='text/html')
    except Exception as e:
        logger.error(f"âŒ å¤„ç†é¦–é¡µè¯·æ±‚å¤±è´¥: {str(e)}")
        return "Internal Server Error", 500

@app.route('/static/<path:filename>')
def static_files(filename):
    try:
        static_path = app.static_folder
        if static_path is None:
            return "é™æ€æ–‡ä»¶è·¯å¾„æœªé…ç½®", 500

        return send_from_directory(static_path, filename)
    except Exception as e:
        logger.error(f"âŒ å¤„ç†é™æ€æ–‡ä»¶è¯·æ±‚å¤±è´¥: {str(e)}")
        return "File not found", 404

@app.route('/api/data', methods=['GET'])
def get_data():
    global current_data_cache
    try:
        return jsonify({
            'last_updated': current_data_cache['last_updated'] or "",
            'daily_rising': current_data_cache['daily_rising'] or [],
            'short_term_active': current_data_cache['short_term_active'] or [],
            'all_cycle_rising': current_data_cache['all_cycle_rising'] or [],
            'analysis_time': current_data_cache.get('analysis_time', 0)
        })
    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
        last_data = get_last_valid_data()
        if last_data:
            return jsonify(last_data)
        return jsonify({'error': str(e)}), 500

@app.route('/api/oi_chart/<symbol>/<period>', methods=['GET'])
def get_oi_chart(symbol, period):
    try:
        # ä¸¥æ ¼éªŒè¯å¸ç§æ ¼å¼
        if not re.match(r"^[A-Z]{3,15}USDT$", symbol):
            logger.error(f"âŒ æ— æ•ˆçš„å¸ç§åç§°æ ¼å¼: {symbol}")
            return jsonify({'error': 'Invalid symbol format'}), 400

        # éªŒè¯å‘¨æœŸ
        if period not in PERIOD_MINUTES:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å‘¨æœŸ: {period}, ä½¿ç”¨5m")
            period = '5m'

        oi_data = get_open_interest(symbol, period, use_cache=True)

        # æ›´å¥½çš„æ•°æ®éªŒè¯
        if not oi_data or 'series' not in oi_data or 'timestamps' not in oi_data:
            logger.error(f"âŒ æ— æ•ˆçš„æŒä»“é‡æ•°æ®ç»“æ„: {symbol} {period}")
            return jsonify({'error': 'Invalid data structure'}), 500

        if len(oi_data['series']) == 0 or len(oi_data['timestamps']) == 0:
            logger.warning(f"âš ï¸ æ— æœ‰æ•ˆæŒä»“é‡æ•°æ®: {symbol} {period}")
            return jsonify({'error': 'No data available'}), 404

        # æ›´å¥½çš„æ—¶é—´æˆ³å¤„ç†
        timestamps = []
        for ts in oi_data['timestamps']:
            try:
                dt = datetime.fromtimestamp(ts / 1000, timezone.utc)
                timestamps.append(dt.strftime('%m-%d %H:%M'))
            except Exception as e:
                logger.warning(f"æ—¶é—´æˆ³è½¬æ¢å¤±è´¥: {ts} - {str(e)}")
                continue

        # ç¡®ä¿æ—¶é—´æˆ³å’Œç³»åˆ—é•¿åº¦ä¸€è‡´
        if len(timestamps) != len(oi_data['series']):
            logger.error(f"âŒ æ•°æ®é•¿åº¦ä¸åŒ¹é…: {len(timestamps)}æ—¶é—´æˆ³ vs {len(oi_data['series'])}æ•°æ®ç‚¹")
            return jsonify({'error': 'Data length mismatch'}), 500

        return jsonify({
            'symbol': symbol,
            'period': period,
            'data': oi_data['series'],
            'labels': timestamps,
            'cache_time': oi_data.get('cache_time', datetime.now(timezone.utc).isoformat())
        })
    except Exception as e:
        logger.error(f"âŒ è·å–æŒä»“é‡å›¾è¡¨å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': 'Internal server error'}), 500

@app.route('/api/kline/<symbol>', methods=['GET'])
def get_kline(symbol):
    # ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
    if not client:
        if not init_client():
            logger.error("æ— æ³•è¿æ¥API")
            return jsonify({'error': 'æ— æ³•è¿æ¥API'}), 500

    try:
        if not symbol or not isinstance(symbol, str) or not symbol.endswith('USDT'):
            return jsonify({'error': 'Invalid symbol format'}), 400

        klines = client.futures_klines(
            symbol=symbol, 
            interval='5m', 
            limit=30,
            timeout=10
        )

        if not klines or len(klines) < 10:
            return jsonify({'error': 'Insufficient kline data'}), 404

        data = []
        for k in klines:
            try:
                data.append({
                    'time': k[0],
                    'open': float(k[1]),
                    'high': float(k[2]),
                    'low': float(k[3]),
                    'close': float(k[4])
                })
            except Exception:
                continue

        return jsonify({'symbol': symbol, 'data': data})
    except Exception as e:
        logger.error(f"âŒ è·å–Kçº¿å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

# ä¿®æ”¹é˜»åŠ›ä½æ¥å£ä»¥æ”¯æŒç®€åŒ–å¸ç§åç§°
@app.route('/api/resistance_levels/<symbol>', methods=['GET'])
def get_resistance_levels(symbol):
    try:
        # æ£€æŸ¥æ˜¯å¦ä¼ å…¥ç®€åŒ–å¸ç§åç§°ï¼ˆä¸å¸¦USDTåç¼€ï¼‰
        if not symbol.endswith('USDT') and len(symbol) <= 5:
            # è‡ªåŠ¨æ·»åŠ USDTåç¼€
            symbol = symbol.upper() + 'USDT'

        # éªŒè¯å¸ç§æ ¼å¼
        if not re.match(r"^[A-Z]{3,15}USDT$", symbol):
            return jsonify({'error': 'Invalid symbol format'}), 400

        # è·å–é˜»åŠ›ä½æ•°æ®
        levels = calculate_resistance_levels(symbol)

        # å¦‚æœè·å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸºç¡€å¸ç§åç§°
        if not levels:
            base_symbol = symbol.replace('USDT', '')
            logger.warning(f"å°è¯•ä½¿ç”¨åŸºç¡€å¸ç§åç§°: {base_symbol}")
            levels = calculate_resistance_levels(base_symbol + 'USDT')

        if not levels:
            return jsonify({'error': 'No resistance levels available'}), 404

        return jsonify(levels)
    except Exception as e:
        logger.error(f"âŒ è·å–é˜»åŠ›ä½å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

# æ–°å¢é˜»åŠ›ä½æŸ¥è¯¢APIï¼ˆæ”¯æŒPOSTå’Œç®€åŒ–åç§°ï¼‰
@app.route('/api/query_resistance_levels', methods=['POST'])
def query_resistance_levels():
    try:
        data = request.get_json()
        if not data or 'symbol' not in data:
            return jsonify({'error': 'Missing symbol parameter'}), 400

        symbol = data['symbol'].strip().upper()

        # å¦‚æœè¾“å…¥çš„æ˜¯ç®€åŒ–å¸ç§åç§°ï¼ˆä¸å¸¦USDTåç¼€ï¼‰
        if not symbol.endswith('USDT'):
            # è‡ªåŠ¨æ·»åŠ USDTåç¼€
            symbol += 'USDT'

        # éªŒè¯å¸ç§æ ¼å¼
        if not re.match(r"^[A-Z]{3,15}USDT$", symbol):
            return jsonify({'error': 'Invalid symbol format'}), 400

        # è·å–é˜»åŠ›ä½æ•°æ®
        levels = calculate_resistance_levels(symbol)

        # å¦‚æœè·å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸºç¡€å¸ç§åç§°
        if not levels:
            base_symbol = symbol.replace('USDT', '')
            logger.warning(f"å°è¯•ä½¿ç”¨åŸºç¡€å¸ç§åç§°: {base_symbol}")
            levels = calculate_resistance_levels(base_symbol + 'USDT')

        if not levels:
            return jsonify({'error': 'No resistance levels available'}), 404

        return jsonify({
            'symbol': symbol,
            'levels': levels
        })
    except Exception as e:
        logger.error(f"âŒ é˜»åŠ›ä½æŸ¥è¯¢å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': 'Internal server error'}), 500

@app.route('/test')
def test():
    return 'âœ… æœåŠ¡æ­£å¸¸è¿è¡Œ!'

def start_background_threads():
    # ç¡®ä¿é™æ€æ–‡ä»¶å¤¹å­˜åœ¨
    static_path = app.static_folder
    if not os.path.exists(static_path):
        os.makedirs(static_path)
    
    # ç¡®ä¿ index.html å­˜åœ¨
    index_path = os.path.join(static_path, 'index.html')
    if not os.path.exists(index_path):
        with open(index_path, 'w') as f:
            f.write("<html><body><h1>è¯·å°†å‰ç«¯æ–‡ä»¶æ”¾å…¥staticç›®å½•</h1></body></html>")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    if not init_client():
        logger.critical("âŒ æ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯")
        return False
    
    # å¯åŠ¨åå°çº¿ç¨‹
    worker_thread = threading.Thread(target=analysis_worker)
    worker_thread.daemon = True
    worker_thread.start()
    
    scheduler_thread = threading.Thread(target=schedule_analysis)
    scheduler_thread.daemon = True
    scheduler_thread.start()
    
    return True

if __name__ == '__main__':
    # è·å–ç«¯å£
    PORT = int(os.environ.get("PORT", 9600))
    
    # å¯åŠ¨åå°çº¿ç¨‹
    if start_background_threads():
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡å™¨...")
        app.run(host='0.0.0.0', port=PORT, debug=False)
    else:
        logger.error("âŒ æœåŠ¡å¯åŠ¨å¤±è´¥")
