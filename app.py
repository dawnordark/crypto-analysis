#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import time
import re
import json
import math
import requests
import threading
import queue
import logging
import urllib3
import asyncio
import aiohttp
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import Flask, jsonify, send_from_directory

# æ£€æŸ¥æ˜¯å¦åœ¨ Render ç¯å¢ƒ
IS_RENDER = os.environ.get('RENDER', False)

# ç®€åŒ–æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
app = Flask(__name__, static_folder=os.path.join(BASE_DIR, 'static'), static_url_path='/')

# å»¶è¿Ÿå¯¼å…¥å¯èƒ½è€—æ—¶çš„æ¨¡å—
try:
    from binance.client import Client
    from flask_cors import CORS
    CORS(app)
    BINANCE_AVAILABLE = True
except ImportError as e:
    logger.warning(f"âŒ éƒ¨åˆ†ä¾èµ–ä¸å¯ç”¨: {e}")
    BINANCE_AVAILABLE = False

# Binance API é…ç½®
API_KEY = os.environ.get('BINANCE_API_KEY', '')
API_SECRET = os.environ.get('BINANCE_API_SECRET', '')

# é‚®ä»¶é…ç½®
SMTP_SERVER = os.environ.get('SMTP_SERVER', 'smtp.qq.com')
SMTP_PORT = int(os.environ.get('SMTP_PORT', '587'))
EMAIL_USER = os.environ.get('EMAIL_USER', '')
EMAIL_PASSWORD = os.environ.get('EMAIL_PASSWORD', '')
EMAIL_RECEIVERS = os.environ.get('EMAIL_RECEIVERS', '').split(',')  # å¤šä¸ªé‚®ç®±ç”¨é€—å·åˆ†éš”

client = None

# æ•°æ®ç¼“å­˜
data_cache = {
    "last_updated": "ä»æœªæ›´æ–°",
    "daily_rising": [],
    "short_term_active": [],
    "all_cycle_rising": [],
    "analysis_time": 0,
    "next_analysis_time": "è®¡ç®—ä¸­..."
}

current_data_cache = data_cache.copy()
oi_data_cache = {}
resistance_cache = {}
RESISTANCE_CACHE_EXPIRATION = 24 * 3600
OI_CACHE_EXPIRATION = 5 * 60

# ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œçº¿ç¨‹é—´é€šä¿¡
analysis_queue = queue.Queue()
executor = ThreadPoolExecutor(max_workers=5)

# å‘¨æœŸé…ç½®
PERIOD_MINUTES = {
    '5m': 5, '15m': 15, '30m': 30, '1h': 60, '2h': 120,
    '4h': 240, '6h': 360, '12h': 720, '1d': 1440
}

VALID_PERIODS = ['5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h', '1d']
RESISTANCE_INTERVALS = ['1m', '15m', '1d']

# åˆ†æçº¿ç¨‹çŠ¶æ€
analysis_thread_running = False
analysis_thread = None

# å¼‚æ­¥HTTPå®¢æˆ·ç«¯ä¼šè¯
aio_session = None

def init_client():
    """åˆå§‹åŒ– Binance å®¢æˆ·ç«¯"""
    global client
    
    if not BINANCE_AVAILABLE:
        logger.error("âŒ Binance å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œç¼ºå°‘ä¾èµ–")
        return False
        
    if not API_KEY or not API_SECRET:
        logger.error("âŒ Binance APIå¯†é’¥æœªè®¾ç½®")
        return False
    
    max_retries = 3
    retry_delay = 5
    
    for attempt in range(max_retries):
        try:
            logger.info(f"ğŸ”§ å°è¯•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ (ç¬¬{attempt+1}æ¬¡)...")
            client = Client(
                api_key=API_KEY, 
                api_secret=API_SECRET,
                requests_params={'timeout': 20}
            )
            
            server_time = client.get_server_time()
            logger.info(f"âœ… Binanceå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            if attempt < max_retries - 1:
                logger.info(f"ğŸ”„ {retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
    
    logger.critical("ğŸ”¥ æ— æ³•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯")
    return False

def send_email_notification(all_cycle_rising_coins):
    """å‘é€é‚®ä»¶æé†’"""
    try:
        if not EMAIL_USER or not EMAIL_PASSWORD or not EMAIL_RECEIVERS:
            logger.warning("âš ï¸ é‚®ä»¶é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡å‘é€é‚®ä»¶")
            return False
        
        if not all_cycle_rising_coins:
            logger.info("ğŸ“§ æ²¡æœ‰å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨çš„å¸ç§ï¼Œä¸å‘é€é‚®ä»¶")
            return False
        
        # å‡†å¤‡é‚®ä»¶å†…å®¹
        subject = "å‡ºç°å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¸ç§"
        
        # æ„å»ºå¸ç§åˆ—è¡¨
        coin_list = "\n".join([f"â€¢ {coin['symbol']}" for coin in all_cycle_rising_coins])
        
        body = f"""
å‘ç° {len(all_cycle_rising_coins)} ä¸ªå…¨éƒ¨å‘¨æœŸä¸Šæ¶¨çš„å¸ç§ï¼š

{coin_list}

åˆ†ææ—¶é—´: {datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S")}
        
è¯·æ³¨æ„ï¼šæ­¤æé†’ä»…åŸºäºæŒä»“é‡åˆ†æï¼ŒæŠ•èµ„æœ‰é£é™©ï¼Œè¯·è°¨æ…å†³ç­–ã€‚
        """
        
        # åˆ›å»ºé‚®ä»¶
        msg = MIMEMultipart()
        msg['From'] = EMAIL_USER
        msg['To'] = ", ".join(EMAIL_RECEIVERS)
        msg['Subject'] = subject
        
        msg.attach(MIMEText(body, 'plain', 'utf-8'))
        
        # å‘é€é‚®ä»¶
        server = None
        try:
            if SMTP_PORT == 587:
                # TLS è¿æ¥
                server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
                server.starttls()
            elif SMTP_PORT == 465:
                # SSL è¿æ¥
                server = smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT)
            else:
                server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
            
            server.login(EMAIL_USER, EMAIL_PASSWORD)
            server.sendmail(EMAIL_USER, EMAIL_RECEIVERS, msg.as_string())
            server.quit()
            
            logger.info(f"âœ… é‚®ä»¶å‘é€æˆåŠŸï¼Œæ”¶ä»¶äºº: {len(EMAIL_RECEIVERS)} ä¸ª")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
            if server:
                server.quit()
            return False
            
    except Exception as e:
        logger.error(f"âŒ é‚®ä»¶é€šçŸ¥å¤„ç†å¤±è´¥: {str(e)}")
        return False

async def init_aio_session():
    """åˆå§‹åŒ–å¼‚æ­¥HTTPä¼šè¯"""
    global aio_session
    if aio_session is None:
        timeout = aiohttp.ClientTimeout(total=30)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)
        aio_session = aiohttp.ClientSession(timeout=timeout, connector=connector)
    return aio_session

async def close_aio_session():
    """å…³é—­å¼‚æ­¥HTTPä¼šè¯"""
    global aio_session
    if aio_session:
        await aio_session.close()
        aio_session = None

async def get_open_interest_async(symbol, period, use_cache=True):
    """å¼‚æ­¥è·å–æŒä»“é‡æ•°æ®"""
    try:
        if not re.match(r"^[A-Z0-9]{1,10}USDT$", symbol):
            return {'series': [], 'timestamps': []}

        if period not in VALID_PERIODS:
            return {'series': [], 'timestamps': []}
        
        current_time = datetime.now(timezone.utc)
        cache_key = f"{symbol}_{period}"
        
        if use_cache and cache_key in oi_data_cache:
            cached_data = oi_data_cache[cache_key]
            if 'expiration' in cached_data and cached_data['expiration'] > current_time:
                return cached_data['data']

        logger.debug(f"ğŸ“¡ å¼‚æ­¥è¯·æ±‚æŒä»“é‡æ•°æ®: {symbol} {period}")
        url = "https://fapi.binance.com/futures/data/openInterestHist"
        params = {'symbol': symbol, 'period': period, 'limit': 30}

        session = await init_aio_session()
        
        async with session.get(url, params=params, headers={'X-MBX-APIKEY': API_KEY} if API_KEY else {}) as response:
            if response.status != 200:
                return {'series': [], 'timestamps': []}

            data = await response.json()
            
            if not isinstance(data, list) or len(data) == 0:
                return {'series': [], 'timestamps': []}
                
            data.sort(key=lambda x: x['timestamp'])
            oi_series = [float(item['sumOpenInterest']) for item in data]
            timestamps = [item['timestamp'] for item in data]

            if len(oi_series) < 30:
                return {'series': [], 'timestamps': []}
                
            oi_data = {
                'series': oi_series, 
                'timestamps': timestamps
            }
            
            expiration = current_time + timedelta(seconds=OI_CACHE_EXPIRATION)
            oi_data_cache[cache_key] = {
                'data': oi_data,
                'expiration': expiration
            }

            return oi_data
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: {str(e)}")
        return {'series': [], 'timestamps': []}

def get_next_analysis_time():
    """è®¡ç®—ä¸‹æ¬¡åˆ†ææ—¶é—´ï¼ˆ5åˆ†é’Ÿå‘¨æœŸ+45ç§’å»¶è¿Ÿï¼‰"""
    tz_shanghai = timezone(timedelta(hours=8))
    now = datetime.now(tz_shanghai)
    
    # è®¡ç®—å½“å‰5åˆ†é’Ÿå‘¨æœŸçš„å¼€å§‹æ—¶é—´
    current_minute = now.minute
    current_period_minute = (current_minute // 5) * 5
    
    # å½“å‰5åˆ†é’Ÿå‘¨æœŸçš„ç»“æŸæ—¶é—´
    current_period_end = now.replace(minute=current_period_minute, second=0, microsecond=0) + timedelta(minutes=5)
    
    # ä¸‹æ¬¡åˆ†ææ—¶é—´ = å½“å‰å‘¨æœŸç»“æŸæ—¶é—´ + 45ç§’å»¶è¿Ÿ
    next_analysis = current_period_end + timedelta(seconds=45)
    
    # å¦‚æœä¸‹æ¬¡åˆ†ææ—¶é—´å·²ç»è¿‡å»ï¼Œåˆ™è®¡ç®—ä¸‹ä¸€ä¸ªå‘¨æœŸ
    if next_analysis <= now:
        next_analysis = current_period_end + timedelta(minutes=5, seconds=45)
    
    return next_analysis

def is_latest_highest(oi_series):
    """æ£€æŸ¥æ˜¯å¦ä¸ºè¿‘æœŸæœ€é«˜ç‚¹"""
    if not oi_series or len(oi_series) < 30:
        return False

    latest_value = oi_series[-1]
    prev_data = oi_series[-30:-1]
    
    return latest_value > max(prev_data) if prev_data else False

async def analyze_short_term_active_batch(symbols_batch):
    """æ‰¹é‡åˆ†æçŸ­æœŸæ´»è·ƒå¸ç§"""
    tasks = []
    for symbol in symbols_batch:
        tasks.append(analyze_short_term_active(symbol))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    short_term_active = []
    for result in results:
        if isinstance(result, dict) and result:
            short_term_active.append(result)
        elif isinstance(result, Exception):
            logger.error(f"âŒ çŸ­æœŸæ´»è·ƒåˆ†æå¼‚å¸¸: {result}")
    
    return short_term_active

async def analyze_short_term_active(symbol):
    """åˆ†æçŸ­æœŸæ´»è·ƒå¸ç§"""
    try:
        # å¹¶è¡Œè·å–5åˆ†é’Ÿå’Œæ—¥çº¿æ•°æ®
        min5_task = get_open_interest_async(symbol, '5m')
        daily_task = get_open_interest_async(symbol, '1d')
        
        min5_oi, daily_oi = await asyncio.gather(min5_task, daily_task)
        
        min5_series = min5_oi.get('series', [])
        daily_series = daily_oi.get('series', [])
        
        if len(min5_series) >= 30 and len(daily_series) >= 30:
            min5_max = max(min5_series[-30:])
            daily_avg = sum(daily_series[-30:]) / 30
            
            if min5_max > 0 and daily_avg > 0:
                ratio = min5_max / daily_avg
                
                if ratio > 1.5:
                    return {
                        'symbol': symbol,
                        'oi': min5_series[-1],
                        'ratio': round(ratio, 2)
                    }
        return None
    except Exception as e:
        logger.error(f"âŒ åˆ†æ{symbol}çŸ­æœŸæ´»è·ƒå¤±è´¥: {str(e)}")
        return None

async def analyze_daily_rising_batch(symbols_batch):
    """æ‰¹é‡åˆ†ææ—¥çº¿ä¸Šæ¶¨å¸ç§"""
    tasks = []
    for symbol in symbols_batch:
        tasks.append(analyze_daily_rising(symbol))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    daily_rising = []
    for result in results:
        if isinstance(result, dict) and result:
            daily_rising.append(result)
        elif isinstance(result, Exception):
            logger.error(f"âŒ æ—¥çº¿ä¸Šæ¶¨åˆ†æå¼‚å¸¸: {result}")
    
    return daily_rising

async def analyze_daily_rising(symbol):
    """åˆ†ææ—¥çº¿ä¸Šæ¶¨å¸ç§"""
    try:
        daily_oi = await get_open_interest_async(symbol, '1d')
        daily_series = daily_oi.get('series', [])
        
        if len(daily_series) >= 30:
            daily_status = is_latest_highest(daily_series)
            
            if daily_status:
                daily_change = ((daily_series[-1] - daily_series[-30]) / daily_series[-30]) * 100
                
                return {
                    'symbol': symbol,
                    'oi': daily_series[-1],
                    'change': round(daily_change, 2),
                    'period_status': {'1d': True},
                    'period_count': 1
                }
        return None
    except Exception as e:
        logger.error(f"âŒ åˆ†æ{symbol}æ—¥çº¿ä¸Šæ¶¨å¤±è´¥: {str(e)}")
        return None

async def analyze_all_cycle_rising_batch(daily_results_batch):
    """æ‰¹é‡åˆ†æå…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¸ç§"""
    tasks = []
    for daily_data in daily_results_batch:
        tasks.append(analyze_all_cycle_rising(daily_data['symbol'], daily_data))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    all_cycle_rising = []
    for result in results:
        if isinstance(result, dict) and result:
            all_cycle_rising.append(result)
        elif isinstance(result, Exception):
            logger.error(f"âŒ å…¨éƒ¨å‘¨æœŸåˆ†æå¼‚å¸¸: {result}")
    
    return all_cycle_rising

async def analyze_all_cycle_rising(symbol, daily_data):
    """åˆ†æå…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¸ç§ï¼ˆåŸºäºæ—¥çº¿ä¸Šæ¶¨å¸ç§ï¼‰"""
    try:
        if not daily_data:
            return None
            
        period_status = daily_data['period_status'].copy()
        period_count = daily_data['period_count']
        all_intervals_up = True
        
        # å¹¶è¡Œè·å–æ‰€æœ‰å‘¨æœŸçš„æ•°æ®
        tasks = []
        periods_to_analyze = [p for p in VALID_PERIODS if p != '1d']
        
        for period in periods_to_analyze:
            tasks.append(get_open_interest_async(symbol, period))
        
        oi_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, period in enumerate(periods_to_analyze):
            oi_result = oi_results[i]
            if isinstance(oi_result, Exception):
                logger.error(f"âŒ è·å–{symbol}çš„{period}æ•°æ®å¤±è´¥: {oi_result}")
                status = False
            else:
                oi_series = oi_result.get('series', [])
                status = len(oi_series) >= 30 and is_latest_highest(oi_series)
            
            period_status[period] = status
            
            if status:
                period_count += 1
            else:
                all_intervals_up = False
        
        if all_intervals_up:
            return {
                'symbol': symbol,
                'oi': daily_data['oi'],
                'change': daily_data['change'],
                'period_count': period_count,
                'period_status': period_status
            }
        
        # å³ä½¿ä¸æ˜¯å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨ï¼Œä¹Ÿæ›´æ–°æ—¥çº¿æ•°æ®çš„å‘¨æœŸçŠ¶æ€
        daily_data['period_status'] = period_status
        daily_data['period_count'] = period_count
        
        return None
    except Exception as e:
        logger.error(f"âŒ åˆ†æ{symbol}å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¤±è´¥: {str(e)}")
        return None

def get_high_volume_symbols():
    """è·å–é«˜äº¤æ˜“é‡å¸ç§"""
    if client is None and not init_client():
        logger.warning("âŒ æ— æ³•è·å–é«˜äº¤æ˜“é‡å¸ç§ï¼šå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        return []

    try:
        tickers = client.futures_ticker()
        # è·å–æ‰€æœ‰é«˜äº¤æ˜“é‡å¸ç§
        filtered = [
            t for t in tickers if float(t.get('quoteVolume', 0)) > 10000000
            and t.get('symbol', '').endswith('USDT')
        ]
        symbols = [t['symbol'] for t in filtered]
        logger.info(f"ğŸ“Š è·å–åˆ° {len(symbols)} ä¸ªé«˜äº¤æ˜“é‡å¸ç§")
        return symbols
    except Exception as e:
        logger.error(f"âŒ è·å–é«˜äº¤æ˜“é‡å¸ç§å¤±è´¥: {str(e)}")
        return []

def split_into_batches(items, batch_size):
    """å°†åˆ—è¡¨åˆ†å‰²æˆæ‰¹æ¬¡"""
    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]

async def analyze_trends_async():
    """å¼‚æ­¥è¶‹åŠ¿åˆ†æé€»è¾‘"""
    start_time = time.time()
    logger.info("ğŸ” å¼€å§‹å¼‚æ­¥åˆ†æå¸ç§è¶‹åŠ¿...")
    
    # æ­¥éª¤1: è·å–é«˜äº¤æ˜“é‡å¸ç§
    symbols = get_high_volume_symbols()
    
    if not symbols:
        logger.warning("âš ï¸ æ²¡æœ‰è·å–åˆ°é«˜äº¤æ˜“é‡å¸ç§ï¼Œè¿”å›ç©ºæ•°æ®")
        return data_cache

    logger.info(f"ğŸ“Š å¼€å§‹åˆ†æ {len(symbols)} ä¸ªå¸ç§...")
    
    # å°†å¸ç§åˆ†æˆ10ä¸ªä¸€æ‰¹
    symbol_batches = split_into_batches(symbols, 10)
    
    short_term_active = []
    daily_rising = []
    all_cycle_rising = []
    
    # æ­¥éª¤2: æ‰¹é‡åˆ†æçŸ­æœŸæ´»è·ƒå¸ç§ï¼ˆæ¯æ‰¹10ä¸ªï¼‰
    logger.info(f"ğŸ”„ æ‰¹é‡åˆ†æçŸ­æœŸæ´»è·ƒå¸ç§...")
    short_term_start = time.time()
    
    for i, batch in enumerate(symbol_batches):
        logger.info(f"ğŸ“¦ çŸ­æœŸæ´»è·ƒæ‰¹æ¬¡ {i+1}/{len(symbol_batches)} ({len(batch)} ä¸ªå¸ç§)")
        batch_results = await analyze_short_term_active_batch(batch)
        short_term_active.extend(batch_results)
        # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
        if i < len(symbol_batches) - 1:
            await asyncio.sleep(0.5)
    
    short_term_time = time.time() - short_term_start
    logger.info(f"âœ… çŸ­æœŸæ´»è·ƒåˆ†æå®Œæˆ: {len(short_term_active)}ä¸ª, è€—æ—¶: {short_term_time:.2f}ç§’")
    
    # æ­¥éª¤3: æ‰¹é‡åˆ†ææ—¥çº¿ä¸Šæ¶¨å¸ç§ï¼ˆæ¯æ‰¹10ä¸ªï¼‰
    logger.info(f"ğŸ”„ æ‰¹é‡åˆ†ææ—¥çº¿ä¸Šæ¶¨å¸ç§...")
    daily_start = time.time()
    
    for i, batch in enumerate(symbol_batches):
        logger.info(f"ğŸ“¦ æ—¥çº¿ä¸Šæ¶¨æ‰¹æ¬¡ {i+1}/{len(symbol_batches)} ({len(batch)} ä¸ªå¸ç§)")
        batch_results = await analyze_daily_rising_batch(batch)
        daily_rising.extend(batch_results)
        # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿ
        if i < len(symbol_batches) - 1:
            await asyncio.sleep(0.5)
    
    daily_time = time.time() - daily_start
    logger.info(f"âœ… æ—¥çº¿ä¸Šæ¶¨åˆ†æå®Œæˆ: {len(daily_rising)}ä¸ª, è€—æ—¶: {daily_time:.2f}ç§’")
    
    # æ­¥éª¤4: å¯¹æ—¥çº¿ä¸Šæ¶¨å¸ç§è¿›è¡Œå…¨éƒ¨å‘¨æœŸåˆ†æï¼ˆæ¯æ‰¹10ä¸ªï¼‰
    if daily_rising:
        logger.info(f"ğŸ”„ æ‰¹é‡åˆ†æå…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¸ç§...")
        all_cycle_start = time.time()
        
        daily_batches = split_into_batches(daily_rising, 10)
        
        for i, batch in enumerate(daily_batches):
            logger.info(f"ğŸ“¦ å…¨éƒ¨å‘¨æœŸæ‰¹æ¬¡ {i+1}/{len(daily_batches)} ({len(batch)} ä¸ªå¸ç§)")
            batch_results = await analyze_all_cycle_rising_batch(batch)
            all_cycle_rising.extend(batch_results)
            # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿ
            if i < len(daily_batches) - 1:
                await asyncio.sleep(0.5)
        
        all_cycle_time = time.time() - all_cycle_start
        logger.info(f"âœ… å…¨éƒ¨å‘¨æœŸåˆ†æå®Œæˆ: {len(all_cycle_rising)}ä¸ª, è€—æ—¶: {all_cycle_time:.2f}ç§’")
    
    # å°†æ—¥çº¿ä¸Šæ¶¨ä½†éå…¨éƒ¨å‘¨æœŸä¸Šæ¶¨çš„å¸ç§åŠ å…¥daily_rising
    final_daily_rising = []
    all_cycle_symbols = {r['symbol'] for r in all_cycle_rising}
    
    for result in daily_rising:
        if result['symbol'] not in all_cycle_symbols:
            final_daily_rising.append(result)

    # æ’åºç»“æœ
    final_daily_rising.sort(key=lambda x: x.get('period_count', 0), reverse=True)
    short_term_active.sort(key=lambda x: x.get('ratio', 0), reverse=True)
    all_cycle_rising.sort(key=lambda x: x.get('period_count', 0), reverse=True)

    analysis_time = time.time() - start_time
    logger.info(f"ğŸ“Š åˆ†æå®Œæˆ: æ—¥çº¿ä¸Šæ¶¨ {len(final_daily_rising)}ä¸ª, çŸ­æœŸæ´»è·ƒ {len(short_term_active)}ä¸ª, å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨ {len(all_cycle_rising)}ä¸ª")
    logger.info(f"â±ï¸ æ€»åˆ†ææ—¶é—´: {analysis_time:.2f}ç§’")

    tz_shanghai = timezone(timedelta(hours=8))
    
    # å‘é€é‚®ä»¶æé†’ï¼ˆå¦‚æœæœ‰å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨çš„å¸ç§ï¼‰
    if all_cycle_rising:
        logger.info(f"ğŸ“§ æ£€æµ‹åˆ° {len(all_cycle_rising)} ä¸ªå…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¸ç§ï¼Œå‘é€é‚®ä»¶æé†’...")
        # åœ¨æ–°çº¿ç¨‹ä¸­å‘é€é‚®ä»¶ï¼Œé¿å…é˜»å¡ä¸»åˆ†ææµç¨‹
        email_thread = threading.Thread(
            target=send_email_notification, 
            args=(all_cycle_rising,)
        )
        email_thread.daemon = True
        email_thread.start()
    else:
        logger.info("ğŸ“§ æ²¡æœ‰æ£€æµ‹åˆ°å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¸ç§ï¼Œä¸å‘é€é‚®ä»¶")

    return {
        'daily_rising': final_daily_rising,
        'short_term_active': short_term_active,
        'all_cycle_rising': all_cycle_rising,
        'analysis_time': analysis_time,
        'last_updated': datetime.now(tz_shanghai).strftime("%Y-%m-%d %H:%M:%S"),
        'next_analysis_time': get_next_analysis_time().strftime("%Y-%m-%d %H:%M:%S")
    }

def analyze_trends():
    """åŒæ­¥åŒ…è£…å™¨ï¼Œç”¨äºè°ƒç”¨å¼‚æ­¥åˆ†æå‡½æ•°"""
    try:
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(analyze_trends_async())
        loop.run_until_complete(close_aio_session())
        loop.close()
        return result
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥åˆ†æå¤±è´¥: {str(e)}")
        # å¦‚æœå¼‚æ­¥åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ–¹å¼
        return analyze_trends_sync_fallback()

def analyze_trends_sync_fallback():
    """åŒæ­¥å›é€€åˆ†æå‡½æ•°"""
    start_time = time.time()
    logger.info("ğŸ”„ ä½¿ç”¨åŒæ­¥å›é€€åˆ†æ...")
    
    symbols = get_high_volume_symbols()
    
    if not symbols:
        return data_cache

    short_term_active = []
    daily_rising = []
    all_cycle_rising = []
    
    # é™åˆ¶åŒæ—¶åˆ†æ10ä¸ªå¸ç§
    symbol_batches = split_into_batches(symbols, 10)
    
    for batch in symbol_batches:
        for symbol in batch:
            try:
                # çŸ­æœŸæ´»è·ƒåˆ†æ
                min5_oi = get_open_interest(symbol, '5m')
                daily_oi = get_open_interest(symbol, '1d')
                
                min5_series = min5_oi.get('series', [])
                daily_series = daily_oi.get('series', [])
                
                if len(min5_series) >= 30 and len(daily_series) >= 30:
                    min5_max = max(min5_series[-30:])
                    daily_avg = sum(daily_series[-30:]) / 30
                    
                    if min5_max > 0 and daily_avg > 0:
                        ratio = min5_max / daily_avg
                        
                        if ratio > 1.5:
                            short_term_active.append({
                                'symbol': symbol,
                                'oi': min5_series[-1],
                                'ratio': round(ratio, 2)
                            })
                
                # æ—¥çº¿ä¸Šæ¶¨åˆ†æ
                if len(daily_series) >= 30:
                    daily_status = is_latest_highest(daily_series)
                    
                    if daily_status:
                        daily_change = ((daily_series[-1] - daily_series[-30]) / daily_series[-30]) * 100
                        
                        daily_data = {
                            'symbol': symbol,
                            'oi': daily_series[-1],
                            'change': round(daily_change, 2),
                            'period_status': {'1d': True},
                            'period_count': 1
                        }
                        
                        # å…¨éƒ¨å‘¨æœŸåˆ†æ
                        period_status = daily_data['period_status'].copy()
                        period_count = daily_data['period_count']
                        all_intervals_up = True
                        
                        for period in VALID_PERIODS:
                            if period == '1d':
                                continue
                                
                            oi_data = get_open_interest(symbol, period)
                            oi_series = oi_data.get('series', [])
                            
                            status = len(oi_series) >= 30 and is_latest_highest(oi_series)
                            period_status[period] = status
                            
                            if status:
                                period_count += 1
                            else:
                                all_intervals_up = False
                        
                        if all_intervals_up:
                            all_cycle_rising.append({
                                'symbol': symbol,
                                'oi': daily_data['oi'],
                                'change': daily_data['change'],
                                'period_count': period_count,
                                'period_status': period_status
                            })
                        else:
                            daily_data['period_status'] = period_status
                            daily_data['period_count'] = period_count
                            daily_rising.append(daily_data)
                            
            except Exception as e:
                logger.error(f"âŒ åŒæ­¥åˆ†æ{symbol}å¤±è´¥: {str(e)}")
    
    # æ’åºç»“æœ
    daily_rising.sort(key=lambda x: x.get('period_count', 0), reverse=True)
    short_term_active.sort(key=lambda x: x.get('ratio', 0), reverse=True)
    all_cycle_rising.sort(key=lambda x: x.get('period_count', 0), reverse=True)

    analysis_time = time.time() - start_time
    logger.info(f"ğŸ“Š åŒæ­¥åˆ†æå®Œæˆ: æ—¥çº¿ä¸Šæ¶¨ {len(daily_rising)}ä¸ª, çŸ­æœŸæ´»è·ƒ {len(short_term_active)}ä¸ª, å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨ {len(all_cycle_rising)}ä¸ª")
    logger.info(f"â±ï¸ æ€»åˆ†ææ—¶é—´: {analysis_time:.2f}ç§’")

    # å‘é€é‚®ä»¶æé†’ï¼ˆå¦‚æœæœ‰å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨çš„å¸ç§ï¼‰
    if all_cycle_rising:
        logger.info(f"ğŸ“§ æ£€æµ‹åˆ° {len(all_cycle_rising)} ä¸ªå…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¸ç§ï¼Œå‘é€é‚®ä»¶æé†’...")
        email_thread = threading.Thread(
            target=send_email_notification, 
            args=(all_cycle_rising,)
        )
        email_thread.daemon = True
        email_thread.start()
    else:
        logger.info("ğŸ“§ æ²¡æœ‰æ£€æµ‹åˆ°å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨å¸ç§ï¼Œä¸å‘é€é‚®ä»¶")

    tz_shanghai = timezone(timedelta(hours=8))
    return {
        'daily_rising': daily_rising,
        'short_term_active': short_term_active,
        'all_cycle_rising': all_cycle_rising,
        'analysis_time': analysis_time,
        'last_updated': datetime.now(tz_shanghai).strftime("%Y-%m-%d %H:%M:%S"),
        'next_analysis_time': get_next_analysis_time().strftime("%Y-%m-%d %H:%M:%S")
    }

def get_open_interest(symbol, period, use_cache=True):
    """åŒæ­¥è·å–æŒä»“é‡æ•°æ®ï¼ˆç”¨äºå›é€€ï¼‰"""
    try:
        if not re.match(r"^[A-Z0-9]{1,10}USDT$", symbol):
            return {'series': [], 'timestamps': []}

        if period not in VALID_PERIODS:
            return {'series': [], 'timestamps': []}
        
        current_time = datetime.now(timezone.utc)
        cache_key = f"{symbol}_{period}"
        
        if use_cache and cache_key in oi_data_cache:
            cached_data = oi_data_cache[cache_key]
            if 'expiration' in cached_data and cached_data['expiration'] > current_time:
                return cached_data['data']

        logger.debug(f"ğŸ“¡ åŒæ­¥è¯·æ±‚æŒä»“é‡æ•°æ®: {symbol} {period}")
        url = "https://fapi.binance.com/futures/data/openInterestHist"
        params = {'symbol': symbol, 'period': period, 'limit': 30}

        headers = {'X-MBX-APIKEY': API_KEY} if API_KEY else {}
        response = requests.get(url, params=params, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return {'series': [], 'timestamps': []}

        data = response.json()
        
        if not isinstance(data, list) or len(data) == 0:
            return {'series': [], 'timestamps': []}
            
        data.sort(key=lambda x: x['timestamp'])
        oi_series = [float(item['sumOpenInterest']) for item in data]
        timestamps = [item['timestamp'] for item in data]

        if len(oi_series) < 30:
            return {'series': [], 'timestamps': []}
            
        oi_data = {
            'series': oi_series, 
            'timestamps': timestamps
        }
        
        expiration = current_time + timedelta(seconds=OI_CACHE_EXPIRATION)
        oi_data_cache[cache_key] = {
            'data': oi_data,
            'expiration': expiration
        }

        return oi_data
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: {str(e)}")
        return {'series': [], 'timestamps': []}

# é˜»åŠ›ä½æ”¯æ’‘ä½åˆ†æå‡½æ•°
def calculate_support_resistance_levels(symbol, interval, klines):
    """è®¡ç®—æ”¯æ’‘ä½å’Œé˜»åŠ›ä½"""
    try:
        if not klines or len(klines) < 20:
            return {'resistance': [], 'support': []}
        
        # æå–æ”¶ç›˜ä»·
        closes = [float(k[4]) for k in klines]
        
        # è®¡ç®—ä»·æ ¼æ°´å¹³åŠå…¶è¢«æµ‹è¯•æ¬¡æ•°
        price_levels = {}
        tolerance = 0.001
        
        for i in range(1, len(closes)-1):
            current_close = closes[i]
            
            # å¯»æ‰¾å±€éƒ¨é«˜ç‚¹å’Œä½ç‚¹
            is_local_high = closes[i] > closes[i-1] and closes[i] > closes[i+1]
            is_local_low = closes[i] < closes[i-1] and closes[i] < closes[i+1]
            
            # å››èˆäº”å…¥åˆ°åˆé€‚çš„ç²¾åº¦
            if current_close > 100:
                rounded_price = round(current_close, 1)
            elif current_close > 10:
                rounded_price = round(current_close, 2)
            elif current_close > 1:
                rounded_price = round(current_close, 3)
            else:
                rounded_price = round(current_close, 4)
            
            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘ç°æœ‰ä»·æ ¼æ°´å¹³
            found_existing = False
            for existing_price in price_levels.keys():
                if abs(existing_price - rounded_price) / existing_price <= tolerance:
                    price_levels[existing_price]['count'] += 1
                    if is_local_high:
                        price_levels[existing_price]['resistance_tests'] += 1
                    if is_local_low:
                        price_levels[existing_price]['support_tests'] += 1
                    found_existing = True
                    break
            
            if not found_existing:
                price_levels[rounded_price] = {
                    'count': 1,
                    'resistance_tests': 1 if is_local_high else 0,
                    'support_tests': 1 if is_local_low else 0
                }
        
        # è·å–å½“å‰ä»·æ ¼
        current_price = closes[-1] if closes else 0
        
        # åˆ†ç¦»é˜»åŠ›ä½å’Œæ”¯æ’‘ä½
        resistance_levels = []
        support_levels = []
        
        for price, data in price_levels.items():
            if data['resistance_tests'] > 0 or data['support_tests'] > 0:
                total_tests = data['resistance_tests'] + data['support_tests']
                strength = min(1.0, total_tests / 10.0)
                
                level_data = {
                    'price': price,
                    'strength': round(strength, 2),
                    'test_count': total_tests,
                    'resistance_tests': data['resistance_tests'],
                    'support_tests': data['support_tests'],
                    'distance_percent': round(((price - current_price) / current_price * 100), 2) if current_price > 0 else 0
                }
                
                if price > current_price and data['resistance_tests'] > 0:
                    resistance_levels.append(level_data)
                elif price < current_price and data['support_tests'] > 0:
                    support_levels.append(level_data)
        
        # æŒ‰è¢«æµ‹è¯•æ¬¡æ•°æ’åºï¼Œåªä¿ç•™å‰3ä¸ª
        resistance_levels.sort(key=lambda x: x['test_count'], reverse=True)
        support_levels.sort(key=lambda x: x['test_count'], reverse=True)
        
        return {
            'resistance': resistance_levels[:3],
            'support': support_levels[:3]
        }
        
    except Exception as e:
        logger.error(f"è®¡ç®—æ”¯æ’‘é˜»åŠ›ä½å¤±è´¥ {symbol} {interval}: {str(e)}")
        return {'resistance': [], 'support': []}

def calculate_resistance_levels(symbol):
    """è®¡ç®—é˜»åŠ›ä½"""
    try:
        logger.info(f"ğŸ“Š è®¡ç®—é˜»åŠ›ä½: {symbol}")
        now = time.time()
        
        cache_key = f"{symbol}_resistance"
        if cache_key in resistance_cache:
            cache_data = resistance_cache[cache_key]
            if cache_data['expiration'] > now:
                return cache_data['levels']
        
        if client is None and not init_client():
            return {'levels': {}, 'current_price': 0}
        
        try:
            ticker = client.futures_symbol_ticker(symbol=symbol)
            current_price = float(ticker['price'])
        except Exception:
            current_price = None
        
        interval_levels = {}
        
        for interval in RESISTANCE_INTERVALS:
            try:
                klines = client.futures_klines(symbol=symbol, interval=interval, limit=100)
                
                if not klines or len(klines) < 20:
                    continue

                levels = calculate_support_resistance_levels(symbol, interval, klines)
                interval_levels[interval] = levels
                
            except Exception as e:
                logger.error(f"è®¡ç®—{symbol}åœ¨{interval}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")

        levels = {
            'levels': interval_levels,
            'current_price': current_price or 0
        }
        
        resistance_cache[cache_key] = {
            'levels': levels,
            'expiration': now + RESISTANCE_CACHE_EXPIRATION
        }
        return levels
    except Exception as e:
        logger.error(f"è®¡ç®—{symbol}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
        return {'levels': {}, 'current_price': 0}

def analysis_worker():
    """åˆ†æå·¥ä½œçº¿ç¨‹"""
    global data_cache, current_data_cache, analysis_thread_running
    
    logger.info("ğŸ”§ æ•°æ®åˆ†æçº¿ç¨‹å¯åŠ¨")
    analysis_thread_running = True
    
    while analysis_thread_running:
        try:
            # ç­‰å¾…åˆ°ä¸‹ä¸€ä¸ªåˆ†ææ—¶é—´
            next_analysis = get_next_analysis_time()
            wait_seconds = max(5, (next_analysis - datetime.now(timezone.utc)).total_seconds())
            
            logger.info(f"â³ ä¸‹æ¬¡åˆ†ææ—¶é—´: {next_analysis.strftime('%H:%M:%S')}")
            logger.info(f"â³ ç­‰å¾…æ—¶é—´: {wait_seconds:.1f} ç§’")
            
            # ç­‰å¾…æœŸé—´æ£€æŸ¥åœæ­¢ä¿¡å·
            wait_start = time.time()
            while time.time() - wait_start < wait_seconds and analysis_thread_running:
                time.sleep(1)
            
            if not analysis_thread_running:
                break
                
            # æ‰§è¡Œåˆ†æ
            analysis_start = datetime.now(timezone.utc)
            logger.info(f"â±ï¸ å¼€å§‹æ›´æ–°æ•°æ®...")

            backup_cache = data_cache.copy()
            current_backup = current_data_cache.copy()

            try:
                result = analyze_trends()
                
                new_data = {
                    "last_updated": result['last_updated'],
                    "daily_rising": result['daily_rising'],
                    "short_term_active": result['short_term_active'],
                    "all_cycle_rising": result['all_cycle_rising'],
                    "analysis_time": result['analysis_time'],
                    "next_analysis_time": result['next_analysis_time']
                }
                
                data_cache = new_data
                current_data_cache = new_data.copy()
                logger.info(f"âœ… æ•°æ®æ›´æ–°æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                data_cache = backup_cache
                current_data_cache = current_backup

            analysis_duration = (datetime.now(timezone.utc) - analysis_start).total_seconds()
            logger.info(f"â±ï¸ åˆ†æè€—æ—¶: {analysis_duration:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            # å‡ºé”™åç­‰å¾…ä¸€æ®µæ—¶é—´å†ç»§ç»­
            time.sleep(60)
    
    logger.info("ğŸ›‘ åˆ†æçº¿ç¨‹å·²åœæ­¢")

def start_background_threads():
    """å¯åŠ¨åå°çº¿ç¨‹"""
    global analysis_thread
    
    static_path = app.static_folder
    if not os.path.exists(static_path):
        os.makedirs(static_path)
    
    index_path = os.path.join(static_path, 'index.html')
    if not os.path.exists(index_path):
        with open(index_path, 'w') as f:
            f.write("<html><body><h1>æŒä»“é‡åˆ†ææœåŠ¡æ­£åœ¨å¯åŠ¨...</h1></body></html>")
    
    global current_data_cache
    tz_shanghai = timezone(timedelta(hours=8))
    current_data_cache = {
        "last_updated": "ç­‰å¾…é¦–æ¬¡åˆ†æ",
        "daily_rising": [],
        "short_term_active": [],
        "all_cycle_rising": [],
        "analysis_time": 0,
        "next_analysis_time": get_next_analysis_time().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # å»¶è¿Ÿå¯åŠ¨åˆ†æçº¿ç¨‹ï¼ˆé¿å…é˜»å¡åº”ç”¨å¯åŠ¨ï¼‰
    def delayed_start():
        time.sleep(10)  # ç­‰å¾…10ç§’è®©åº”ç”¨å…ˆå¯åŠ¨
        
        logger.info("ğŸ”„ å¼€å§‹åˆå§‹åŒ–åˆ†æç»„ä»¶...")
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        if not init_client():
            logger.error("âŒ æ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
            # å³ä½¿å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œä¹Ÿç»§ç»­å¯åŠ¨åˆ†æçº¿ç¨‹ï¼Œä½†ä¼šè¿”å›ç©ºæ•°æ®
        else:
            logger.info("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # å¯åŠ¨åˆ†æçº¿ç¨‹
        global analysis_thread
        analysis_thread = threading.Thread(target=analysis_worker, name="AnalysisWorker")
        analysis_thread.daemon = True
        analysis_thread.start()
        
        logger.info("ğŸ”„ åˆ†æçº¿ç¨‹å·²å¯åŠ¨ï¼Œå°†åœ¨ä¸‹ä¸€ä¸ª5åˆ†é’Ÿå‘¨æœŸ+45ç§’åå¼€å§‹åˆ†æ")
    
    start_thread = threading.Thread(target=delayed_start)
    start_thread.daemon = True
    start_thread.start()
    
    logger.info("âœ… åå°çº¿ç¨‹å¯åŠ¨æˆåŠŸ")
    return True

# APIè·¯ç”±
@app.route('/')
def index():
    try:
        return send_from_directory(app.static_folder, 'index.html')
    except Exception as e:
        logger.error(f"âŒ å¤„ç†é¦–é¡µè¯·æ±‚å¤±è´¥: {str(e)}")
        return "æœåŠ¡æ­£åœ¨å¯åŠ¨ï¼Œè¯·ç¨ååˆ·æ–°...", 200

@app.route('/<path:filename>')
def static_files(filename):
    return send_from_directory(app.static_folder, filename)

@app.route('/api/data', methods=['GET'])
def get_data():
    global current_data_cache
    try:
        if not current_data_cache or current_data_cache.get("last_updated") == "ä»æœªæ›´æ–°":
            return jsonify({
                'last_updated': "æ•°æ®ç”Ÿæˆä¸­...",
                'daily_rising': [],
                'short_term_active': [],
                'all_cycle_rising': [],
                'analysis_time': 0,
                'next_analysis_time': "è®¡ç®—ä¸­..."
            })
        
        def validate_coins(coins):
            valid_coins = []
            for coin in coins:
                if not isinstance(coin, dict):
                    continue
                if 'symbol' not in coin:
                    coin['symbol'] = 'æœªçŸ¥å¸ç§'
                if 'oi' not in coin:
                    coin['oi'] = 0
                if 'change' not in coin:
                    coin['change'] = 0
                if 'ratio' not in coin:
                    coin['ratio'] = 0
                if 'period_count' not in coin:
                    coin['period_count'] = 0
                if 'period_status' not in coin:
                    coin['period_status'] = {}
                valid_coins.append(coin)
            return valid_coins
        
        daily_rising = validate_coins(current_data_cache.get('daily_rising', []))
        short_term_active = validate_coins(current_data_cache.get('short_term_active', []))
        all_cycle_rising = validate_coins(current_data_cache.get('all_cycle_rising', []))
        
        all_cycle_symbols = {coin['symbol'] for coin in all_cycle_rising}
        filtered_daily_rising = [coin for coin in daily_rising if coin['symbol'] not in all_cycle_symbols]
        
        data = {
            'last_updated': current_data_cache.get('last_updated', ""),
            'daily_rising': filtered_daily_rising,
            'short_term_active': short_term_active,
            'all_cycle_rising': all_cycle_rising,
            'analysis_time': current_data_cache.get('analysis_time', 0),
            'next_analysis_time': current_data_cache.get('next_analysis_time', "")
        }
        
        return jsonify(data)
    
    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
        tz_shanghai = timezone(timedelta(hours=8))
        return jsonify({
            'last_updated': datetime.now(tz_shanghai).strftime("%Y-%m-%d %H:%M:%S"),
            'daily_rising': [],
            'short_term_active': [],
            'all_cycle_rising': [],
            'analysis_time': 0,
            'next_analysis_time': get_next_analysis_time().strftime("%Y-%m-%d %H:%M:%S")
        })

@app.route('/api/resistance_levels/<symbol>', methods=['GET'])
def get_resistance_levels(symbol):
    try:
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            return jsonify({'error': 'Invalid symbol format'}), 400

        if not BINANCE_AVAILABLE:
            return jsonify({'error': 'Binance client not available'}), 503

        levels = calculate_resistance_levels(symbol)
        
        return jsonify(levels)
    except Exception as e:
        logger.error(f"âŒ è·å–é˜»åŠ›ä½æ•°æ®å¤±è´¥: {symbol}, {str(e)}")
        return jsonify({
            'levels': {},
            'current_price': 0,
            'error': str(e)
        }), 500

@app.route('/api/oi_chart/<symbol>/<period>', methods=['GET'])
def get_oi_chart_data(symbol, period):
    try:
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            return jsonify({'error': 'Invalid symbol format'}), 400

        if period not in VALID_PERIODS:
            return jsonify({'error': 'Unsupported period'}), 400

        oi_data = get_open_interest(symbol, period, use_cache=True)
        return jsonify({
            'data': oi_data.get('series', []),
            'timestamps': oi_data.get('timestamps', [])
        })
    except Exception as e:
        logger.error(f"âŒ è·å–æŒä»“é‡å›¾è¡¨æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    try:
        binance_status = 'not initialized'
        if client:
            try:
                client.get_server_time()
                binance_status = 'ok'
            except:
                binance_status = 'error'
        
        tz_shanghai = timezone(timedelta(hours=8))
        return jsonify({
            'status': 'healthy',
            'binance': binance_status,
            'analysis_thread_running': analysis_thread_running,
            'last_updated': current_data_cache.get('last_updated', 'N/A'),
            'next_analysis_time': current_data_cache.get('next_analysis_time', 'N/A'),
            'server_time': datetime.now(tz_shanghai).strftime("%Y-%m-%d %H:%M:%S"),
            'environment': 'render' if IS_RENDER else 'local'
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e)
        }), 500

@app.route('/api/status', methods=['GET'])
def status():
    """ç®€åŒ–çŠ¶æ€æ£€æŸ¥"""
    return jsonify({
        'status': 'running',
        'analysis_thread_running': analysis_thread_running,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/trigger-analysis', methods=['POST'])
def trigger_analysis():
    """æ‰‹åŠ¨è§¦å‘åˆ†æï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    try:
        logger.info("ğŸ”„ æ‰‹åŠ¨è§¦å‘åˆ†æ...")
        result = analyze_trends()
        
        global current_data_cache
        current_data_cache = {
            "last_updated": result['last_updated'],
            "daily_rising": result['daily_rising'],
            "short_term_active": result['short_term_active'],
            "all_cycle_rising": result['all_cycle_rising'],
            "analysis_time": result['analysis_time'],
            "next_analysis_time": result['next_analysis_time']
        }
        
        return jsonify({
            'status': 'success',
            'message': 'åˆ†æå®Œæˆ',
            'last_updated': result['last_updated']
        })
    except Exception as e:
        logger.error(f"âŒ æ‰‹åŠ¨åˆ†æå¤±è´¥: {str(e)}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

if __name__ == '__main__':
    PORT = int(os.environ.get("PORT", 10000))
    
    logger.info("=" * 50)
    logger.info(f"ğŸš€ å¯åŠ¨åŠ å¯†è´§å¸æŒä»“é‡åˆ†ææœåŠ¡")
    logger.info(f"ğŸŒ æœåŠ¡ç«¯å£: {PORT}")
    logger.info(f"ğŸ·ï¸ ç¯å¢ƒ: {'Render' if IS_RENDER else 'Local'}")
    logger.info("=" * 50)
    
    if start_background_threads():
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡å™¨...")
        
        # å¦‚æœåœ¨ Render ç¯å¢ƒï¼Œä½¿ç”¨æ›´ç®€å•çš„å¯åŠ¨æ–¹å¼
        if IS_RENDER:
            # ä½¿ç”¨ Gunicorn å…¼å®¹çš„æ–¹å¼
            app.run(host='0.0.0.0', port=PORT, debug=False, threaded=True)
        else:
            app.run(host='0.0.0.0', port=PORT, debug=False)
    else:
        logger.critical("ğŸ”¥ æ— æ³•å¯åŠ¨æœåŠ¡")
