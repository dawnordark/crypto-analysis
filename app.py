#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import time
import re
import json
import math
import requests
import threading
import queue
import logging
import traceback
import urllib3
import numpy as np
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import Flask, jsonify, send_from_directory
from binance.client import Client

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è®¾ç½®æ—¥å¿—çº§åˆ«
LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()
root_logger = logging.getLogger()
root_logger.setLevel(getattr(logging, LOG_LEVEL))

# åˆ›å»ºæ—¥å¿—å¤„ç†å™¨
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(getattr(logging, LOG_LEVEL))
file_handler = logging.FileHandler('app.log')
file_handler.setLevel(getattr(logging, LOG_LEVEL))

# æ—¥å¿—æ ¼å¼
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)

# æ·»åŠ å¤„ç†å™¨
root_logger.addHandler(console_handler)
root_logger.addHandler(file_handler)

logger = logging.getLogger(__name__)
logger.info(f"âœ… æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º: {LOG_LEVEL}")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
app = Flask(__name__, static_folder=os.path.join(BASE_DIR, 'static'), static_url_path='/static')

# Binance API é…ç½®
API_KEY = os.environ.get('BINANCE_API_KEY', '')
API_SECRET = os.environ.get('BINANCE_API_SECRET', '')
client = None

# æ•°æ®ç¼“å­˜
data_cache = {
    "last_updated": "ä»æœªæ›´æ–°",
    "daily_rising": [],
    "short_term_active": [],
    "all_cycle_rising": [],
    "analysis_time": 0,
    "next_analysis_time": "è®¡ç®—ä¸­..."
}

current_data_cache = data_cache.copy()
oi_data_cache = {}
resistance_cache = {}
RESISTANCE_CACHE_EXPIRATION = 24 * 3600
OI_CACHE_EXPIRATION = 5 * 60  # 5åˆ†é’Ÿç¼“å­˜è¿‡æœŸ

# ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œçº¿ç¨‹é—´é€šä¿¡
analysis_queue = queue.Queue()
executor = ThreadPoolExecutor(max_workers=10)

PERIOD_MINUTES = {
    '5m': 5,
    '15m': 15,
    '30m': 30,
    '1h': 60,
    '2h': 120,
    '4h': 240,
    '6h': 360,
    '12h': 720,
    '1d': 1440
}

RESISTANCE_INTERVALS = ['5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h', '1d']
ALL_PERIODS = ['5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h', '1d']

def init_client():
    global client
    max_retries = 5
    retry_delay = 5
    
    # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦è®¾ç½®
    if not API_KEY or not API_SECRET:
        logger.error("âŒ Binance APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡BINANCE_API_KEYå’ŒBINANCE_API_SECRET")
        return False
    
    for attempt in range(max_retries):
        try:
            logger.info(f"ğŸ”§ å°è¯•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ (ç¬¬{attempt+1}æ¬¡)...")
            client = Client(
                api_key=API_KEY, 
                api_secret=API_SECRET,
                requests_params={'timeout': 30}
            )
            
            # æµ‹è¯•è¿æ¥
            server_time = client.get_server_time()
            logger.info(f"âœ… Binanceå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒæœåŠ¡å™¨æ—¶é—´: {datetime.fromtimestamp(server_time['serverTime']/1000)}")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            if attempt < max_retries - 1:
                logger.info(f"ğŸ”„ {retry_delay}ç§’åé‡è¯•åˆå§‹åŒ–å®¢æˆ·ç«¯...")
                time.sleep(retry_delay)
    logger.critical("ğŸ”¥ æ— æ³•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    return False

def get_next_update_time():
    now = datetime.now(timezone.utc)
    # è®¡ç®—ä¸‹ä¸€ä¸ª5åˆ†5ç§’çš„æ—¶é—´ç‚¹
    current_minute = now.minute
    current_second = now.second
    minutes_to_add = 5 - (current_minute % 5)
    seconds_to_add = 5 - current_second if minutes_to_add == 0 else (5 - current_second) + (minutes_to_add - 1) * 60
    next_update = now + timedelta(seconds=seconds_to_add)
    return next_update

def get_open_interest(symbol, period, use_cache=True):
    try:
        # éªŒè¯å¸ç§æ ¼å¼
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return {'series': [], 'timestamps': []}

        current_time = datetime.now(timezone.utc)
        cache_key = f"{symbol}_{period}"
        
        if use_cache and cache_key in oi_data_cache:
            cached_data = oi_data_cache[cache_key]
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if 'expiration' in cached_data and cached_data['expiration'] > current_time:
                logger.debug(f"ğŸ“ˆ ä½¿ç”¨ç¼“å­˜æ•°æ®: {symbol} {period}")
                return cached_data['data']

        logger.info(f"ğŸ“¡ è¯·æ±‚æŒä»“é‡æ•°æ®: symbol={symbol}, period={period}")
        url = "https://fapi.binance.com/futures/data/openInterestHist"
        params = {'symbol': symbol, 'period': period, 'limit': 30}

        response = requests.get(url, params=params, timeout=15)
        logger.debug(f"ğŸ“¡ å“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code != 200:
            logger.error(f"âŒ è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: HTTP {response.status_code}")
            return {'series': [], 'timestamps': []}

        data = response.json()
        logger.debug(f"ğŸ“¡ è·å–åˆ° {len(data)} æ¡æŒä»“é‡æ•°æ®")

        if not isinstance(data, list) or len(data) == 0:
            logger.warning(f"âš ï¸ {symbol}çš„{period}æŒä»“é‡æ•°æ®ä¸ºç©º")
            return {'series': [], 'timestamps': []}
            
        data.sort(key=lambda x: x['timestamp'])
        oi_series = [float(item['sumOpenInterest']) for item in data]
        timestamps = [item['timestamp'] for item in data]

        if len(oi_series) < 5:
            logger.warning(f"âš ï¸ {symbol}çš„{period}æŒä»“é‡æ•°æ®ä¸è¶³")
            return {'series': [], 'timestamps': []}
            
        oi_data = {
            'series': oi_series, 
            'timestamps': timestamps
        }
        
        # è®¾ç½®5åˆ†é’Ÿç¼“å­˜è¿‡æœŸ
        expiration = current_time + timedelta(seconds=OI_CACHE_EXPIRATION)
        oi_data_cache[cache_key] = {
            'data': oi_data,
            'expiration': expiration
        }

        logger.info(f"ğŸ“ˆ è·å–æ–°æ•°æ®: {symbol} {period} ({len(oi_series)}ç‚¹)")
        return oi_data
    except Exception as e:
        logger.error(f"âŒ è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return {'series': [], 'timestamps': []}

def is_latest_highest(oi_data):
    if not oi_data or len(oi_data) < 30:
        logger.debug("æŒä»“é‡æ•°æ®ä¸è¶³30ä¸ªç‚¹")
        return False

    latest_value = oi_data[-1]
    prev_data = oi_data[-30:-1]
    
    return latest_value > max(prev_data) if prev_data else False

def find_swing_points(prices, window=5):
    """è¯†åˆ«æ‘†åŠ¨ç‚¹ï¼ˆå±€éƒ¨é«˜ç‚¹å’Œä½ç‚¹ï¼‰"""
    highs = []
    lows = []
    
    for i in range(window, len(prices) - window):
        # æ£€æŸ¥æ˜¯å¦ä¸ºå±€éƒ¨é«˜ç‚¹
        if all(prices[i] > prices[i - j] for j in range(1, window+1)) and \
           all(prices[i] > prices[i + j] for j in range(1, window+1)):
            highs.append(prices[i])
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå±€éƒ¨ä½ç‚¹
        if all(prices[i] < prices[i - j] for j in range(1, window+1)) and \
           all(prices[i] < prices[i + j] for j in range(1, window+1)):
            lows.append(prices[i])
    
    return highs, lows

def calculate_fib_levels(high, low):
    """è®¡ç®—æ–æ³¢é‚£å¥‘å›è°ƒæ°´å¹³"""
    if high <= low:
        return []
    
    levels = []
    ratios = [0.236, 0.382, 0.5, 0.618, 0.786, 1.0, 1.272, 1.618]
    
    for ratio in ratios:
        level = high - (high - low) * ratio
        levels.append(level)
    
    return levels

def calculate_trendline_levels(prices):
    """ä½¿ç”¨çº¿æ€§å›å½’è®¡ç®—è¶‹åŠ¿çº¿æ°´å¹³"""
    if len(prices) < 10:
        return []
    
    try:
        # ä½¿ç”¨çº¿æ€§å›å½’
        x = np.arange(len(prices))
        y = np.array(prices)
        
        # è®¡ç®—æ–œç‡ (m) å’Œæˆªè· (b)
        A = np.vstack([x, np.ones(len(x))]).T
        m, b = np.linalg.lstsq(A, y, rcond=None)[0]
        
        # è®¡ç®—å½“å‰æ—¶åˆ»çš„è¶‹åŠ¿çº¿ä»·æ ¼
        current_level = m * len(prices) + b
        return [current_level]
    except:
        return []

def detect_resonance_levels(levels, tolerance=0.005):
    """æ£€æµ‹å…±æŒ¯æ°´å¹³ï¼ˆèšç±»åˆ†æï¼‰"""
    if not levels:
        return {'resistance': [], 'support': []}
    
    levels.sort()
    clusters = []
    current_cluster = [levels[0]]
    
    for level in levels[1:]:
        if level <= current_cluster[-1] * (1 + tolerance):
            current_cluster.append(level)
        else:
            clusters.append(current_cluster)
            current_cluster = [level]
    
    clusters.append(current_cluster)
    
    # è®¡ç®—æ¯ä¸ªèšç±»çš„å¹³å‡æ°´å¹³å’Œå¼ºåº¦
    cluster_levels = []
    for cluster in clusters:
        avg = sum(cluster) / len(cluster)
        strength = len(cluster)
        cluster_levels.append((avg, strength))
    
    # æŒ‰å¼ºåº¦é™åºæ’åº
    cluster_levels.sort(key=lambda x: x[1], reverse=True)
    
    # åªè¿”å›å‰3ä¸ªé˜»åŠ›ä½å’Œå‰3ä¸ªæ”¯æ’‘ä½
    resistance = [x[0] for x in cluster_levels if x[0] > 0][:3]
    support = [x[0] for x in cluster_levels if x[0] > 0][:3]
    
    return {'resistance': resistance, 'support': support}

def calculate_resistance_levels(symbol):
    try:
        logger.info(f"ğŸ“Š è®¡ç®—é˜»åŠ›ä½: {symbol}")
        now = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        if symbol in resistance_cache:
            cache_data = resistance_cache[symbol]
            if cache_data['expiration'] > now:
                logger.debug(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜çš„é˜»åŠ›ä½æ•°æ®: {symbol}")
                return cache_data['levels']
        
        # ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
        if client is None and not init_client():
            logger.error("âŒ æ— æ³•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ï¼Œæ— æ³•è®¡ç®—é˜»åŠ›ä½")
            return {'resistance': [], 'support': [], 'current_price': 0}
        
        # è·å–å½“å‰ä»·æ ¼
        try:
            ticker = client.futures_symbol_ticker(symbol=symbol)
            current_price = float(ticker['price'])
            logger.info(f"ğŸ“Š {symbol}å½“å‰ä»·æ ¼: {current_price}")
        except Exception as e:
            logger.error(f"âŒ è·å–{symbol}å½“å‰ä»·æ ¼å¤±è´¥: {str(e)}")
            current_price = 0
        
        all_levels = []
        interval_levels_map = {}
        
        for interval in RESISTANCE_INTERVALS:
            try:
                logger.info(f"ğŸ“Š è·å–Kçº¿æ•°æ®: {symbol} {interval}")
                klines = client.futures_klines(symbol=symbol, interval=interval, limit=100)
                
                if not klines or len(klines) < 10:
                    logger.warning(f"âš ï¸ {symbol}åœ¨{interval}çš„Kçº¿æ•°æ®ä¸è¶³")
                    continue

                high_prices = [float(k[2]) for k in klines]
                low_prices = [float(k[3]) for k in klines]
                close_prices = [float(k[4]) for k in klines]
                
                # 1. è¯†åˆ«æ‘†åŠ¨ç‚¹
                swing_highs, swing_lows = find_swing_points(high_prices)
                logger.debug(f"ğŸ“Š {symbol}åœ¨{interval}çš„æ‘†åŠ¨é«˜ç‚¹: {swing_highs}, æ‘†åŠ¨ä½ç‚¹: {swing_lows}")
                
                # 2. è®¡ç®—æ–æ³¢é‚£å¥‘æ°´å¹³
                recent_high = max(high_prices[-30:]) if len(high_prices) >= 30 else max(high_prices)
                recent_low = min(low_prices[-30:]) if len(low_prices) >= 30 else min(low_prices)
                fib_levels = calculate_fib_levels(recent_high, recent_low)
                logger.debug(f"ğŸ“Š {symbol}åœ¨{interval}çš„æ–æ³¢é‚£å¥‘æ°´å¹³: {fib_levels}")
                
                # 3. è®¡ç®—è¶‹åŠ¿çº¿æ°´å¹³
                trendline_levels = calculate_trendline_levels(close_prices)
                logger.debug(f"ğŸ“Š {symbol}åœ¨{interval}çš„è¶‹åŠ¿çº¿æ°´å¹³: {trendline_levels}")
                
                # åˆå¹¶æ‰€æœ‰æ°´å¹³
                interval_levels = swing_highs + swing_lows + fib_levels + trendline_levels
                interval_levels_map[interval] = interval_levels
                
                # æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨
                all_levels.extend(interval_levels)
                
            except Exception as e:
                logger.error(f"è®¡ç®—{symbol}åœ¨{interval}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
                logger.error(traceback.format_exc())

        # å…¨å±€å…±æŒ¯æ£€æµ‹
        final_levels = detect_resonance_levels(all_levels)
        logger.info(f"ğŸ“Š {symbol}å…¨å±€æœ€ä¼˜é˜»åŠ›ä½: {final_levels['resistance']}, æ”¯æ’‘ä½: {final_levels['support']}")
        
        # æ‰¾å‡ºå…³é”®é˜»åŠ›ä½å’Œæ”¯æ’‘ä½ï¼ˆæœ€æ¥è¿‘å½“å‰ä»·æ ¼çš„ï¼‰
        key_resistance = None
        key_support = None
        
        if current_price > 0:
            # æ‰¾å‡ºæœ€æ¥è¿‘çš„é˜»åŠ›ä½ï¼ˆé«˜äºå½“å‰ä»·æ ¼ï¼‰
            resistances_above = [r for r in final_levels['resistance'] if r > current_price]
            if resistances_above:
                key_resistance = min(resistances_above, key=lambda x: abs(x - current_price))
                resistance_distance = ((key_resistance - current_price) / current_price) * 100
            
            # æ‰¾å‡ºæœ€æ¥è¿‘çš„æ”¯æ’‘ä½ï¼ˆä½äºå½“å‰ä»·æ ¼ï¼‰
            supports_below = [s for s in final_levels['support'] if s < current_price]
            if supports_below:
                key_support = max(supports_below, key=lambda x: abs(x - current_price))
                support_distance = ((current_price - key_support) / current_price) * 100
        
        # æ‰¾å‡ºå…³é”®å‘¨æœŸï¼ˆå…±æŒ¯å¼ºåº¦æœ€é«˜çš„é˜»åŠ›ä½/æ”¯æ’‘ä½å‡ºç°çš„å‘¨æœŸï¼‰
        key_resistance_intervals = {}
        key_support_intervals = {}
        
        for interval, levels in interval_levels_map.items():
            for level in levels:
                # æ£€æŸ¥æ˜¯å¦ä¸ºå…³é”®é˜»åŠ›ä½
                if key_resistance and abs(level - key_resistance) / key_resistance < 0.01:
                    key_resistance_intervals[interval] = level
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå…³é”®æ”¯æ’‘ä½
                if key_support and abs(level - key_support) / key_support < 0.01:
                    key_support_intervals[interval] = level
        
        result = {
            'resistance': final_levels['resistance'],
            'support': final_levels['support'],
            'current_price': current_price,
            'key_resistance': key_resistance,
            'key_support': key_support,
            'key_resistance_intervals': key_resistance_intervals,
            'key_support_intervals': key_support_intervals
        }
        
        resistance_cache[symbol] = {
            'levels': result,
            'expiration': now + RESISTANCE_CACHE_EXPIRATION
        }
        return result
    except Exception as e:
        logger.error(f"è®¡ç®—{symbol}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'resistance': [], 
            'support': [], 
            'current_price': 0,
            'key_resistance': None,
            'key_support': None,
            'key_resistance_intervals': {},
            'key_support_intervals': {}
        }

def analyze_daily_rising(symbol):
    """åˆ†ææ—¥çº¿ä¸Šæ¶¨æ¡ä»¶"""
    try:
        # 1. è·å–æ—¥çº¿æŒä»“é‡æ•°æ®
        daily_oi = get_open_interest(symbol, '1d')
        daily_series = daily_oi.get('series', [])
        
        # 2. æ£€æŸ¥æ—¥çº¿ä¸Šæ¶¨æ¡ä»¶
        if len(daily_series) >= 30 and is_latest_highest(daily_series):
            daily_change = ((daily_series[-1] - daily_series[-30]) / daily_series[-30]) * 100
            logger.info(f"ğŸ“Š {symbol} æ—¥çº¿ä¸Šæ¶¨æ¡ä»¶æ»¡è¶³ï¼Œæ¶¨å¹…: {daily_change:.2f}%")
            
            # è·å–å…³é”®é˜»åŠ›ä½
            resistance_data = calculate_resistance_levels(symbol)
            key_resistance = resistance_data.get('key_resistance', 0)
            
            return {
                'symbol': symbol,
                'oi': daily_series[-1],
                'change': round(daily_change, 2),
                'key_resistance': key_resistance
            }
        return None
    except Exception as e:
        logger.error(f"âŒ åˆ†æ{symbol}æ—¥çº¿ä¸Šæ¶¨æ—¶å‡ºé”™: {str(e)}")
        return None

def analyze_all_cycles(symbol, daily_rising_item):
    """åˆ†æå…¨å‘¨æœŸä¸Šæ¶¨æ¡ä»¶"""
    try:
        logger.info(f"ğŸ“Š å¼€å§‹å…¨å‘¨æœŸåˆ†æ: {symbol}")
        period_status = {}
        period_count = 0
        
        # ç¡®ä¿æ—¥çº¿çŠ¶æ€ä¸ºTrue
        period_status['1d'] = True
        
        for period in ALL_PERIODS:
            if period == '1d':
                continue
                
            oi_data = get_open_interest(symbol, period)
            oi_series = oi_data.get('series', [])
            
            # ç¡®ä¿æ­£ç¡®è®¡ç®—å‘¨æœŸæ•°é‡
            status = len(oi_series) >= 30 and is_latest_highest(oi_series)
            period_status[period] = status
            
            if status:
                period_count += 1
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å‘¨æœŸéƒ½æ»¡è¶³æ¡ä»¶
        all_intervals_up = all(period_status.values())
        
        if all_intervals_up:
            logger.info(f"ğŸ“Š {symbol} å…¨å‘¨æœŸä¸Šæ¶¨æ¡ä»¶æ»¡è¶³")
            
            # è·å–å…³é”®é˜»åŠ›ä½
            resistance_data = calculate_resistance_levels(symbol)
            key_resistance = resistance_data.get('key_resistance', 0)
            
            return {
                'symbol': symbol,
                'oi': daily_rising_item['oi'],
                'change': daily_rising_item['change'],
                'period_status': period_status,
                'period_count': period_count,
                'key_resistance': key_resistance
            }
        return None
    except Exception as e:
        logger.error(f"âŒ åˆ†æ{symbol}å…¨å‘¨æœŸæ—¶å‡ºé”™: {str(e)}")
        return None

def analyze_short_term_active(symbol):
    """åˆ†æçŸ­æœŸæ´»è·ƒæ¡ä»¶"""
    try:
        # è·å–5åˆ†é’ŸæŒä»“é‡æ•°æ®
        min5_oi = get_open_interest(symbol, '5m')
        min5_series = min5_oi.get('series', [])
        
        # è·å–æ—¥çº¿æŒä»“é‡æ•°æ®
        daily_oi = get_open_interest(symbol, '1d')
        daily_series = daily_oi.get('series', [])
        
        if len(min5_series) >= 30 and len(daily_series) >= 30:
            min5_max = max(min5_series[-30:])
            daily_avg = sum(daily_series[-30:]) / 30
            
            if min5_max > 0 and daily_avg > 0:
                ratio = min5_max / daily_avg
                logger.debug(f"ğŸ“Š {symbol} çŸ­æœŸæ´»è·ƒæ¯”ç‡: {ratio:.2f}")
                
                if ratio > 1.5:
                    logger.info(f"ğŸ“Š {symbol} çŸ­æœŸæ´»è·ƒæ¡ä»¶æ»¡è¶³")
                    return {
                        'symbol': symbol,
                        'oi': min5_series[-1],
                        'ratio': round(ratio, 2)
                    }
        return None
    except Exception as e:
        logger.error(f"âŒ åˆ†æ{symbol}çŸ­æœŸæ´»è·ƒæ—¶å‡ºé”™: {str(e)}")
        return None

def analyze_trends():
    start_time = time.time()
    logger.info("ğŸ” å¼€å§‹åˆ†æå¸ç§è¶‹åŠ¿...")
    symbols = get_high_volume_symbols()
    
    if not symbols:
        logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°é«˜äº¤æ˜“é‡å¸ç§")
        return {
            'daily_rising': [],
            'short_term_active': [],
            'all_cycle_rising': [],
            'analysis_time': 0
        }

    logger.info(f"ğŸ” å¼€å§‹åˆ†æ {len(symbols)} ä¸ªå¸ç§")

    daily_rising = []
    short_term_active = []
    all_cycle_rising = []

    # ä½¿ç”¨10ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=10) as executor:
        # ç¬¬ä¸€æ­¥ï¼šå¹¶è¡Œåˆ†ææ—¥çº¿ä¸Šæ¶¨
        daily_futures = {executor.submit(analyze_daily_rising, symbol): symbol for symbol in symbols}
        
        # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œåˆ†æçŸ­æœŸæ´»è·ƒ
        short_term_futures = {executor.submit(analyze_short_term_active, symbol): symbol for symbol in symbols}
        
        # ç­‰å¾…æ—¥çº¿ä¸Šæ¶¨ç»“æœ
        for future in as_completed(daily_futures):
            symbol = daily_futures[future]
            try:
                result = future.result()
                if result:
                    daily_rising.append(result)
            except Exception as e:
                logger.error(f"âŒ å¤„ç†{symbol}çš„æ—¥çº¿ä¸Šæ¶¨æ—¶å‡ºé”™: {str(e)}")
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆ†æå…¨å‘¨æœŸä¸Šæ¶¨ï¼ˆåŸºäºæ—¥çº¿ä¸Šæ¶¨çš„ç»“æœï¼‰
        if daily_rising:
            all_cycle_futures = {executor.submit(analyze_all_cycles, coin['symbol'], coin): coin['symbol'] for coin in daily_rising}
            for future in as_completed(all_cycle_futures):
                symbol = all_cycle_futures[future]
                try:
                    result = future.result()
                    if result:
                        all_cycle_rising.append(result)
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†{symbol}çš„å…¨å‘¨æœŸæ—¶å‡ºé”™: {str(e)}")
        
        # ç­‰å¾…çŸ­æœŸæ´»è·ƒç»“æœ
        for future in as_completed(short_term_futures):
            symbol = short_term_futures[future]
            try:
                result = future.result()
                if result:
                    short_term_active.append(result)
            except Exception as e:
                logger.error(f"âŒ å¤„ç†{symbol}çš„çŸ­æœŸæ´»è·ƒæ—¶å‡ºé”™: {str(e)}")

    # æ’åºç»“æœ
    daily_rising.sort(key=lambda x: x.get('change', 0), reverse=True)
    short_term_active.sort(key=lambda x: x.get('ratio', 0), reverse=True)
    all_cycle_rising.sort(key=lambda x: x.get('period_count', 0), reverse=True)

    analysis_time = time.time() - start_time
    logger.info(f"ğŸ“Š åˆ†æç»“æœ: æ—¥çº¿ä¸Šæ¶¨ {len(daily_rising)}ä¸ª, çŸ­æœŸæ´»è·ƒ {len(short_term_active)}ä¸ª, å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨ {len(all_cycle_rising)}ä¸ª")
    logger.info(f"âœ… åˆ†æå®Œæˆ: ç”¨æ—¶{analysis_time:.2f}ç§’")

    return {
        'daily_rising': daily_rising,
        'short_term_active': short_term_active,
        'all_cycle_rising': all_cycle_rising,
        'analysis_time': analysis_time
    }

def get_high_volume_symbols():
    # ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
    if client is None and not init_client():
        logger.error("âŒ æ— æ³•è¿æ¥API")
        return []

    try:
        logger.info("ğŸ“Š è·å–é«˜äº¤æ˜“é‡å¸ç§...")
        tickers = client.futures_ticker()
        filtered = [
            t for t in tickers if float(t.get('quoteVolume', 0)) > 10000000
            and t.get('symbol', '').endswith('USDT')
        ]
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(filtered)} ä¸ªé«˜äº¤æ˜“é‡å¸ç§")
        return [t['symbol'] for t in filtered]
    except Exception as e:
        logger.error(f"âŒ è·å–é«˜äº¤æ˜“é‡å¸ç§å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return []

def analysis_worker():
    global data_cache, current_data_cache
    logger.info("ğŸ”§ æ•°æ®åˆ†æçº¿ç¨‹å¯åŠ¨")

    # åˆå§‹æ•°æ®ä½¿ç”¨é»˜è®¤ç¼“å­˜
    data_cache = {
        "last_updated": "ä»æœªæ›´æ–°",
        "daily_rising": [],
        "short_term_active": [],
        "all_cycle_rising": [],
        "analysis_time": 0,
        "next_analysis_time": "è®¡ç®—ä¸­..."
    }
    current_data_cache = data_cache.copy()

    while True:
        try:
            task = analysis_queue.get()
            if task == "STOP":
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»“æŸåˆ†æçº¿ç¨‹")
                break

            analysis_start = datetime.now(timezone.utc)
            logger.info(f"â±ï¸ å¼€å§‹æ›´æ–°æ•°æ® ({analysis_start.strftime('%Y-%m-%d %H:%M:%S')})...")

            backup_cache = data_cache.copy()
            current_backup = current_data_cache.copy()

            try:
                result = analyze_trends()
                next_analysis_time = get_next_update_time()
                
                new_data = {
                    "last_updated": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"),
                    "daily_rising": result['daily_rising'],
                    "short_term_active": result['short_term_active'],
                    "all_cycle_rising": result['all_cycle_rising'],
                    "analysis_time": result['analysis_time'],
                    "next_analysis_time": next_analysis_time.strftime("%Y-%m-%d %H:%M:%S")
                }
                
                logger.info(f"ğŸ“Š åˆ†æç»“æœå·²ç”Ÿæˆ")
                logger.info(f"å…¨å‘¨æœŸä¸Šæ¶¨å¸ç§æ•°é‡: {len(new_data['all_cycle_rising'])}")
                logger.info(f"æ—¥çº¿ä¸Šæ¶¨å¸ç§æ•°é‡: {len(new_data['daily_rising'])}")
                logger.info(f"çŸ­æœŸæ´»è·ƒå¸ç§æ•°é‡: {len(new_data['short_term_active'])}")
                
                data_cache = new_data
                current_data_cache = new_data.copy()
                logger.info(f"âœ… æ•°æ®æ›´æ–°æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
                data_cache = backup_cache
                current_data_cache = current_backup
                logger.info("ğŸ”„ æ¢å¤å†å²æ•°æ®")

            analysis_end = datetime.now(timezone.utc)
            analysis_duration = (analysis_end - analysis_start).total_seconds()
            logger.info(f"â±ï¸ åˆ†æè€—æ—¶: {analysis_duration:.2f}ç§’")
            
            # è®°å½•ä¸‹ä¸€æ¬¡åˆ†ææ—¶é—´
            next_time = get_next_update_time()
            wait_seconds = (next_time - analysis_end).total_seconds()
            logger.info(f"â³ ä¸‹æ¬¡åˆ†æå°†åœ¨ {wait_seconds:.1f} ç§’å ({next_time.strftime('%Y-%m-%d %H:%M:%S')})")
            
            logger.info("=" * 50)
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())

def schedule_analysis():
    logger.info("â° å®šæ—¶åˆ†æè°ƒåº¦å™¨å¯åŠ¨")
    now = datetime.now(timezone.utc)
    next_time = get_next_update_time()
    initial_wait = (next_time - now).total_seconds()
    logger.info(f"â³ é¦–æ¬¡åˆ†æå°†åœ¨ {initial_wait:.1f} ç§’åå¼€å§‹ ({next_time.strftime('%Y-%m-%d %H:%M:%S')})...")
    time.sleep(max(0, initial_wait))

    while True:
        analysis_start = datetime.now(timezone.utc)
        logger.info(f"ğŸ”” è§¦å‘å®šæ—¶åˆ†æä»»åŠ¡ ({analysis_start.strftime('%Y-%m-%d %H:%M:%S')}")
        analysis_queue.put("ANALYZE")
        
        analysis_duration = (datetime.now(timezone.utc) - analysis_start).total_seconds()
        now = datetime.now(timezone.utc)
        next_time = get_next_update_time()
        wait_time = (next_time - now).total_seconds()

        if wait_time < 0:
            wait_time = 0
        elif analysis_duration > 240:
            wait_time = 0
            logger.warning("âš ï¸ åˆ†æè€—æ—¶è¿‡é•¿ï¼Œç«‹å³å¼€å§‹ä¸‹ä¸€æ¬¡åˆ†æ")
        elif wait_time > 300:
            adjusted_wait = max(60, wait_time - 120)
            logger.info(f"â³ è°ƒæ•´ç­‰å¾…æ—¶é—´: {wait_time:.1f}ç§’ -> {adjusted_wait:.1f}ç§’")
            wait_time = adjusted_wait

        logger.info(f"â³ ä¸‹æ¬¡åˆ†æå°†åœ¨ {wait_time:.1f} ç§’å ({next_time.strftime('%Y-%m-%d %H:%M:%S')})")
        time.sleep(wait_time)

# APIè·¯ç”±
@app.route('/')
def index():
    try:
        return send_from_directory(app.static_folder, 'index.html')
    except Exception as e:
        logger.error(f"âŒ å¤„ç†é¦–é¡µè¯·æ±‚å¤±è´¥: {str(e)}")
        return "Internal Server Error", 500

@app.route('/static/<path:filename>')
def static_files(filename):
    return send_from_directory(app.static_folder, filename)

@app.after_request
def add_cors_headers(response):
    # æ·»åŠ  CORS å¤´
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
    response.headers['Access-Control-Allow-Methods'] = 'GET, OPTIONS'
    return response

@app.route('/api/data', methods=['GET'])
def get_data():
    global current_data_cache
    try:
        logger.info("ğŸ“¡ æ”¶åˆ° /api/data è¯·æ±‚")
        
        # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
        if not current_data_cache or not isinstance(current_data_cache, dict):
            logger.warning("âš ï¸ å½“å‰æ•°æ®ç¼“å­˜æ ¼å¼é”™è¯¯ï¼Œé‡ç½®ä¸ºé»˜è®¤")
            current_data_cache = {
                "last_updated": "ä»æœªæ›´æ–°",
                "daily_rising": [],
                "short_term_active": [],
                "all_cycle_rising": [],
                "analysis_time": 0,
                "next_analysis_time": "è®¡ç®—ä¸­..."
            }
        
        # ç¡®ä¿æ‰€æœ‰æ•°ç»„å…ƒç´ éƒ½æœ‰å¿…è¦çš„å­—æ®µ
        def validate_coins(coins):
            valid_coins = []
            for coin in coins:
                if not isinstance(coin, dict):
                    continue
                if 'symbol' not in coin:
                    coin['symbol'] = 'æœªçŸ¥å¸ç§'
                if 'oi' not in coin:
                    coin['oi'] = 0
                if 'change' not in coin:
                    coin['change'] = 0
                if 'ratio' not in coin:
                    coin['ratio'] = 0
                if 'period_count' not in coin:
                    coin['period_count'] = 0
                if 'period_status' not in coin:
                    coin['period_status'] = {}
                if 'key_resistance' not in coin:
                    coin['key_resistance'] = 0
                valid_coins.append(coin)
            return valid_coins
        
        # ç¡®ä¿æ•°ç»„ç±»å‹æ­£ç¡®
        daily_rising = validate_coins(current_data_cache.get('daily_rising', []))
        short_term_active = validate_coins(current_data_cache.get('short_term_active', []))
        all_cycle_rising = validate_coins(current_data_cache.get('all_cycle_rising', []))
        
        # è¿‡æ»¤æ‰å…¨å‘¨æœŸä¸Šæ¶¨çš„å¸ç§ï¼ˆä¸åœ¨æ—¥çº¿ä¸Šæ¶¨åˆ—è¡¨æ˜¾ç¤ºï¼‰
        all_cycle_symbols = {coin['symbol'] for coin in all_cycle_rising}
        filtered_daily_rising = [coin for coin in daily_rising if coin['symbol'] not in all_cycle_symbols]
        
        data = {
            'last_updated': current_data_cache.get('last_updated', ""),
            'daily_rising': filtered_daily_rising,
            'short_term_active': short_term_active,
            'all_cycle_rising': all_cycle_rising,
            'analysis_time': current_data_cache.get('analysis_time', 0),
            'next_analysis_time': current_data_cache.get('next_analysis_time', "")
        }
        
        logger.info(f"ğŸ“¦ è¿”å›æ•°æ®: æ—¥çº¿ä¸Šæ¶¨ {len(data['daily_rising'])}ä¸ª, å…¨å‘¨æœŸä¸Šæ¶¨ {len(data['all_cycle_rising'])}ä¸ª")
        return jsonify(data)
    
    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
        # è¿”å›æœ‰ç»“æ„çš„ç©ºæ•°æ®è€Œä¸æ˜¯é”™è¯¯
        return jsonify({
            'last_updated': datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"),
            'daily_rising': [],
            'short_term_active': [],
            'all_cycle_rising': [],
            'analysis_time': 0,
            'next_analysis_time': datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
        })

@app.route('/api/resistance_levels/<symbol>', methods=['GET'])
def get_resistance_levels(symbol):
    try:
        # éªŒè¯å¸ç§æ ¼å¼
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return jsonify({'error': 'Invalid symbol format'}), 400

        logger.info(f"ğŸ“Š è·å–é˜»åŠ›ä½æ•°æ®: {symbol}")
        levels = calculate_resistance_levels(symbol)
        return jsonify(levels)
    except Exception as e:
        logger.error(f"âŒ è·å–é˜»åŠ›ä½æ•°æ®å¤±è´¥: {symbol}, {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/oi_chart/<symbol>/<period>', methods=['GET'])
def get_oi_chart_data(symbol, period):
    try:
        # éªŒè¯å¸ç§æ ¼å¼
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return jsonify({'error': 'Invalid symbol format'}), 400

        if period not in PERIOD_MINUTES:
            logger.warning(f"âš  ä¸æ”¯æŒçš„å‘¨æœŸ: {period}")
            return jsonify({'error': 'Unsupported period'}), 400

        logger.info(f"ğŸ“ˆ è·å–æŒä»“é‡å›¾è¡¨æ•°æ®: symbol={symbol}, period={period}")
        oi_data = get_open_interest(symbol, period, use_cache=True)
        return jsonify({
            'data': oi_data.get('series', []),
            'timestamps': oi_data.get('timestamps', [])
        })
    except Exception as e:
        logger.error(f"âŒ è·å–æŒä»“é‡å›¾è¡¨æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    try:
        # æ£€æŸ¥Binanceè¿æ¥
        binance_status = 'ok'
        if client:
            try:
                client.get_server_time()
            except:
                binance_status = 'error'
        else:
            binance_status = 'not initialized'
        
        return jsonify({
            'status': 'healthy',
            'binance': binance_status,
            'last_updated': current_data_cache.get('last_updated', 'N/A'),
            'next_analysis_time': current_data_cache.get('next_analysis_time', 'N/A'),
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e)
        }), 500

def start_background_threads():
    # ç¡®ä¿é™æ€æ–‡ä»¶å¤¹å­˜åœ¨
    static_path = app.static_folder
    if not os.path.exists(static_path):
        os.makedirs(static_path)
    
    # ç¡®ä¿ index.html å­˜åœ¨
    index_path = os.path.join(static_path, 'index.html')
    if not os.path.exists(index_path):
        with open(index_path, 'w') as f:
            f.write("<html><body><h1>è¯·å°†å‰ç«¯æ–‡ä»¶æ”¾å…¥staticç›®å½•</h1></body></html>")
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    if not init_client():
        logger.critical("âŒ æ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯")
        return False
    
    # ç¡®ä¿ç¼“å­˜ä¸­æœ‰åˆå§‹æ•°æ®
    global current_data_cache
    if not current_data_cache or not current_data_cache.get('last_updated') or current_data_cache.get('last_updated') == "ä»æœªæ›´æ–°":
        # åˆ›å»ºåˆå§‹æ•°æ®è®°å½•
        current_data_cache = {
            "last_updated": "ç­‰å¾…é¦–æ¬¡åˆ†æ",
            "daily_rising": [],
            "short_term_active": [],
            "all_cycle_rising": [],
            "analysis_time": 0,
            "next_analysis_time": "è®¡ç®—ä¸­..."
        }
        logger.info("ğŸ†• åˆ›å»ºåˆå§‹å†…å­˜æ•°æ®è®°å½•")
    
    # å¯åŠ¨åå°çº¿ç¨‹
    worker_thread = threading.Thread(target=analysis_worker, name="AnalysisWorker")
    worker_thread.daemon = True
    worker_thread.start()
    
    scheduler_thread = threading.Thread(target=schedule_analysis, name="AnalysisScheduler")
    scheduler_thread.daemon = True
    scheduler_thread.start()
    
    logger.info("âœ… åå°çº¿ç¨‹å¯åŠ¨æˆåŠŸ")
    return True

if __name__ == '__main__':
    PORT = int(os.environ.get("PORT", 9600))
    
    logger.info("=" * 50)
    logger.info(f"ğŸš€ å¯åŠ¨åŠ å¯†è´§å¸æŒä»“é‡åˆ†ææœåŠ¡ (å†…å­˜å­˜å‚¨ç‰ˆ)")
    logger.info(f"ğŸ”‘ APIå¯†é’¥: {API_KEY[:5]}...{API_KEY[-3:] if API_KEY else 'æœªè®¾ç½®'}")
    logger.info(f"ğŸŒ æœåŠ¡ç«¯å£: {PORT}")
    logger.info("ğŸ’¾ æ•°æ®å­˜å‚¨: å†…å­˜å­˜å‚¨ (æ— æŒä¹…åŒ–)")
    logger.info("=" * 50)
    
    if start_background_threads():
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡å™¨...")
        app.run(host='0.0.0.0', port=PORT, debug=False)
    else:
        logger.critical("ğŸ”¥ æ— æ³•å¯åŠ¨æœåŠ¡ï¼Œè¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—")
