#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import time
import re
import json
import math
import sqlite3
import requests
import threading
import queue
import logging
import traceback
import urllib3
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import Flask, jsonify, send_from_directory, request
from binance.client import Client

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è®¾ç½®æ—¥å¿—çº§åˆ« (é»˜è®¤æ”¹ä¸ºINFO)
LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()

# è·å–æ ¹è®°å½•å™¨å¹¶è®¾ç½®çº§åˆ«
root_logger = logging.getLogger()
root_logger.setLevel(getattr(logging, LOG_LEVEL))

# é™ä½ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("binance").setLevel(logging.WARNING)
logging.getLogger("requests").setLevel(logging.WARNING)

# åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(getattr(logging, LOG_LEVEL))

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler('app.log')
file_handler.setLevel(getattr(logging, LOG_LEVEL))

# åˆ›å»ºæ ¼å¼å™¨
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)

# æ·»åŠ å¤„ç†å™¨
root_logger.addHandler(console_handler)
root_logger.addHandler(file_handler)

logger = logging.getLogger(__name__)
logger.info(f"âœ… æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º: {LOG_LEVEL}")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
app = Flask(__name__, static_folder=os.path.join(BASE_DIR, 'static'), static_url_path='/static')

# Binance API é…ç½®
API_KEY = os.environ.get('BINANCE_API_KEY', 'your_api_key_here')
API_SECRET = os.environ.get('BINANCE_API_SECRET', 'your_api_secret_here')
client = None

# ==================== æ•°æ®ç¼“å­˜ ====================
data_cache = {
    "last_updated": None,
    "daily_rising": [],
    "short_term_active": [],
    "all_cycle_rising": [],
    "analysis_time": 0,
    "next_analysis_time": None
}

current_data_cache = data_cache.copy()
oi_data_cache = {}
resistance_cache = {}
RESISTANCE_CACHE_EXPIRATION = 24 * 3600

# ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œçº¿ç¨‹é—´é€šä¿¡
analysis_queue = queue.Queue()
executor = ThreadPoolExecutor(max_workers=10)

PERIOD_MINUTES = {
    '5m': 5,
    '15m': 15,
    '30m': 30,
    '1h': 60,
    '2h': 120,
    '4h': 240,
    '6h': 360,
    '12h': 720,
    '1d': 1440
}

RESISTANCE_INTERVALS = ['1m', '3m', '5m', '15m', '30m', '1h', '2h', '4h', '6h', '8h', '12h', '1d', '3d', '1w', '1M']
ALL_PERIODS = ['5m', '15m', '30m', '1h', '2h', '4æé€Ÿèµ›è½¦å¼€å¥–ç½‘
def init_db():
    try:
        logger.debug("ğŸ› ï¸ å¼€å§‹åˆå§‹åŒ–æ•°æ®åº“...")
        conn = sqlite3.connect('data.db')
        c = conn.cursor()

        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='crypto_data'")
        table_exists = c.fetchone()

        if not table_exists:
            logger.info("ğŸ› ï¸ åˆ›å»ºæ•°æ®åº“è¡¨...")
            c.execute('''CREATE TABLE crypto_data
                        (id INTEGER PRIMARY KEY, 
                        last_updated TEXT,
                        daily_rising TEXT,
                        short_term_active TEXT,
                        all_cycle_rising TEXT,
                        analysis_time REAL,
                        next_analysis_time TEXT,
                        resistance_data TEXT)''')
            conn.commit()
            logger.info("âœ… æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ")
        else:
            # æ£€æŸ¥è¡¨ç»“æ„æ˜¯å¦æœ€æ–°
            c.execute("PRAGMA table_info(crypto_data)")
            columns = [col[1] for col in c.fetchall()]
            if 'next_analysis_time' not in columns:
                logger.info("ğŸ› ï¸ æ›´æ–°æ•°æ®åº“è¡¨ç»“æ„...")
                c.execute("ALTER TABLE crypto_data ADD COLUMN next_analysis_time TEXT")
                conn.commit()
                logger.info("âœ… æ•°æ®åº“è¡¨ç»“æ„æ›´æ–°æˆåŠŸ")
            else:
                logger.info("âœ… æ•°æ®åº“è¡¨å·²å­˜åœ¨ä¸”ç»“æ„æœ€æ–°")

        conn.close()
        logger.debug("ğŸ› ï¸ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        try:
            logger.warning("ğŸ—‘ï¸ å°è¯•åˆ é™¤æŸåçš„æ•°æ®åº“æ–‡ä»¶...")
            os.remove('data.db')
            logger.warning("ğŸ—‘ï¸ åˆ é™¤æŸåçš„æ•°æ®åº“æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ•°æ®åº“")
            init_db()
        except Exception as e2:
            logger.critical(f"ğŸ”¥ æ— æ³•ä¿®å¤æ•°æ®åº“: {str(e2)}")
            logger.critical(traceback.format_exc())

def save_to_db(data):
    try:
        logger.debug("ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
        conn = sqlite3.connect('æé€Ÿèµ›è½¦å¼€å¥–ç½‘
        c = conn.cursor()

        # ç¡®ä¿è¡¨å­˜åœ¨
        c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='crypto_data'")
        if not c.fetchone():
            logger.warning("âš ï¸ æ•°æ®åº“è¡¨ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            init_db()

        resistance_json = json.dumps(resistance_cache)

        # æ’å…¥æˆ–æ›´æ–°æ•°æ®
        c.execute(
            "INSERT INTO crypto_data VALUES (NULL, ?, ?, ?, ?, ?, ?, ?)",
            (data['last_updated'], 
             json.dumps(data['daily_rising']),
             json.dumps(data['short_term_active']),
             json.dumps(data['all_cycle_rising']), 
             data['analysis_time'],
             data.get('next_analysis_time', None),
             resistance_json))
        conn.commit()
        conn.close()
        logger.info("ğŸ’¾ æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())

def get_last_valid_data():
    try:
        logger.debug("ğŸ” å°è¯•è·å–æœ€åæœ‰æ•ˆæ•°æ®...")
        conn = sqlite3.connect('data.db')
        c = conn.cursor()

        c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='crypto_data'")
        if not c.fetchone():
            logger.warning("âš ï¸ æ•°æ®åº“è¡¨ä¸å­˜åœ¨")
            return None

        c.execute("SELECT * FROM crypto_data ORDER BY id DESC LIMIT 1")
        row = c.fetchone()
        conn.close()

        if row:
            logger.debug(f"ğŸ” æ‰¾åˆ°æœ€åæœ‰æ•ˆæ•°æ®: ID={row[0]}, æ—¶é—´={row[1]}")
            resistance_data = json.loads(row[7]) if row[7] else {}
            for symbol, data in resistance_data.items():
                resistance_cache[symbol] = data

            return {
                'last_updated': row[1],
                'daily_rising': json.loads(row[2]) if row[2] else [],
                'short_term_active': json.loads(row[3]) if row[3] else [],
                'all_cycle_rising': json.loads(row[4]) if row[4] else [],
                'analysis_time': row[5] or 0,
                'next_analysis_time': row[6]
            }
        logger.debug("ğŸ” æ•°æ®åº“ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        return None
    except Exception as e:
        logger.error(f"âŒ è·å–æœ€åæœ‰æ•ˆæ•°æ®å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def init_client():
    global client
    max_retries = 3
    retry_delay = 5
    
    for attempt in range(max_retries):
        try:
            logger.debug(f"ğŸ”§ å°è¯•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ (ç¬¬{attempt+1}æ¬¡)...")
            client_params = {
                'api_key': API_KEY, 
                'api_secret': API_SECRET,
                'requests_params': {'timeout': 30}
            }

            client = Client(**client_params)
            
            # æµ‹è¯•è¿æ¥
            server_time = client.get_server_time()
            logger.info(f"âœ… Binanceå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒæœåŠ¡å™¨æ—¶é—´: {datetime.fromtimestamp(server_time['serverTime']/1000)}")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            if attempt < max_retries - 1:
                logger.info(f"ğŸ”„ {retry_delay}ç§’åé‡è¯•åˆå§‹åŒ–å®¢æˆ·ç«¯...")
                time.sleep(retry_delay)
            else:
                logger.critical("ğŸ”¥ æ— æ³•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return False
    return False

def get_next_update_time(period):
    minutes = PERIOD_MINUTES.get(period, 5)
    now = datetime.now(timezone.utc)

    if period.endswith('m'):
        period_minutes = int(period[:-1])
        current_minute = now.minute
        current_period_minute = (current_minute // period_minutes) * period_minutes
        current_period_start = now.replace(minute=current_period_minute,
                                           second=0,
                                           microsecond=0)
    elif period.endswith('h'):
        period_hours = int(period[:-1])
        current_hour = now.hour
        current_period_hour = (current_hour // period_hours) * period_hours
        current_period_start = now.replace(hour=current_period_hour,
                                           minute=0,
                                           second=0,
                                           microsecond=0)
    else:  # 1d
        current_period_start = now.replace(hour=0,
                                           minute=0,
                                           second=0,
                                           microsecond=0)

    next_update = current_period_start + timedelta(minutes=minutes)

    if next_update < now:
        next_update += timedelta(minutes=minutes)

    return next_update

def get_open_interest(symbol, period, use_cache=True):
    try:
        # æ›´å®½æ¾çš„å¸ç§æ ¼å¼éªŒè¯ (å…è®¸æ•°å­—å’Œæ›´çŸ­çš„ä»£ç )
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

        if period not in PERIOD_MINUTES:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å‘¨æœŸ: {period}, ä½¿ç”¨é»˜è®¤5m")
            period = '5m'

        current_time = datetime.now(timezone.utc)

        cache_key = f"{symbol}_{period}"
        if use_cache and oi_data_cache.get(cache_key):
            cached_data = oi_data_cache[cache_key]
            if 'next_update' in cached_data and cached_data['next_update'] > current_time:
                logger.debug(f"ğŸ“ˆ ä½¿ç”¨ç¼“å­˜æ•°æ®: {symbol} {period}")
                return {
                    'series': cached_data['data']['series'].copy(),
                    'timestamps': cached_data['data']['timestamps'].copy(),
                    'cache_time': cached_data['data']['cache_time']
                }

        logger.info(f"ğŸ“¡ è¯·æ±‚æŒä»“é‡æ•°æ®: symbol={symbol}, period={period}")
        url = "https://fapi.binance.com/futures/data/openInterestHist"
        params = {'symbol': symbol, 'period': period, 'limit': 30}

        response = requests.get(url, params=params, timeout=15)
        logger.debug(f"ğŸ“¡ å“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code != 200:
            logger.error(f"âŒ è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: HTTP {response.status_code} - {response.text}")
            return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

        data = response.json()
        logger.debug(f"ğŸ“¡ å“åº”æ•°æ®: {data[:1]}...")

        if not isinstance(data, list):
            logger.error(f"âŒ æ— æ•ˆçš„æŒä»“é‡æ•°æ®æ ¼å¼: {symbol} {period} - å“åº”: {data}")
            return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

        if len(data) == 0:
            logger.warning(f"âš ï¸ {symbol}çš„{period}æŒä»“é‡æ•°æ®ä¸ºç©º")
            return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

        data.sort(key=lambda x: x['timestamp'])
        oi_series = [float(item['sumOpenInterest']) for item in data]
        timestamps = [item['timestamp'] for item in data]
        cache_time = datetime.now(timezone.utc).isoformat()

        if len(oi_series) < 5:
            logger.warning(f"âš ï¸ {symbol}çš„{period}æŒä»“é‡æ•°æ®ä¸è¶³: åªæœ‰{len(oi_series)}ä¸ªç‚¹")
            return {'series': [], 'timestamps': [], 'cache_time': cache_time}

        oi_data = {
            'series': oi_series, 
            'timestamps': timestamps,
            'cache_time': cache_time
        }
        next_update = get_next_update_time(period)

        oi_data_cache[cache_key] = {
            'data': oi_data,
            'next_update': next_update
        }

        logger.info(f"ğŸ“ˆ è·å–æ–°æ•°æ®: {symbol} {period} ({len(oi_series)}ç‚¹)")
        return oi_data
    except Exception as e:
        logger.error(f"âŒ è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return {'series': [], 'timestamps': [], 'cache_time': datetime.now(timezone.utc).isoformat()}

def is_latest_highest(oi_data):
    if len(oi_data) < 30:
        logger.debug("æŒä»“é‡æ•°æ®ä¸è¶³30ä¸ªç‚¹")
        return False

    latest_value = oi_data[-1]
    prev_data = oi_data[-30:-1]

    if not prev_data:
        logger.debug("æ— å†å²æ•°æ®ç”¨äºæ¯”è¾ƒ")
        return False

    result = latest_value > max(prev_data)
    logger.debug(f"æŒä»“é‡åˆ›æ–°é«˜æ£€æŸ¥: æœ€æ–°å€¼={latest_value}, å†å²æœ€å¤§å€¼={max(prev_data)}, ç»“æœ={result}")
    return result

def calculate_resistance_levels(symbol):
    try:
        logger.debug(f"ğŸ“Š è®¡ç®—é˜»åŠ›ä½: {symbol}")
        
        # ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
        if client is None:
            logger.warning("âš ï¸ Binanceå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–")
            if not init_client():
                logger.error("âŒ æ— æ³•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ï¼Œæ— æ³•è®¡ç®—é˜»åŠ›ä½")
                return {}
        
        now = time.time()
        if symbol in resistance_cache:
            cache_data = resistance_cache[symbol]
            if cache_data['expiration'] > now:
                logger.debug(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜çš„é˜»åŠ›ä½æ•°æ®: {symbol}")
                return cache_data['levels']

        levels = {}
        logger.debug(f"ğŸ“Š å¼€å§‹è®¡ç®—{symbol}çš„é˜»åŠ›ä½")

        for interval in RESISTANCE_INTERVALS:
            try:
                logger.debug(f"ğŸ“Š è·å–Kçº¿æ•°æ®: {symbol} {interval}")
                klines = client.futures_klines(symbol=symbol, interval=interval, limit=100)
                
                if not klines or len(klines) < 10:
                    logger.warning(f"âš ï¸ {symbol}åœ¨{interval}çš„Kçº¿æ•°æ®ä¸è¶³")
                    continue

                high_prices = [float(k[2]) for k in klines]
                low_prices = [float(k[3]) for k in klines]
                recent_high = max(high_prices[-10:])
                recent_low = min(low_prices[-10:])

                lookback = min(30, len(high_prices))
                recent_max = max(high_prices[-lookback:])
                recent_min = min(low_prices[-lookback:])

                if recent_max <= recent_min:
                    logger.warning(f"âš ï¸ {symbol}åœ¨{interval}çš„æœ€è¿‘é«˜ç‚¹å’Œä½ç‚¹æ— æ•ˆ")
                    continue

                fib_levels = {
                    '0.236': recent_max - (recent_max - recent_min) * 0.236,
                    '0.382': recent_max - (recent_max - recent_min) * 0.382,
                    '0.5': recent_max - (recent_max - recent_min) * 0.5,
                    '0.618': recent_max - (recent_max - recent_min) * 0.618,
                    '0.786': recent_max - (recent_max - recent_min) * 0.786,
                    '1.0': recent_max
                }

                price_levels = [recent_high, recent_low]
                price_levels.extend(fib_levels.values())

                base = 10 ** (math.floor(math.log10(recent_high)) - 1)
                integer_level = round(recent_high / base) * base
                price_levels.append(integer_level)

                price_levels = sorted(set(price_levels), reverse=True)

                resistance = [p for p in price_levels if p > recent_min and p <= recent_max]
                support = [p for p in price_levels if p < recent_min or p > recent_max]

                levels[interval] = {
                    'resistance': resistance[:3],
                    'support': support[:3]
                }
                
                logger.debug(f"ğŸ“Š {symbol}åœ¨{interval}çš„é˜»åŠ›ä½è®¡ç®—å®Œæˆ")
            except Exception as e:
                logger.error(f"è®¡ç®—{symbol}åœ¨{interval}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
                logger.error(traceback.format_exc())
                levels[interval] = {
                    'resistance': [],
                    'support': []
                }

        resistance_cache[symbol] = {
            'levels': levels,
            'expiration': now + RESISTANCE_CACHE_EXPIRATION
        }
        logger.info(f"ğŸ“Š {symbol}çš„é˜»åŠ›ä½è®¡ç®—å®Œæˆ")
        return levels
    except Exception as e:
        logger.error(f"è®¡ç®—{symbol}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return {}

def analyze_symbol(symbol):
    try:
        logger.info(f"ğŸ” å¼€å§‹åˆ†æå¸ç§: {symbol}")
        symbol_result = {
            'symbol': symbol,
            'daily_rising': None,
            'short_term_active': None,
            'all_cycle_rising': None,
            'rising_periods': [],
            'period_status': {p: False for p in ALL_PERIODS},
            'period_count': 0,
            'oi_data': {}
        }

        # 1. è·å–æ—¥çº¿æŒä»“é‡æ•°æ®
        logger.debug(f"ğŸ“Š è·å–æ—¥çº¿æŒä»“é‡: {symbol}")
        daily_oi = get_open_interest(symbol, '1d', use_cache=True)
        symbol_result['oi_data']['1d'] = daily_oi
        daily_series = daily_oi['series'] if daily_oi and 'series' in daily_oi else []
        
        logger.debug(f"ğŸ“Š æ—¥çº¿æŒä»“é‡æ•°æ®é•¿åº¦: {len(daily_series)}")

        # 2. æ£€æŸ¥æ—¥çº¿ä¸Šæ¶¨æ¡ä»¶
        if len(daily_series) >= 30:
            daily_up = is_latest_highest(daily_series)
            loggeræé€Ÿèµ›è½¦å¼€å¥–ç½‘
            if daily_up:
                daily_change = ((daily_series[-1] - daily_series[-30]) / daily_series[-30]) * 100
                logger.debug(f"ğŸ“Š æ—¥çº¿å˜åŒ–: {daily_change:.2f}%")
                
                symbol_result['daily_rising'] = {
                    'symbol': symbol,
                    'oi': daily_series[-1],
                    'change': round(daily_change, 2),
                    'period_count': 1
                }
                symbol_result['rising_periods'].append('1d')
                symbol_result['period_status']['1d'] = True
                symbol_result['period_count'] = 1

                # 3. å…¨å‘¨æœŸåˆ†æ
                logger.debug(f"ğŸ“Š å¼€å§‹å…¨å‘¨æœŸåˆ†æ: {symbol}")
                all_intervals_up = True
                for period in ALL_PERIODS:
                    if period == '1d':
                        continue

                    logger.debug(f"ğŸ“Š åˆ†æå‘¨æœŸ: {period}")
                    oi_data = get_open_interest(symbol, period, use_cache=True)
                    symbol_result['oi_data'][period] = oi_data
                    oi_series = oi_data['series'] if oi_data and 'series' in oi_data else []
                    
                    if len(oi_series) < 30:
                        logger.debug(f"ğŸ“Š æ•°æ®ä¸è¶³: {symbol} {period}åªæœ‰{len(oi_series)}ä¸ªç‚¹")
                        status = False
                    else:
                        status = is_latest_highest(oi_series)
                        
                    symbol_result['period_status'][period] = status
                    logger.debug(f"ğŸ“Š å‘¨æœŸçŠ¶æ€: {period} = {status}")

                    if status:
                        symbol_result['rising_periods'].append(period)
                        symbol_result['period_count'] += 1
                    else:
                        all_intervals_up = False

                if all_intervals_up:
                    logger.debug(f"ğŸ“Š å…¨å‘¨æœŸä¸Šæ¶¨: {symbol}")
                    symbol_result['all_cycle_rising'] = {
                        'symbol': symbol,
                        'oi': daily_series[-1],
                        'change': round(daily_change, 2),
                        'periods': symbol_result['rising_periods'],
                        'period_status': symbol_result['period_status'],
                        'period_count': symbol_result['period_count']
                    }

        # 4. çŸ­æœŸæ´»è·ƒåº¦åˆ†æ
        logger.debug(f"ğŸ“Š åˆ†æçŸ­æœŸæ´»è·ƒåº¦: {symbol}")
        min5_oi = get_open_interest(symbol, '5m', use_cache=True)
        
        # æ·»åŠ ç©ºæ•°æ®æ£€æŸ¥
        if not min5_oi or not min5_oi.get('series'):
            logger.warning(f"âš ï¸ {symbol} 5mæŒä»“é‡æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡çŸ­æœŸæ´»è·ƒåº¦åˆ†æ")
            min5_series = []
        else:
            min5_series = min5_oi['series']
            symbol_result['oi_data']['5m'] = min5_oi

        if len(min5_series) >= 30 and len(daily_series) >= 30:
            min5_max = max(min5_series[-30:])
            daily_avg = sum(daily_series[-30:]) / 30
            logger.debug(f"ğŸ“Š çŸ­æœŸæœ€å¤§å€¼: {min5_max}, æ—¥å‡å€¼: {daily_avg}")

            if min5_max > 0 and daily_avg > 0:
                ratio = min5_max / daily_avg
                logger.debug(f"ğŸ“Š çŸ­æœŸæ´»è·ƒæ¯”ç‡: {ratio:.2f}")
                
                if ratio > 1.5:
                    logger.debug(f"ğŸ“Š çŸ­æœŸæ´»è·ƒ: {symbol} æ¯”ç‡={ratio:.2f}")
                    symbol_result['short_term_active'] = {
                        'symbol': symbol,
                        'oi': min5_series[-1],
                        'ratio': round(ratio, 2),
                        'period_status': symbol_result['period_status'],
                        'period_count': symbol_result['period_count']
                    }

        if symbol_result['daily_rising']:
            symbol_result['daily_rising']['period_status'] = symbol_result['period_status']
            symbol_result['daily_rising']['period_count'] = symbol_result['period_count']

        logger.info(f"âœ… å®Œæˆåˆ†æå¸ç§: {symbol}")
        return symbol_result
    except Exception as e:
        logger.error(f"âŒ å¤„ç†{symbol}æ—¶å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'symbol': symbol,
            'period_status': {p: False for p in ALL_PERIODS},
            'period_count': 0,
            'oi_data': {}
        }

def analyze_trends():
    start_time = time.time()
    logger.info("ğŸ” å¼€å§‹åˆ†æå¸ç§è¶‹åŠ¿...")
    symbols = get_high_volume_symbols()
    
    if not symbols:
        logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°é«˜äº¤æ˜“é‡å¸ç§")
        return data_cache

    logger.info(f"ğŸ” å¼€å§‹åˆ†æ {len(symbols)} ä¸ªå¸ç§")

    daily_rising = []
    short_term_active = []
    all_cycle_rising = []

    futures = {
        executor.submit(analyze_symbol, symbol): symbol
        for symbol in symbols
    }
    processed = 0
    total_symbols = len(symbols)

    for future in as_completed(futures):
        processed += 1
        symbol = futures[future]

        try:
            result = future.result()
            if result.get('daily_rising'):
                daily_rising.append(result['daily_rising'])
            if result.get('short_term_active'):
                short_term_active.append(result['short_term_active'])
            if result.get('all_cycle_rising'):
                all_cycle_rising.append(result['all_cycle_rising'])
        except Exception as e:
            logger.error(f"âŒ å¤„ç†{symbol}æ—¶å‡ºé”™: {str(e)}")

        if processed % max(1, total_symbols // 10) == 0 or processed == total_symbols:
            logger.info(f"â³ åˆ†æè¿›åº¦: {processed}/{total_symbols} ({int(processed/total_symbols*100)}%)")

    daily_rising.sort(key=lambda x: (x.get('period_count', 0), x.get('change', 0)), reverse=True)
    short_term_active.sort(key=lambda x: (æé€Ÿèµ›è½¦å¼€å¥–ç½‘
    all_cycle_rising.sort(key=lambda x: (x.get('period_count', 0), x.get('change', 0)), reverse=True)

    logger.info(f"ğŸ“Š åˆ†æç»“æœ: æ—¥çº¿ä¸Šæ¶¨ {len(daily_rising)}ä¸ª, çŸ­æœŸæ´»è·ƒ {len(short_term_active)}ä¸ª, å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨ {len(all_cycle_rising)}ä¸ª")
    analysis_time = time.time() - start_time
    logger.info(f"âœ… åˆ†æå®Œæˆ: ç”¨æ—¶{analysis_time:.2f}ç§’")

    return {
        'daily_rising': daily_rising,
        'short_term_active': short_term_active,
        'all_cycle_rising': all_cycle_rising,
        'analysis_time': analysis_time
    }

def get_high_volume_symbols():
    # ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
    if client is None:
        logger.warning("âš ï¸ Binanceå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–")
        if not init_client():
            logger.error("âŒ æ— æ³•è¿æ¥API")
            return []

    try:
        logger.info("ğŸ“Š è·å–é«˜äº¤æ˜“é‡å¸ç§...")
        tickers = client.futures_ticker()
        filtered = [
            t for t in tickers if float(t.get('quoteVolume', 0)) > 10000000
            and t.get('symbol', '').endswith('USDT')
        ]
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(filtered)} ä¸ªé«˜äº¤æ˜“é‡å¸ç§")
        logger.debug(f"ğŸ“Š å‰5ä¸ªå¸ç§: {[t['symbol'] for t in filtered[:5]]}")
        return [t['symbol'] for t in filtered]
    except Exception as e:
        logger.error(f"âŒ è·å–é«˜äº¤æ˜“é‡å¸ç§å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return []

def analysis_worker():
    global data_cache, current_data_cache
    logger.info("ğŸ”§ æ•°æ®åˆ†æçº¿ç¨‹å¯åŠ¨")
    init_db()

    initial_data = get_last_valid_data()
    if initial_data:
        logger.info("ğŸ” åŠ è½½å†å²æ•°æ®")
        data_cache = initial_data
        current_data_cache = data_cache.copy()
        logger.debug(f"ğŸ” åŠ è½½çš„æ•°æ®: {json.dumps(initial_data, indent=2)}")
    else:
        logger.info("ğŸ†• æ²¡æœ‰å†å²æ•°æ®ï¼Œå°†è¿›è¡Œé¦–æ¬¡åˆ†æ")

    while True:
        try:
            task = analysis_queue.get()
            if task == "STOP":
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»“æŸåˆ†æçº¿ç¨‹")
                break

            analysis_start = datetime.now(timezone.utc)
            logger.info(f"â±ï¸ å¼€å§‹æ›´æ–°æ•°æ® ({analysis_start.strftime('%Y-%m-%d %H:%M:%S')})...")

            backup_cache = data_cache.copy()
            current_backup = current_data_cache.copy()

            try:
                result = analyze_trends()
                next_analysis_time = get_next_update_time('5m')
                
                new_data = {
                    "last_updated": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"),
                    "daily_rising": result['daily_rising'],
                    "short_term_active": result['short_term_active'],
                    "all_cycle_rising": result['all_cycle_rising'],
                    "analysis_time": result['analysis_time'],
                    "next_analysis_time": next_analysis_time.strftime("%Y-%m-%d %H:%M:%S")
                }
                
                logger.debug(f"ğŸ“Š åˆ†æç»“æœ: {json.dumps(new_data, indent=2)}")

                data_cache = new_data
                save_to_db(new_data)
                current_data_cache = new_data.copy()
                logger.info(f"âœ… æ•°æ®æ›´æ–°æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
                data_cache = backup_cache
                current_data_cache = current_backup
                logger.info("ğŸ”„ æ¢å¤å†å²æ•°æ®")

            analysis_end = datetime.now(timezone.utc)
            analysis_duration = (analysis_end - analysis_start).total_seconds()
            logger.info(f"â±ï¸ åˆ†æè€—æ—¶: {analysis_duration:.2f}ç§’")
            
            # è®°å½•ä¸‹ä¸€æ¬¡åˆ†ææ—¶é—´
            next_time = get_next_update_time('5m')
            wait_seconds = (next_time - analysis_end).total_seconds()
            logger.info(f"â³ ä¸‹æ¬¡åˆ†æå°†åœ¨ {wait_seconds:.1f} ç§’å ({next_time.strftime('%Y-%m-%d %H:%M:%S')})")
            
            logger.info("=" * 50)

        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
        analysis_queue.task_done()

def schedule_analysis():
    logger.info("â° å®šæ—¶åˆ†æè°ƒåº¦å™¨å¯åŠ¨")
    now = datetime.now(timezone.utc)
    current_minute = now.minute
    next_minute = ((current_minute // 5) + 1) * 5
    if next_minute >= 60:
        next_minute = 0
        next_time = now.replace(hour=now.hour + 1, minute=next_minute, second=0, microsecond=0)
    else:
        next_time = now.replace(minute=next_minute, second=0, microsecond=0)

    initial_wait = (next_time - now).total_seconds()
    logger.info(f"â³ é¦–æ¬¡åˆ†æå°†åœ¨ {initial_wait:.1f} ç§’åå¼€å§‹ ({next_time.strftime('%Y-%m-%d %H:%M:%S')})...")
    time.sleep(initial_wait)

    while True:
        analysis_start = datetime.now(timezone.utc)
        logger.info(f"ğŸ”” è§¦å‘å®šæ—¶åˆ†æä»»åŠ¡ ({analysis_start.strftime('%Y-%m-%d %H:%M:%S')})")
        analysis_queue.put("ANALYZE")
        analysis_queue.join()

        analysis_duration = (datetime.now(timezone.utc) - analysis_start).total_seconds()
        now = datetime.now(timezone.utc)
        current_minute = now.minute
        next_minute = ((current_minute // 5) + 1) * 5
        if next_minute >= 60:
            next_minute = 0
            next_time = now.replace(hour=now.hour + 1, minute=next_minute, second=0, microsecond=0)
        else:
            next_time = now.replace(minute=next_minute, second=0, microsecond=0)

        wait_time = (next_time - now).total_seconds()

        if wait_time < 0:
            wait_time = 0
        elif analysis_duration > 240:
            wait_time = 0
            logger.warning("âš ï¸ åˆ†æè€—æ—¶è¿‡é•¿ï¼Œç«‹å³å¼€å§‹ä¸‹ä¸€æ¬¡åˆ†æ")
        elif wait_time > 300:
            adjusted_wait = max(60, wait_time - 120)
            logger.info(f"â³ è°ƒæ•´ç­‰å¾…æ—¶é—´: {wait_time:.1f}ç§’ -> {adjusted_wait:.1f}ç§’")
            wait_time = adjusted_wait

        logger.info(f"â³ ä¸‹æ¬¡åˆ†æå°†åœ¨ {wait_time:.1f} ç§’å ({next_time.strftime('%Y-%m-%d %H:%M:%S')})")
        time.sleep(wait_time)

# APIè·¯ç”±
@app.route('/')
def index():
    try:
        logger.debug("ğŸŒ å¤„ç†é¦–é¡µè¯·æ±‚")
        static_path = app.static_folder
        if static_path is None:
            return "é™æ€æ–‡ä»¶è·¯å¾„æœªé…ç½®", 500

        index_path = os.path.join(static_path, 'index.html')
        if not os.path.exists(index_path):
            return "index.html not found", 404
        return send_from_directory(static_path, 'index.html', mimetype='text/html')
    except Exception as e:
        logger.error(f"âŒ å¤„ç†é¦–é¡µè¯·æ±‚å¤±è´¥: {str(e)}")
        return "Internal Server Error", 500

@app.route('/static/<path:filename>')
def static_files(filename):
    try:
        logger.debug(f"ğŸ“ è¯·æ±‚é™æ€æ–‡ä»¶: {filename}")
        static_path = app.static_folder
        if static_path is None:
            return "é™æ€æ–‡ä»¶è·¯å¾„æœªé…ç½®", 500

        return send_from_directory(static_path, filename)
    except Exception as e:
        logger.error(f"âŒ å¤„ç†é™æ€æ–‡ä»¶è¯·æ±‚å¤±è´¥: {str(e)}")
        return "File not found", 404

@app.route('/api/data', methods=['GET'])
def get_data():
    global current_data_cache
    try:
        logger.debug("ğŸ“¡ è¯·æ±‚/api/data")
        data = {
            'last_updated': current_data_cache.get('last_updated', "") or "",
            'daily_rising': current_data_cache.get('daily_rising', []) or [],
            'short_term_active': current_data_cache.get('short_term_active', []) or [],
            'all_cycle_rising': current_data_cache.get('all_cycle_rising', []) or [],
            'analysis_time': current_data_cache.get('analysis_time', 0),
            'next_analysis_time': current_data_cache.get('next_analysis_time', "") or ""
        }
        logger.debug(f"ğŸ“¡ è¿”å›æ•°æ®: {json.dumps(data, indent=2)}")
        return jsonify(data)
    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
        last_data = get_last_valid_data()
        if last_data:
            return jsonify(last_data)
        return jsonify({'error': str(e)}), 500

# é˜»åŠ›ä½APIç«¯ç‚¹
@app.route('/api/resistance_levels/<symbol>', methods=['GET'])
def get_resistance_levels(symbol):
    try:
        # éªŒè¯å¸ç§æ ¼å¼
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return jsonify({'error': 'Invalid symbol format'}), 400

        logger.info(f"ğŸ“Š è·å–é˜»åŠ›ä½æ•°æ®: {symbol}")
        levels = calculate_resistance_levels(symbol)
        
        if not levels:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é˜»åŠ›ä½æ•°æ®: {symbol}")
            return jsonify({'error': 'Resistance levels not found'}), 404
            
        return jsonify(levels)
    except Exception as e:
        logger.error(f"âŒ è·å–é˜»åŠ›ä½æ•°æ®å¤±è´¥: {symbol}, {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

# æŒä»“é‡å›¾è¡¨æ•°æ®ç«¯ç‚¹
@app.route('/api/oi_chart/<symbol>/<period>', methods=['GET'])
def get_oi_chart_data(symbol, period):
    try:
        # éªŒè¯å¸ç§æ ¼å¼
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return jsonify({'error': 'Invalid symbol format'}), 400

        if period not in PERIOD_MINUTES:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å‘¨æœŸ: {period}")
            return jsonify({'error': 'Unsupported period'}), 400

        logger.info(f"ğŸ“ˆ è·å–æŒä»“é‡å›¾è¡¨æ•°æ®: symbol={symbol}, period={period}")
        oi_data = get_open_interest(symbol, period, use_cache=True)

        # è¿”å›æ•°æ®
        return jsonify({
            'data': oi_data['series'],
            'timestamps': oi_data['timestamps']
        })
    except Exception as e:
        logger.error(f"âŒ è·å–æŒä»“é‡å›¾è¡¨æ•°æ®å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.route('/health', methods=['GET'])
def health_check():
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        conn = sqlite3.connect('data.db')
        c = conn.cursor()
        c.execute("SELECT 1")
        conn.close()
        
        # æ£€æŸ¥Binanceè¿æ¥
        binance_status = 'ok'
        if client:
            try:
                client.get_server_time()
            except:
                binance_status = 'error'
        
        return jsonify({
            'status': 'healthy',
            'database': 'ok',
            'binance': binance_status,
            'last_updated': current_data_cache.get('last_updated', 'N/A'),
            'next_analysis_time': current_data_cache.get('next_analysis_time', 'N/A'),
            'worker_alive': threading.current_thread().is_alive()
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e)
        }), 500

def start_background_threads():
    # ç¡®ä¿é™æ€æ–‡ä»¶å¤¹å­˜åœ¨
    static_path = app.static_folder
    if not os.path.exists(static_path):
        os.makedirs(static_path)
    
    # ç¡®ä¿ index.html å­˜åœ¨
    index_path = os.path.join(static_path, 'index.html')
    if not os.path.exists(index_path):
        with open(index_path, 'w') as f:
            f.write("<html><body><h1>è¯·å°†å‰ç«¯æ–‡ä»¶æ”¾å…¥staticç›®å½•</h1></body></html>")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    if not init_client():
        logger.critical("âŒ æ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯")
        return False
    
    # å¯åŠ¨åå°çº¿ç¨‹
    worker_thread = threading.Thread(target=analysis_worker, name="AnalysisWorker")
    worker_thread.daemon = True
    worker_thread.start()
    
    scheduler_thread = threading.Thread(target=schedule_analysis, name="AnalysisScheduler")
    scheduler_thread.daemon = True
    scheduler_thread.start()
    
    logger.info("âœ… åå°çº¿ç¨‹å¯åŠ¨æˆåŠŸ")
    return True

if __name__ == '__main__':
    PORT = int(os.environ.get("PORT", 9600))
    
    logger.info("=" * 50)
    logger.info(f"ğŸš€ å¯åŠ¨åŠ å¯†è´§å¸æŒä»“é‡åˆ†ææœåŠ¡")
    logger.info(f"ğŸ”‘ APIå¯†é’¥: {API_KEY[:5]}...{API_KEY[-3:]}")
    logger.info(f"ğŸŒ æœåŠ¡ç«¯å£: {PORT}")
    logger.info("=" * 50)
    
    if start_background_threads():
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡å™¨...")
        app.run(host='0.0.0.0', port=PORT, debug=False)
    else:
        logger.error("âŒ æœåŠ¡å¯åŠ¨å¤±è´¥")
