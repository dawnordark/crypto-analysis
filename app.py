#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import subprocess
import traceback

# å®Œå…¨ç§»é™¤ TA-Lib ä¾èµ–
talib = None
print("â„¹ï¸ ä½¿ç”¨å†…ç½®æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å‡½æ•°ï¼Œæ— éœ€å¤–éƒ¨ä¾èµ–")

import time
import re
import json
import math
import requests
import threading
import queue
import logging
import urllib3
import numpy as np
import sqlite3
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import Flask, jsonify, send_from_directory
from binance.client import Client
from flask_cors import CORS
from collections import OrderedDict
from contextlib import contextmanager
from functools import wraps
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®ç±»
class Config:
    PORT = int(os.environ.get("PORT", 9600))
    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()
    API_KEY = os.environ.get('BINANCE_API_KEY', '')
    API_SECRET = os.environ.get('BINANCE_API_SECRET', '')
    
    # ç¼“å­˜é…ç½®
    CACHE_CONFIG = {
        'oi': {
            '5m': 5 * 60, '15m': 15 * 60, '30m': 30 * 60,
            '1h': 60 * 60, '2h': 2 * 60 * 60, '4h': 4 * 60 * 60,
            '6h': 6 * 60 * 60, '12h': 12 * 60 * 60, '1d': 24 * 60 * 60
        },
        'resistance': 24 * 3600,
        'volume': 5 * 60
    }
    
    # åˆ†æé…ç½®
    VOLUME_THRESHOLD = 10000000  # 1000ä¸‡USDT
    MAX_WORKERS = 10
    REQUEST_TIMEOUT = 30
    MAX_RETRIES = 3
    RETRY_DELAY = 5

config = Config()

# è®¾ç½®æ—¥å¿—çº§åˆ«
root_logger = logging.getLogger()
root_logger.setLevel(getattr(logging, config.LOG_LEVEL))

# åˆ›å»ºæ—¥å¿—å¤„ç†å™¨
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(getattr(logging, config.LOG_LEVEL))
file_handler = logging.FileHandler('app.log')
file_handler.setLevel(getattr(logging, config.LOG_LEVEL))

# æ—¥å¿—æ ¼å¼
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)

# æ·»åŠ å¤„ç†å™¨
root_logger.addHandler(console_handler)
root_logger.addHandler(file_handler)

logger = logging.getLogger(__name__)
logger.info(f"âœ… æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º: {config.LOG_LEVEL}")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
app = Flask(__name__, static_folder=os.path.join(BASE_DIR, 'static'), static_url_path='/static')
# æ‰©å±• CORS é…ç½®
CORS(app, resources={
    r"/api/*": {"origins": "*"},
    r"/static/*": {"origins": "*"}
})

# Binance API é…ç½®
client = None

# æ•°æ®ç¼“å­˜ - ä¼˜åŒ–ç¼“å­˜ç»“æ„
data_cache = {
    "last_updated": "ä»æœªæ›´æ–°",
    "daily_rising": [],
    "short_term_active": [],
    "all_cycle_rising": [],
    "analysis_time": 0,
    "next_analysis_time": "è®¡ç®—ä¸­..."
}

current_data_cache = data_cache.copy()

# LRUç¼“å­˜å®ç°
class LRUCache:
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.lock = threading.RLock()
    
    def get(self, key):
        with self.lock:
            if key not in self.cache:
                return None
            self.cache.move_to_end(key)
            return self.cache[key]
    
    def put(self, key, value):
        with self.lock:
            if key in self.cache:
                self.cache.move_to_end(key)
            self.cache[key] = value
            if len(self.cache) > self.capacity:
                self.cache.popitem(last=False)
    
    def clear_expired(self, expiration_check_func):
        with self.lock:
            expired_keys = [
                key for key, value in self.cache.items()
                if expiration_check_func(value)
            ]
            for key in expired_keys:
                del self.cache[key]
            return len(expired_keys)
    
    def __len__(self):
        return len(self.cache)

# ä¼˜åŒ–çš„ç¼“å­˜ç»“æ„
oi_data_cache = LRUCache(capacity=1000)
resistance_cache = LRUCache(capacity=500)
symbol_volume_cache = LRUCache(capacity=500)

# ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œçº¿ç¨‹é—´é€šä¿¡
analysis_queue = queue.Queue()
executor = ThreadPoolExecutor(max_workers=config.MAX_WORKERS)

# åªä¿ç•™æœ‰æ•ˆçš„9ä¸ªå‘¨æœŸ
PERIOD_MINUTES = {
    '5m': 5,
    '15m': 15,
    '30m': 30,
    '1h': 60,
    '2h': 120,
    '4h': 240,
    '6h': 360,
    '12h': 720,
    '1d': 1440
}

VALID_PERIODS = ['5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h', '1d']
RESISTANCE_INTERVALS = ['5m', '15m', '30m', '1h', '2h', '4h', '6h', '12h', '1d']

# æ€§èƒ½ç›‘æ§è£…é¥°å™¨
def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            execution_time = time.time() - start_time
            if execution_time > 1.0:  # è®°å½•è¶…è¿‡1ç§’çš„æ“ä½œ
                logger.warning(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
    return wrapper

# æ•°æ®åº“ç®¡ç†
class AnalysisDatabase:
    def __init__(self, db_path="analysis.db"):
        self.db_path = db_path
        self._init_db()
    
    def _init_db(self):
        with self._get_connection() as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS analysis_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    period TEXT NOT NULL,
                    oi_value REAL,
                    is_highest BOOLEAN,
                    analysis_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(symbol, period, analysis_time)
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS resistance_levels (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    interval TEXT NOT NULL,
                    level_type TEXT NOT NULL,
                    price REAL,
                    strength REAL,
                    test_count INTEGER,
                    analysis_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
    
    @contextmanager
    def _get_connection(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            conn.close()
    
    def save_analysis_result(self, symbol, period, oi_value, is_highest):
        with self._get_connection() as conn:
            conn.execute('''
                INSERT INTO analysis_results (symbol, period, oi_value, is_highest)
                VALUES (?, ?, ?, ?)
            ''', (symbol, period, oi_value, is_highest))

# åˆ›å»ºæ•°æ®åº“å®ä¾‹
db = AnalysisDatabase()

# Binanceå®¢æˆ·ç«¯å•ä¾‹
class BinanceClient:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._client = None
            cls._instance._initialized = False
        return cls._instance
    
    def initialize(self):
        if not self._initialized and config.API_KEY and config.API_SECRET:
            max_retries = config.MAX_RETRIES
            retry_delay = config.RETRY_DELAY
            
            for attempt in range(max_retries):
                try:
                    logger.info(f"ğŸ”§ å°è¯•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ (ç¬¬{attempt+1}æ¬¡)...")
                    self._client = Client(
                        api_key=config.API_KEY, 
                        api_secret=config.API_SECRET,
                        requests_params={'timeout': config.REQUEST_TIMEOUT}
                    )
                    
                    server_time = self._client.get_server_time()
                    logger.info(f"âœ… Binanceå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒæœåŠ¡å™¨æ—¶é—´: {datetime.fromtimestamp(server_time['serverTime']/1000)}")
                    self._initialized = True
                    return True
                except Exception as e:
                    logger.error(f"âŒ åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
                    if attempt < max_retries - 1:
                        logger.info(f"ğŸ”„ {retry_delay}ç§’åé‡è¯•åˆå§‹åŒ–å®¢æˆ·ç«¯...")
                        time.sleep(retry_delay)
            logger.critical("ğŸ”¥ æ— æ³•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
        return self._initialized
    
    @property
    def client(self):
        if not self._initialized:
            if not self.initialize():
                raise RuntimeError("Binance client not initialized")
        return self._client

# åˆ›å»ºBinanceå®¢æˆ·ç«¯å®ä¾‹
binance_client = BinanceClient()

def init_client():
    return binance_client.initialize()

def get_next_update_time(period):
    tz_shanghai = timezone(timedelta(hours=8))
    now = datetime.now(tz_shanghai)
    minutes = PERIOD_MINUTES.get(period, 5)
    
    if period.endswith('m'):
        period_minutes = int(period[:-1])
        current_minute = now.minute
        current_period_minute = (current_minute // period_minutes) * period_minutes
        next_update = now.replace(minute=current_period_minute, second=2, microsecond=0) + timedelta(minutes=period_minutes)
        if next_update < now:
            next_update += timedelta(minutes=period_minutes)
    elif period.endswith('h'):
        period_hours = int(period[:-1])
        current_hour = now.hour
        current_period_hour = (current_hour // period_hours) * period_hours
        next_update = now.replace(hour=current_period_hour, minute=0, second=2, microsecond=0) + timedelta(hours=period_hours)
    else:
        next_update = now.replace(hour=0, minute=0, second=2, microsecond=0) + timedelta(days=1)

    return next_update

def cleanup_cache():
    """æ¸…ç†ä¸ç¬¦åˆæ¡ä»¶çš„ç¼“å­˜æ•°æ®"""
    try:
        current_time = datetime.now(timezone.utc)
        cleaned_count = 0
        
        # æ¸…ç†æŒä»“é‡ç¼“å­˜
        def oi_expiration_check(cached_data):
            return 'expiration' in cached_data and cached_data['expiration'] <= current_time
        
        cleaned_count += oi_data_cache.clear_expired(oi_expiration_check)
        
        # æ¸…ç†ä½äº¤æ˜“é‡å¸ç§çš„é˜»åŠ›ä½ç¼“å­˜
        for cache_key in list(resistance_cache.cache.keys()):
            symbol = cache_key.replace('_resistance', '')
            volume_data = symbol_volume_cache.get(symbol)
            if volume_data and (volume_data.get('expiration', current_time) <= current_time or 
                volume_data.get('volume', 0) < config.VOLUME_THRESHOLD):
                resistance_cache.cache.pop(cache_key, None)
                cleaned_count += 1
        
        logger.info(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆï¼Œæ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸæˆ–æ— æ•ˆæ¡ç›®")
        return cleaned_count
    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
        return 0

@timing_decorator
def get_symbol_volume(symbol):
    """è·å–å¸ç§äº¤æ˜“é‡å¹¶ç¼“å­˜"""
    try:
        current_time = datetime.now(timezone.utc)
        
        cached_data = symbol_volume_cache.get(symbol)
        if cached_data and cached_data['expiration'] > current_time:
            return cached_data['volume']
        
        if not binance_client.initialize():
            return 0
            
        ticker = binance_client.client.futures_ticker(symbol=symbol)
        volume = float(ticker.get('quoteVolume', 0))
        
        symbol_volume_cache.put(symbol, {
            'volume': volume,
            'expiration': current_time + timedelta(seconds=config.CACHE_CONFIG['volume'])
        })
        
        return volume
    except Exception as e:
        logger.error(f"âŒ è·å–{symbol}äº¤æ˜“é‡å¤±è´¥: {str(e)}")
        return 0

def get_open_interest_with_retry(symbol, period, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æŒä»“é‡è·å–"""
    for attempt in range(max_retries):
        try:
            return get_open_interest(symbol, period)
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
            if attempt == max_retries - 1:
                raise
            wait_time = (attempt + 1) * 2  # æŒ‡æ•°é€€é¿
            logger.warning(f"ç¬¬{attempt+1}æ¬¡é‡è¯•è·å–{symbol}çš„{period}æŒä»“é‡ï¼Œç­‰å¾…{wait_time}ç§’")
            time.sleep(wait_time)

@timing_decorator
def get_open_interest(symbol, period, use_cache=True):
    """è·å–æŒä»“é‡æ•°æ®ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜"""
    try:
        if not re.match(r"^[A-Z0-9]{1,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return {'series': [], 'timestamps': []}

        if period not in VALID_PERIODS:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å‘¨æœŸ: {period}")
            return {'series': [], 'timestamps': []}
        
        current_time = datetime.now(timezone.utc)
        cache_key = f"{symbol}_{period}"
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cached_data = oi_data_cache.get(cache_key)
            if cached_data and 'expiration' in cached_data and cached_data['expiration'] > current_time:
                logger.debug(f"ğŸ“ˆ ä½¿ç”¨ç¼“å­˜æ•°æ®: {symbol} {period}")
                return cached_data['data']
            
            # ç¼“å­˜è¿‡æœŸä½†è¿˜åœ¨è·å–æ–°æ•°æ®çš„æ—¶é—´çª—å£å†…ï¼Œæš‚æ—¶ä½¿ç”¨ç¼“å­˜
            next_update_time = get_next_update_time(period)
            time_until_update = (next_update_time - current_time).total_seconds()
            
            if time_until_update > 0 and time_until_update < 300 and cached_data:  # 5åˆ†é’Ÿå†…
                logger.info(f"â³ {symbol} {period} ç¼“å­˜è¿‡æœŸï¼Œä½†æ¥è¿‘æ›´æ–°æ—¶é—´ï¼Œæš‚æ—¶ä½¿ç”¨ç¼“å­˜")
                return cached_data['data']

        # è·å–æ–°æ•°æ®
        logger.info(f"ğŸ“¡ è¯·æ±‚æŒä»“é‡æ•°æ®: symbol={symbol}, period={period}")
        url = "https://fapi.binance.com/futures/data/openInterestHist"
        params = {'symbol': symbol, 'period': period, 'limit': 30}

        # ä½¿ç”¨ä¼šè¯å’Œé‡è¯•ç­–ç•¥
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=10)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        response = session.get(url, params=params, timeout=15)
        logger.debug(f"ğŸ“¡ å“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code != 200:
            logger.error(f"âŒ è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: HTTP {response.status_code}")
            # è¿”å›ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            cached_data = oi_data_cache.get(cache_key)
            if cached_data:
                return cached_data['data']
            return {'series': [], 'timestamps': []}

        data = response.json()
        logger.debug(f"ğŸ“¡ è·å–åˆ° {len(data)} æ¡æŒä»“é‡æ•°æ®")

        if not isinstance(data, list) or len(data) == 0:
            logger.warning(f"âš ï¸ {symbol}çš„{period}æŒä»“é‡æ•°æ®ä¸ºç©º")
            # è¿”å›ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            cached_data = oi_data_cache.get(cache_key)
            if cached_data:
                return cached_data['data']
            return {'series': [], 'timestamps': []}
            
        data.sort(key=lambda x: x['timestamp'])
        oi_series = [float(item['sumOpenInterest']) for item in data]
        timestamps = [item['timestamp'] for item in data]

        if len(oi_series) < 30:
            logger.warning(f"âš ï¸ {symbol}çš„{period}æŒä»“é‡æ•°æ®ä¸è¶³30ä¸ªç‚¹")
            # è¿”å›ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            cached_data = oi_data_cache.get(cache_key)
            if cached_data:
                return cached_data['data']
            return {'series': [], 'timestamps': []}
            
        oi_data = {
            'series': oi_series, 
            'timestamps': timestamps
        }
        
        # è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´
        expiration = current_time + timedelta(seconds=config.CACHE_CONFIG['oi'].get(period, 5*60))
        oi_data_cache.put(cache_key, {
            'data': oi_data,
            'expiration': expiration
        })

        # ä¿å­˜åˆ°æ•°æ®åº“
        try:
            db.save_analysis_result(symbol, period, oi_series[-1] if oi_series else 0, True)
        except Exception as e:
            logger.warning(f"ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")

        logger.info(f"ğŸ“ˆ è·å–æ–°æ•°æ®: {symbol} {period} ({len(oi_series)}ç‚¹)")
        return oi_data
    except Exception as e:
        logger.error(f"âŒ è·å–{symbol}çš„{period}æŒä»“é‡å¤±è´¥: {str(e)}")
        # è¿”å›ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        cached_data = oi_data_cache.get(cache_key)
        if cached_data:
            return cached_data['data']
        logger.error(traceback.format_exc())
        return {'series': [], 'timestamps': []}

def is_latest_highest(oi_data):
    if not oi_data or len(oi_data) < 30:
        logger.debug("æŒä»“é‡æ•°æ®ä¸è¶³30ä¸ªç‚¹")
        return False

    latest_value = oi_data[-1]
    prev_data = oi_data[-30:-1]
    
    return latest_value > max(prev_data) if prev_data else False

@timing_decorator
def detect_price_reaction_levels(symbol, interval):
    """åŸºäºä»·æ ¼ååº”æ£€æµ‹é˜»åŠ›ä½å’Œæ”¯æ’‘ä½"""
    try:
        logger.info(f"ğŸ“Š åˆ†æä»·æ ¼ååº”: {symbol} {interval}")
        
        # è·å–Kçº¿æ•°æ®
        klines = binance_client.client.futures_klines(symbol=symbol, interval=interval, limit=200)
        if not klines or len(klines) < 100:
            return [], []
        
        high_prices = [float(k[2]) for k in klines]
        low_prices = [float(k[3]) for k in klines]
        close_prices = [float(k[4]) for k in klines]
        
        # æ£€æµ‹å±€éƒ¨é«˜ç‚¹å’Œä½ç‚¹
        resistance_levels = []
        support_levels = []
        
        # æ£€æµ‹é˜»åŠ›ä½ï¼ˆä»·æ ¼å¤šæ¬¡ä¸Šæ¶¨è‡³æ­¤åŒºåŸŸåå›è½ï¼‰
        for i in range(2, len(high_prices)-2):
            if (high_prices[i] > high_prices[i-1] and 
                high_prices[i] > high_prices[i-2] and 
                high_prices[i] > high_prices[i+1] and 
                high_prices[i] > high_prices[i+2]):
                
                # æ£€æŸ¥è¯¥ä»·æ ¼åŒºåŸŸæ˜¯å¦å¤šæ¬¡è¢«æµ‹è¯•
                test_count = 0
                price_tolerance = high_prices[i] * 0.002  # 0.2%çš„å®¹å·®èŒƒå›´
                
                for j in range(len(high_prices)):
                    if abs(high_prices[j] - high_prices[i]) <= price_tolerance:
                        test_count += 1
                
                # å¦‚æœè¢«æµ‹è¯•å¤šæ¬¡ä¸”ä»·æ ¼å›è½ï¼Œåˆ™è®¤ä¸ºæ˜¯é˜»åŠ›ä½
                if test_count >= 3:
                    # è®¡ç®—å¼ºåº¦åŸºäºæµ‹è¯•æ¬¡æ•°å’Œå›è½å¹…åº¦
                    decline_after_test = 0
                    for j in range(i, min(i+5, len(close_prices))):
                        if close_prices[j] < high_prices[i]:
                            decline_after_test += 1
                    
                    strength = min(1.0, (test_count * 0.2 + decline_after_test * 0.1))
                    
                    resistance_levels.append({
                        'price': high_prices[i],
                        'strength': round(strength, 2),
                        'test_count': test_count,
                        'type': 'price_reaction'
                    })
        
        # æ£€æµ‹æ”¯æ’‘ä½ï¼ˆä»·æ ¼å¤šæ¬¡ä¸‹è·Œè‡³æ­¤åŒºåŸŸååå¼¹ï¼‰
        for i in range(2, len(low_prices)-2):
            if (low_prices[i] < low_prices[i-1] and 
                low_prices[i] < low_prices[i-2] and 
                low_prices[i] < low_prices[i+1] and 
                low_prices[i] < low_prices[i+2]):
                
                # æ£€æŸ¥è¯¥ä»·æ ¼åŒºåŸŸæ˜¯å¦å¤šæ¬¡è¢«æµ‹è¯•
                test_count = 0
                price_tolerance = low_prices[i] * 0.002  # 0.2%çš„å®¹å·®èŒƒå›´
                
                for j in range(len(low_prices)):
                    if abs(low_prices[j] - low_prices[i]) <= price_tolerance:
                        test_count += 1
                
                # å¦‚æœè¢«æµ‹è¯•å¤šæ¬¡ä¸”ä»·æ ¼åå¼¹ï¼Œåˆ™è®¤ä¸ºæ˜¯æ”¯æ’‘ä½
                if test_count >= 3:
                    # è®¡ç®—å¼ºåº¦åŸºäºæµ‹è¯•æ¬¡æ•°å’Œåå¼¹å¹…åº¦
                    rise_after_test = 0
                    for j in range(i, min(i+5, len(close_prices))):
                        if close_prices[j] > low_prices[i]:
                            rise_after_test += 1
                    
                    strength = min(1.0, (test_count * 0.2 + rise_after_test * 0.1))
                    
                    support_levels.append({
                        'price': low_prices[i],
                        'strength': round(strength, 2),
                        'test_count': test_count,
                        'type': 'price_reaction'
                    })
        
        # åˆå¹¶ç›¸è¿‘çš„æ°´å¹³
        resistance_levels = merge_similar_levels(resistance_levels)
        support_levels = merge_similar_levels(support_levels)
        
        # æŒ‰å¼ºåº¦æ’åºå¹¶è¿”å›å‰5ä¸ª
        resistance_levels.sort(key=lambda x: x['strength'], reverse=True)
        support_levels.sort(key=lambda x: x['strength'], reverse=True)
        
        return resistance_levels[:5], support_levels[:5]
        
    except Exception as e:
        logger.error(f"ä»·æ ¼ååº”åˆ†æå¤±è´¥: {str(e)}")
        return [], []

def merge_similar_levels(levels):
    """åˆå¹¶ç›¸è¿‘çš„ä»·æ ¼æ°´å¹³"""
    if not levels:
        return []
    
    merged = []
    levels.sort(key=lambda x: x['price'])
    
    i = 0
    while i < len(levels):
        current = levels[i]
        group = [current]
        
        j = i + 1
        while j < len(levels) and abs(levels[j]['price'] - current['price']) <= current['price'] * 0.005:
            group.append(levels[j])
            j += 1
        
        # é€‰æ‹©ç»„ä¸­å¼ºåº¦æœ€é«˜çš„æ°´å¹³
        best_level = max(group, key=lambda x: x['strength'])
        # åˆå¹¶æµ‹è¯•æ¬¡æ•°
        total_tests = sum(level['test_count'] for level in group)
        best_level['test_count'] = total_tests
        # é‡æ–°è®¡ç®—å¼ºåº¦
        best_level['strength'] = min(1.0, best_level['strength'] * (1 + 0.1 * (len(group) - 1)))
        
        merged.append(best_level)
        i = j
    
    return merged

@timing_decorator
def calculate_resistance_levels(symbol):
    try:
        logger.info(f"ğŸ“Š è®¡ç®—é˜»åŠ›ä½: {symbol}")
        now = time.time()
        
        cache_key = f"{symbol}_resistance"
        cached_data = resistance_cache.get(cache_key)
        if cached_data and cached_data['expiration'] > now:
            logger.debug(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜çš„é˜»åŠ›ä½æ•°æ®: {symbol}")
            return cached_data['levels']
        
        if not binance_client.initialize():
            logger.error("âŒ æ— æ³•åˆå§‹åŒ–Binanceå®¢æˆ·ç«¯ï¼Œæ— æ³•è®¡ç®—é˜»åŠ›ä½")
            return {'resistance': {}, 'support': {}, 'current_price': 0}
        
        try:
            ticker = binance_client.client.futures_symbol_ticker(symbol=symbol)
            current_price = float(ticker['price'])
            logger.info(f"ğŸ“Š {symbol}å½“å‰ä»·æ ¼: {current_price}")
        except Exception as e:
            logger.error(f"âŒ è·å–{symbol}å½“å‰ä»·æ ¼å¤±è´¥: {str(e)}")
            current_price = None
        
        interval_levels = {}
        
        for interval in RESISTANCE_INTERVALS:
            try:
                logger.info(f"ğŸ“Š åˆ†æä»·æ ¼ååº”: {symbol} {interval}")
                resistance_levels, support_levels = detect_price_reaction_levels(symbol, interval)
                
                # è®¡ç®—è·ç¦»å½“å‰ä»·æ ¼çš„ç™¾åˆ†æ¯”
                resistance_with_distance = []
                support_with_distance = []
                
                if current_price and current_price > 0:
                    for level in resistance_levels:
                        distance_percent = (level['price'] - current_price) / current_price * 100
                        resistance_with_distance.append({
                            'price': round(level['price'], 4),
                            'strength': level['strength'],
                            'distance_percent': round(distance_percent, 2),
                            'test_count': level['test_count'],
                            'type': level['type']
                        })
                    
                    for level in support_levels:
                        distance_percent = (level['price'] - current_price) / current_price * 100
                        support_with_distance.append({
                            'price': round(level['price'], 4),
                            'strength': level['strength'],
                            'distance_percent': round(distance_percent, 2),
                            'test_count': level['test_count'],
                            'type': level['type']
                        })
                
                interval_levels[interval] = {
                    'resistance': resistance_with_distance[:3],  # åªè¿”å›å‰3ä¸ª
                    'support': support_with_distance[:3]         # åªè¿”å›å‰3ä¸ª
                }
                
                logger.info(f"ğŸ“Š {symbol}åœ¨{interval}çš„æœ‰æ•ˆé˜»åŠ›ä½: {len(resistance_with_distance)}ä¸ª, æ”¯æ’‘ä½: {len(support_with_distance)}ä¸ª")
                
            except Exception as e:
                logger.error(f"è®¡ç®—{symbol}åœ¨{interval}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
                logger.error(traceback.format_exc())

        levels = {
            'levels': interval_levels,
            'current_price': current_price or 0
        }
        
        resistance_cache.put(cache_key, {
            'levels': levels,
            'expiration': now + config.CACHE_CONFIG['resistance']
        })
        return levels
    except Exception as e:
        logger.error(f"è®¡ç®—{symbol}çš„é˜»åŠ›ä½å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return {'levels': {}, 'current_price': 0}

@timing_decorator
def analyze_symbol(symbol):
    try:
        logger.info(f"ğŸ” å¼€å§‹åˆ†æå¸ç§: {symbol}")
        
        # æ£€æŸ¥äº¤æ˜“é‡ï¼Œè¿‡æ»¤ä½äº¤æ˜“é‡å¸ç§
        volume = get_symbol_volume(symbol)
        if volume < config.VOLUME_THRESHOLD:
            logger.info(f"â­ï¸ è·³è¿‡ä½äº¤æ˜“é‡å¸ç§: {symbol} (äº¤æ˜“é‡: {volume:.0f})")
            return {
                'symbol': symbol,
                'period_status': {p: False for p in VALID_PERIODS},
                'period_count': 0
            }

        symbol_result = {
            'symbol': symbol,
            'daily_rising': None,
            'short_term_active': None,
            'all_cycle_rising': None,
            'period_status': {p: False for p in VALID_PERIODS},
            'period_count': 0
        }

        daily_oi = get_open_interest_with_retry(symbol, '1d')
        daily_series = daily_oi.get('series', [])
        
        if len(daily_series) >= 30:
            daily_status = is_latest_highest(daily_series)
            symbol_result['period_status']['1d'] = daily_status
            
            if daily_status:
                daily_change = ((daily_series[-1] - daily_series[-30]) / daily_series[-30]) * 100
                logger.info(f"ğŸ“Š {symbol} æ—¥çº¿ä¸Šæ¶¨æ¡ä»¶æ»¡è¶³ï¼Œæ¶¨å¹…: {daily_change:.2f}%")
                
                daily_rising_item = {
                    'symbol': symbol,
                    'oi': daily_series[-1],
                    'change': round(daily_change, 2),
                    'period_status': symbol_result['period_status'].copy()
                }
                symbol_result['daily_rising'] = daily_rising_item
                symbol_result['period_count'] = 1

                logger.info(f"ğŸ“Š å¼€å§‹å…¨å‘¨æœŸåˆ†æ: {symbol}")
                all_intervals_up = True
                for period in VALID_PERIODS:
                    if period == '1d':
                        continue
                        
                    oi_data = get_open_interest_with_retry(symbol, period)
                    oi_series = oi_data.get('series', [])
                    
                    status = len(oi_series) >= 30 and is_latest_highest(oi_series)
                    symbol_result['period_status'][period] = status
                    
                    if status:
                        symbol_result['period_count'] += 1
                    else:
                        all_intervals_up = False

                if all_intervals_up:
                    logger.info(f"ğŸ“Š {symbol} å…¨å‘¨æœŸä¸Šæ¶¨æ¡ä»¶æ»¡è¶³")
                    symbol_result['all_cycle_rising'] = {
                        'symbol': symbol,
                        'oi': daily_series[-1],
                        'change': round(daily_change, 2),
                        'period_count': symbol_result['period_count'],
                        'period_status': symbol_result['period_status'].copy()
                    }
                
                if symbol_result['daily_rising']:
                    symbol_result['daily_rising']['period_status'] = symbol_result['period_status'].copy()
                    symbol_result['daily_rising']['period_count'] = symbol_result['period_count']

        min5_oi = get_open_interest_with_retry(symbol, '5m')
        min5_series = min5_oi.get('series', [])
        
        if len(min5_series) >= 30 and len(daily_series) >= 30:
            min5_max = max(min5_series[-30:])
            daily_avg = sum(daily_series[-30:]) / 30
            
            if min5_max > 0 and daily_avg > 0:
                ratio = min5_max / daily_avg
                logger.debug(f"ğŸ“Š {symbol} çŸ­æœŸæ´»è·ƒæ¯”ç‡: {ratio:.2f}")
                
                if ratio > 1.5:
                    logger.info(f"ğŸ“Š {symbol} çŸ­æœŸæ´»è·ƒæ¡ä»¶æ»¡è¶³")
                    symbol_result['short_term_active'] = {
                        'symbol': symbol,
                        'oi': min5_series[-1],
                        'ratio': round(ratio, 2)
                    }

        logger.info(f"âœ… å®Œæˆåˆ†æå¸ç§: {symbol}")
        return symbol_result
    except Exception as e:
        logger.error(f"âŒ å¤„ç†{symbol}æ—¶å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'symbol': symbol,
            'period_status': {p: False for p in VALID_PERIODS},
            'period_count': 0
        }

@timing_decorator
def analyze_trends():
    start_time = time.time()
    logger.info("ğŸ” å¼€å§‹åˆ†æå¸ç§è¶‹åŠ¿...")
    
    # æ¸…ç†è¿‡æœŸç¼“å­˜
    cleanup_cache()
    
    symbols = get_high_volume_symbols()
    
    if not symbols:
        logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°é«˜äº¤æ˜“é‡å¸ç§")
        return data_cache

    logger.info(f"ğŸ” å¼€å§‹åˆ†æ {len(symbols)} ä¸ªå¸ç§")

    daily_rising = []
    short_term_active = []
    all_cycle_rising = []

    futures = [executor.submit(analyze_symbol, symbol) for symbol in symbols]
    
    for future in as_completed(futures):
        try:
            result = future.result()
            if result.get('daily_rising'):
                daily_rising.append(result['daily_rising'])
            if result.get('short_term_active'):
                short_term_active.append(result['short_term_active'])
            if result.get('all_cycle_rising'):
                all_cycle_rising.append(result['all_cycle_rising'])
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¸ç§æ—¶å‡ºé”™: {str(e)}")

    daily_rising.sort(key=lambda x: x.get('period_count', 0), reverse=True)
    short_term_active.sort(key=lambda x: x.get('ratio', 0), reverse=True)
    all_cycle_rising.sort(key=lambda x: x.get('period_count', 0), reverse=True)

    analysis_time = time.time() - start_time
    logger.info(f"ğŸ“Š åˆ†æç»“æœ: æ—¥çº¿ä¸Šæ¶¨ {len(daily_rising)}ä¸ª, çŸ­æœŸæ´»è·ƒ {len(short_term_active)}ä¸ª, å…¨éƒ¨å‘¨æœŸä¸Šæ¶¨ {len(all_cycle_rising)}ä¸ª")
    logger.info(f"âœ… åˆ†æå®Œæˆ: ç”¨æ—¶{analysis_time:.2f}ç§’")

    tz_shanghai = timezone(timedelta(hours=8))
    return {
        'daily_rising': daily_rising,
        'short_term_active': short_term_active,
        'all_cycle_rising': all_cycle_rising,
        'analysis_time': analysis_time,
        'last_updated': datetime.now(tz_shanghai).strftime("%Y-%m-%d %H:%M:%S"),
        'next_analysis_time': get_next_update_time('5m').strftime("%Y-%m-%d %H:%M:%S")
    }

def get_high_volume_symbols():
    if not binance_client.initialize():
        logger.error("âŒ æ— æ³•è¿æ¥API")
        return []

    try:
        logger.info("ğŸ“Š è·å–é«˜äº¤æ˜“é‡å¸ç§...")
        tickers = binance_client.client.futures_ticker()
        filtered = [
            t for t in tickers if float(t.get('quoteVolume', 0)) > config.VOLUME_THRESHOLD
            and t.get('symbol', '').endswith('USDT')
        ]
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(filtered)} ä¸ªé«˜äº¤æ˜“é‡å¸ç§")
        return [t['symbol'] for t in filtered]
    except Exception as e:
        logger.error(f"âŒ è·å–é«˜äº¤æ˜“é‡å¸ç§å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return []

def analysis_worker():
    global data_cache, current_data_cache
    logger.info("ğŸ”§ æ•°æ®åˆ†æçº¿ç¨‹å¯åŠ¨")

    data_cache = {
        "last_updated": "ä»æœªæ›´æ–°",
        "daily_rising": [],
        "short_term_active": [],
        "all_cycle_rising": [],
        "analysis_time": 0,
        "next_analysis_time": "è®¡ç®—ä¸­..."
    }
    current_data_cache = data_cache.copy()

    while True:
        try:
            task = analysis_queue.get()
            if task == "STOP":
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»“æŸåˆ†æçº¿ç¨‹")
                analysis_queue.task_done()
                break

            analysis_start = datetime.now(timezone.utc)
            logger.info(f"â±ï¸ å¼€å§‹æ›´æ–°æ•°æ® ({analysis_start.strftime('%Y-%m-%d %H:%M:%S')})...")

            backup_cache = data_cache.copy()
            current_backup = current_data_cache.copy()

            try:
                result = analyze_trends()
                
                new_data = {
                    "last_updated": result['last_updated'],
                    "daily_rising": result['daily_rising'],
                    "short_term_active": result['short_term_active'],
                    "all_cycle_rising": result['all_cycle_rising'],
                    "analysis_time": result['analysis_time'],
                    "next_analysis_time": result['next_analysis_time']
                }
                
                logger.info(f"ğŸ“Š åˆ†æç»“æœå·²ç”Ÿæˆ")
                logger.info(f"å…¨å‘¨æœŸä¸Šæ¶¨å¸ç§æ•°é‡: {len(new_data['all_cycle_rising'])}")
                logger.info(f"æ—¥çº¿ä¸Šæ¶¨å¸ç§æ•°é‡: {len(new_data['daily_rising'])}")
                logger.info(f"çŸ­æœŸæ´»è·ƒå¸ç§æ•°é‡: {len(new_data['short_term_active'])}")
                
                data_cache = new_data
                current_data_cache = new_data.copy()
                logger.info(f"âœ… æ•°æ®æ›´æ–°æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
                data_cache = backup_cache
                current_data_cache = current_backup
                logger.info("ğŸ”„ æ¢å¤å†å²æ•°æ®")

            analysis_end = datetime.now(timezone.utc)
            analysis_duration = (analysis_end - analysis_start).total_seconds()
            logger.info(f"â±ï¸ åˆ†æè€—æ—¶: {analysis_duration:.2f}ç§’")
            
            next_time = get_next_update_time('5m')
            wait_seconds = (next_time - analysis_end).total_seconds()
            logger.info(f"â³ ä¸‹æ¬¡åˆ†æå°†åœ¨ {wait_seconds:.1f} ç§’å ({next_time.strftime('%Y-%m-%d %H:%M:%S')})")
            
            logger.info("=" * 50)
            
            analysis_queue.task_done()
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            analysis_queue.task_done()

def schedule_analysis():
    logger.info("â° å®šæ—¶åˆ†æè°ƒåº¦å™¨å¯åŠ¨")
    now = datetime.now(timezone.utc)
    next_time = get_next_update_time('5m')
    initial_wait = (next_time - now).total_seconds()
    logger.info(f"â³ é¦–æ¬¡åˆ†æå°†åœ¨ {initial_wait:.1f} ç§’åå¼€å§‹ ({next_time.strftime('%Y-%m-%d %H:%M:%S')})...")
    time.sleep(max(0, initial_wait))

    while True:
        analysis_start = datetime.now(timezone.utc)
        logger.info(f"ğŸ”” è§¦å‘å®šæ—¶åˆ†æä»»åŠ¡ ({analysis_start.strftime('%Y-%m-%d %H:%M:%S')}")
        analysis_queue.put("ANALYZE")
        analysis_queue.join()

        analysis_duration = (datetime.now(timezone.utc) - analysis_start).total_seconds()
        now = datetime.now(timezone.utc)
        next_time = get_next_update_time('5m')
        wait_time = (next_time - now).total_seconds()

        if wait_time < 0:
            wait_time = 0
        elif analysis_duration > 240:
            wait_time = 0
            logger.warning("âš ï¸ åˆ†æè€—æ—¶è¿‡é•¿ï¼Œç«‹å³å¼€å§‹ä¸‹ä¸€æ¬¡åˆ†æ")
        elif wait_time > 300:
            adjusted_wait = max(60, wait_time - 120)
            logger.info(f"â³ è°ƒæ•´ç­‰å¾…æ—¶é—´: {wait_time:.1f}ç§’ -> {adjusted_wait:.1f}ç§’")
            wait_time = adjusted_wait

        logger.info(f"â³ ä¸‹æ¬¡åˆ†æå°†åœ¨ {wait_time:.1f} ç§’å ({next_time.strftime('%Y-%m-%d %H:%M:%S')})")
        time.sleep(wait_time)

# APIè·¯ç”±
@app.route('/')
def index():
    try:
        return send_from_directory(app.static_folder, 'index.html')
    except Exception as e:
        logger.error(f"âŒ å¤„ç†é¦–é¡µè¯·æ±‚å¤±è´¥: {str(e)}")
        return "Internal Server Error", 500

@app.route('/static/<path:filename>')
def static_files(filename):
    return send_from_directory(app.static_folder, filename)

@app.route('/api/data', methods=['GET'])
def get_data():
    global current_data_cache
    try:
        logger.info("ğŸ“¡ æ”¶åˆ° /api/data è¯·æ±‚")
        
        # å¼ºåˆ¶åˆ·æ–°æ•°æ®å¦‚æœç¼“å­˜ä¸ºç©º
        if not current_data_cache or current_data_cache.get("last_updated") == "ä»æœªæ›´æ–°":
            logger.info("ğŸ”„ ç¼“å­˜ä¸ºç©ºï¼Œè§¦å‘å³æ—¶åˆ†æ")
            analysis_queue.put("ANALYZE")
            return jsonify({
                'last_updated': "æ•°æ®ç”Ÿæˆä¸­...",
                'daily_rising': [],
                'short_term_active': [],
                'all_cycle_rising': [],
                'analysis_time': 0,
                'next_analysis_time': "è®¡ç®—ä¸­..."
            })
        
        if not current_data_cache or not isinstance(current_data_cache, dict):
            logger.warning("âš ï¸ å½“å‰æ•°æ®ç¼“å­˜æ ¼å¼é”™è¯¯ï¼Œé‡ç½®ä¸ºé»˜è®¤")
            current_data_cache = {
                "last_updated": "ä»æœªæ›´æ–°",
                "daily_rising": [],
                "short_term_active": [],
                "all_cycle_rising": [],
                "analysis_time": 0,
                "next_analysis_time": "è®¡ç®—ä¸­..."
            }
        
        def validate_coins(coins):
            valid_coins = []
            for coin in coins:
                if not isinstance(coin, dict):
                    continue
                if 'symbol' not in coin:
                    coin['symbol'] = 'æœªçŸ¥å¸ç§'
                if 'oi' not in coin:
                    coin['oi'] = 0
                if 'change' not in coin:
                    coin['change'] = 0
                if 'ratio' not in coin:
                    coin['ratio'] = 0
                if 'period_count' not in coin:
                    coin['period_count'] = 0
                if 'period_status' not in coin:
                    coin['period_status'] = {}
                valid_coins.append(coin)
            return valid_coins
        
        daily_rising = validate_coins(current_data_cache.get('daily_rising', []))
        short_term_active = validate_coins(current_data_cache.get('short_term_active', []))
        all_cycle_rising = validate_coins(current_data_cache.get('all_cycle_rising', []))
        
        all_cycle_symbols = {coin['symbol'] for coin in all_cycle_rising}
        filtered_daily_rising = [coin for coin in daily_rising if coin['symbol'] not in all_cycle_symbols]
        
        data = {
            'last_updated': current_data_cache.get('last_updated', ""),
            'daily_rising': filtered_daily_rising,
            'short_term_active': short_term_active,
            'all_cycle_rising': all_cycle_rising,
            'analysis_time': current_data_cache.get('analysis_time', 0),
            'next_analysis_time': current_data_cache.get('next_analysis_time', "")
        }
        
        logger.info(f"ğŸ“¦ è¿”å›æ•°æ®: æ—¥çº¿ä¸Šæ¶¨ {len(filtered_daily_rising)}ä¸ª, å…¨å‘¨æœŸä¸Šæ¶¨ {len(all_cycle_rising)}ä¸ª")
        return jsonify(data)
    
    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {str(e)}")
        tz_shanghai = timezone(timedelta(hours=8))
        return jsonify({
            'last_updated': datetime.now(tz_shanghai).strftime("%Y-%m-%d %H:%M:%S"),
            'daily_rising': [],
            'short_term_active': [],
            'all_cycle_rising': [],
            'analysis_time': 0,
            'next_analysis_time': get_next_update_time('5m').strftime("%Y-%m-%d %H:%M:%S")
        })

@app.route('/api/resistance_levels/<symbol>', methods=['GET'])
def get_resistance_levels(symbol):
    try:
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return jsonify({'error': 'Invalid symbol format'}), 400

        logger.info(f"ğŸ“Š è·å–é˜»åŠ›ä½æ•°æ®: {symbol}")
        levels = calculate_resistance_levels(symbol)
        
        if not isinstance(levels, dict):
            logger.error(f"âŒ é˜»åŠ›ä½æ•°æ®ç»“æ„é”™è¯¯: {type(levels)}")
            return jsonify({'error': 'Invalid resistance data structure'}), 500
        
        if 'levels' not in levels:
            levels['levels'] = {}
        if 'current_price' not in levels:
            levels['current_price'] = 0
        
        return jsonify(levels)
    except Exception as e:
        logger.error(f"âŒ è·å–é˜»åŠ›ä½æ•°æ®å¤±è´¥: {symbol}, {str(e)}")
        return jsonify({
            'levels': {},
            'current_price': 0,
            'error': str(e)
        }), 500

@app.route('/api/oi_chart/<symbol>/<period>', methods=['GET'])
def get_oi_chart_data(symbol, period):
    try:
        if not re.match(r"^[A-Z0-9]{2,10}USDT$", symbol):
            logger.warning(f"âš ï¸ æ— æ•ˆçš„å¸ç§åç§°: {symbol}")
            return jsonify({'error': 'Invalid symbol format'}), 400

        if period not in VALID_PERIODS:
            logger.warning(f"âš  ä¸æ”¯æŒçš„å‘¨æœŸ: {period}")
            return jsonify({'error': 'Unsupported period'}), 400

        logger.info(f"ğŸ“ˆ è·å–æŒä»“é‡å›¾è¡¨æ•°æ®: symbol={symbol}, period={period}")
        oi_data = get_open_interest_with_retry(symbol, period, use_cache=True)
        return jsonify({
            'data': oi_data.get('series', []),
            'timestamps': oi_data.get('timestamps', [])
        })
    except Exception as e:
        logger.error(f"âŒ è·å–æŒä»“é‡å›¾è¡¨æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    try:
        binance_status = 'ok'
        if binance_client._initialized:
            try:
                binance_client.client.get_server_time()
            except:
                binance_status = 'error'
        else:
            binance_status = 'not initialized'
        
        tz_shanghai = timezone(timedelta(hours=8))
        return jsonify({
            'status': 'healthy',
            'binance': binance_status,
            'last_updated': current_data_cache.get('last_updated', 'N/A'),
            'next_analysis_time': current_data_cache.get('next_analysis_time', 'N/A'),
            'server_time': datetime.now(tz_shanghai).strftime("%Y-%m-%d %H:%M:%S"),
            'cache_stats': {
                'oi_cache_size': len(oi_data_cache),
                'resistance_cache_size': len(resistance_cache),
                'volume_cache_size': len(symbol_volume_cache)
            },
            'config': {
                'volume_threshold': config.VOLUME_THRESHOLD,
                'max_workers': config.MAX_WORKERS
            }
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e)
        }), 500

@app.route('/api/status')
def status():
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0',
        'cache_stats': {
            'oi_cache': len(oi_data_cache),
            'resistance_cache': len(resistance_cache),
            'volume_cache': len(symbol_volume_cache)
        }
    })

def start_background_threads():
    static_path = app.static_folder
    if not os.path.exists(static_path):
        os.makedirs(static_path)
    
    index_path = os.path.join(static_path, 'index.html')
    if not os.path.exists(index_path):
        with open(index_path, 'w') as f:
            f.write("<html><body><h1>è¯·å°†å‰ç«¯æ–‡ä»¶æ”¾å…¥staticç›®å½•</h1></body></html>")
    
    if not binance_client.initialize():
        logger.critical("âŒ æ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯")
        return False
    
    global current_data_cache
    tz_shanghai = timezone(timedelta(hours=8))
    current_data_cache = {
        "last_updated": "ç­‰å¾…é¦–æ¬¡åˆ†æ",
        "daily_rising": [],
        "short_term_active": [],
        "all_cycle_rising": [],
        "analysis_time": 0,
        "next_analysis_time": get_next_update_time('5m').strftime("%Y-%m-%d %H:%M:%S")
    }
    logger.info("ğŸ†• åˆ›å»ºåˆå§‹å†…å­˜æ•°æ®è®°å½•")
    
    worker_thread = threading.Thread(target=analysis_worker, name="AnalysisWorker")
    worker_thread.daemon = True
    worker_thread.start()
    
    scheduler_thread = threading.Thread(target=schedule_analysis, name="AnalysisScheduler")
    scheduler_thread.daemon = True
    scheduler_thread.start()
    
    # æ·»åŠ åˆå§‹åˆ†æä»»åŠ¡
    analysis_queue.put("ANALYZE")
    logger.info("ğŸ”„ å·²æäº¤åˆå§‹åˆ†æä»»åŠ¡")
    
    logger.info("âœ… åå°çº¿ç¨‹å¯åŠ¨æˆåŠŸ")
    return True

if __name__ == '__main__':
    logger.info("=" * 50)
    logger.info(f"ğŸš€ å¯åŠ¨åŠ å¯†è´§å¸æŒä»“é‡åˆ†ææœåŠ¡")
    logger.info(f"ğŸ”‘ APIå¯†é’¥: {config.API_KEY[:5]}...{config.API_KEY[-3:] if config.API_KEY else 'æœªè®¾ç½®'}")
    logger.info(f"ğŸŒ æœåŠ¡ç«¯å£: {config.PORT}")
    logger.info("ğŸ’¾ æ•°æ®å­˜å‚¨: å†…å­˜å­˜å‚¨ + SQLiteæŒä¹…åŒ–")
    logger.info("=" * 50)
    
    if start_background_threads():
        logger.info("ğŸš€ å¯åŠ¨æœåŠ¡å™¨...")
        app.run(host='0.0.0.0', port=config.PORT, debug=False)
    else:
        logger.critical("ğŸ”¥ æ— æ³•å¯åŠ¨æœåŠ¡ï¼Œè¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—")
